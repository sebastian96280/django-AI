aprende
machine
learning

escrito por
juan ignacio bagnato

basado en
el contenido del blog

aprende machine learning en español
teoría práctica python

juan ignacio bagnato
este libro está a la venta en httpleanpubcomaprendeml

esta versión se publicó en 

la

leanpub

éste es un libro de leanpub leanpub anima a los autores y publicadoras con el proceso de
publicación lean publishing es el acto de publicar un libro en progreso usando herramientas
sencillas y muchas iteraciones para obtener retroalimentación del lector hasta conseguir el libro
adecuado

o juan ignacio bagnato

índice general

 crear etiquetas y clases ea 
creamos sets de entrenamiento y test validación y preprocesar 
 creamos la red aquí la magia 
entrenamos la cnn dedier eee eee 
resultados de la clasificación 
resumen emedrererderverdederverve verdevedederererer 
cómo funcionan las convolutional neural networks qoqer 
muchas imágenes r a anene nn 
pixeles y neuronas e aa ede nnc 
convoluciones oedeaerede eee ereerecveeveee 
filtro conjunto de kernels 
la función de activación eeiiee eee 
subsampling eeec 
ya terminamos no ahora más convoluciones 
conectar con una red neuronal tradicional 
y cómo aprendió la cnn a ver backpropagation e 
comparativa entre una red neuronal tradicional y una cnn e 
arquitectura básica eñeere ene nn 
resumen emedrererderverdederverve verdevedederererer 
detección de objetos con python s 
en qué consiste la detección yolo a 
el proyecto propuesto detectar personajes de lego 
crea un dataset imágenes y anotaciones e a 
el lego dataset 
el código python as 
leer el dataset i eee eee 
train y validación 
data augmentation r eee 
crear la red de clasificación 
crear la red de detección i re e e 
generar las anclas ei i eee eee 
entrenar la red eee 
revisar los resultados emrriee eee 
probar la red i eee eee 
resumen emedrererderverdederverve verdevedederererer 
anexo i webscraping ere eee oeeeere 
ejemplo web scraping en python ibex la bolsa de madrid 
requerimientos 
conocimientos básicos de html y css 
inspección manual de la web 

código webscraping python

n

a



datos desbalanceados

bbc balancedbaggingclassifierbaseestimatordecisiontreeclassifier

samplingstrategyauto
replacementfalse

randomstate

train the classifier
bbcfitxtrain ytrain

predy bbc predictxtest
mostrarresultadosytest predy

confusion matrix

normal

fraud

normal fraud
predicted class

precision recall flscore

o 

 

accuracy 
macro avg 
weighted avg 

tampoco está mal vemos siempre mejora con respecto al modelo inicial con un recall de para

los casos de fraude

support












resultados de las estrategias

veamos en una tabla ordenada de mejor a peor los resultados obtenidos

d httpwwwaprendemachinelearningcomwpcontentuploadsbalanceensemblepng

datos desbalanceados 

algorithm precision recall overall

 penalizacion 
 nearmiss subsampling 
 random oversampling 
 ensemble 
 smote tomek 
o regresion logística 



en nuestro caso las estrategias de penalización y subsampling nos dan el mejor resultado cada
una con un recall de 

pero quedémonos con esto con cualquiera de las técnicas que aplicamos mejoramos el
modelo inicial de regresión logística que lograba un de recall para la clase de fraude y
no olvidemos que hay un tremendo desbalance de clases en el dataset

importante esto no quiere decir que siempre hay que aplicar penalización ó nearmiss subsam
pling dependerá del caso del desbalanceo y del modelo en este caso usamos regresión logística
pero podría ser otro

resumen

es muy frecuente encontrarnos con datasets con clases desbalanceadas de hecho lo más raro sería
encontrar datasets bien equilibrados

siempre que puedas sal a la calle y consigue más muestras de la clase minoritaria pero la

realidad es que a veces no es posible conseguir más datos de esas clases como por ejemplo en casos
de salud

vimos diversas estrategias a seguir para combatir esta problemática eliminar muestras del set
mayoritario crear muestras sintéticas con algún criterio ensamble y penalización

además revisamos la matriz de confusión y comprendimos que las métricas pueden ser
engañosas si miramos a nuestros aciertos únicamente puede que pensemos que tenemos un buen
clasificador cuando realmente está fallando

httpwwwaprendemachinelearningcomwpcontentuploadsimbalanceresultpng

datos desbalanceados

recursos

 notebook con todo el ejercicio y más cositas

 descarga el dataset desde kaggle
 librería de imbalancedlearn

enlaces de interés

 tactics to combat imbalanced classes

 how to fix unbalanced dataset

httpsgithubcomjbagnatomachinelearningblobmasterejercicioimbalanceddataipynb

httpswww kagglecommlgulbcreditcardfrauddata

httpsimbalancedlearnreadthedocsioenstable

 httpsmachinelearningmasterycomtacticstocombatimbalancedclassesinyourmachinelearning dataset
httpswwwwkdnuggetscomfixunbalanced datasethtml

random forest el poder del
ensamble

luego del algoritmo de árbol de decisión tu próximo paso es el de estudiar random forest
comprende qué és y cómo funciona con un ejemplo práctico en python

random forest es un tipo de ensamble en machine learning en donde combinaremos diversos
árboles ya veremos cómo y con qué características y la salida de cada uno se contará como un
voto y la opción más votada será la respuesta del bosque aleatorio

random forest al igual que el árbol e decisión es un modelo de aprendizaje supervisado

clasificación aunque también puede usarse para problemas de regresión

para

cómo surge random forest

uno de los problemas que aparecía con la creación de un árbol de decisión es que si le damos
la profundidad suficiente el árbol tiende a memorizar las soluciones en vez de generalizar el
aprendizaje es decir a padecer de overfitting la solución para evitar esto es la de crear muchos
árboles y que trabajen en conjunto veamos cómo

cómo funciona random forest

random forest funciona así

 seleccionamos k features columnas de las m totales siendo k menor a m y creamos un árbol
de decisión con esas k características

 creamos n árboles variando siempre la cantidad de k features y también podríamos variar la
cantidad de muestras que pasamos a esos árboles esto es conocido como bootstrap sample

 tomamos cada uno de los n árboles y le pedimos que hagan una misma clasificación
guardamos el resultado de cada árbol obteniendo n salidas

 calculamos los votos obtenidos para cada clase seleccionada y consideraremos a la más
votada como la clasificación final de nuestro bosque

 httpwwwaprendemachinelearningcomarbolde decisionenpythonclasificacionyprediccion
httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning

s httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado

 httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning
shttpwwwaprendemachinelearningcomqueesoverfittingyunderfittingycomosolucionarlo

random forest el poder del ensamble 

por qué es aleatorio

contamos con una doble aleatoriedad tanto en la selección del valor k de características para cada
árbol como en la cantidad de muestras que usaremos para entrenar cada árbol creado

es curioso que para este algoritmo la aleatoriedad sea tan importante y de hecho es lo que lo hace
bueno pues le brinda flexibilidad suficiente como para poder obtener gran variedad de árboles y de
muestras que en su conjunto aparentemente caótico producen una salida concreta darwin estaría
orgulloso 

ventajas y desventajas del uso de random forest

vemos algunas de sus ventajas son

 funciona bien aún sin ajuste de hiperparámetros

 sirve para problemas de clasificación y también de regresión

 al utilizar múltiples árboles se reduce considerablemente el riesgo de overfiting

 se mantiene estable con nuevas muestras puesto que al utilizar cientos de árboles sigue
prevaleciendo el promedio de sus votaciones



y sus desventajas

 en algunos datos de entrada particulares random forest también puede caer en overfitting

 es mucho más costoso de crear y ejecutar que un sólo árbol de decisión

 puede requerir muchísimo tiempo de entrenamiento

 random forest no funciona bien con datasets pequeños

 es muy difícil poder interpretar los cientos de árboles creados en el bosque si quisiéramos
comprender y explicar a un cliente su comportamiento

vamos al código python

continuaremos con el ejercicio propuesto en el artículo desbalanceo de datos en donde
utilizamos el dataset de kaggle con información de fraude en tarjetas de crédito cuenta con 
filas y columnas de características nuestra salida será si es un cliente normal o si hizo uso
fraudulento

httpwwwaprendemachinelearningcompasosmachinelearningconstruirmaquina

 httpwwwaprendemachinelearningcomqueesoverfittingyunderfittingycomosolucionarlo
 httpwwwaprendemachinelearningcominterpretacionde modelosdemachinelearning

 httpwwwaprendemachinelearningcomclasificacioncondatos desbalanceados

random forest el poder del ensamble 

frequency by observation number








number of observations

o
normal fraud

class

llegas a ver la mínima linea roja que representa los casos de fraude son apenas frente a más
de casos de uso normal

retomaremos el mejor caso que obtuvimos en el ejercicio anterior utilizando regresión logística

y logrando un de aciertos pero recuerda también las métricas de f precisión y recall que eran
las que realmente nos ayudaban a validar el modelo

creamos el modelo y lo entrenamos

utilizaremos el modelo randomforrestclassifier de scikitlearn

 httpsenwikipediaorgwikilogisticregression

rn 

o 

random forest el poder del ensamble 

from sklearnensemble import randomforestclassifier

 crear el modelo con arboles

model randomforestclassifiernestimators
bootstrap true verbose
maxfeatures sgrt

 a entrenar

model fitxtrain ytrain

luego de unos minutos obtendremos el modelo entrenado en mi caso minuto segundos

los hiperparámetros más importantes

al momento de ajustar el modelo debemos tener en cuenta los siguientes hiperparámetros estos
nos ayudarán a que el bosque de mejores resultados para cada ejercicio recuerda que esto no se
trata de copiar y pegar

 nestimators será la cantidad de árboles que generaremos

 maxfeatures la manera de seleccionar la cantidad máxima de features para cada árbol

 minsampleleaf número mínimo de elementos en las hojas para permitir un nuevo split
división del nodo

 oobscore es un método que emula el crossvalidation en árboles y permite mejorar la
precisión y evitar overfitting

 boostrap para utilizar diversos tamaños de muestras para entrenar si se pone en falso utilizará
siempre el dataset completo

 njobs si tienes multiples cores en tu cpu puedes indicar cuantos puede usar el modelo al
entrenar para acelerar el entrenamiento

evaluamos resultados

veamos la matriz de confusión y las métricas sobre el conjunto de test no confundir con el de
training

random forest el poder del ensamble

normal

true class

fraud

u
normal

vemos muy buenos resultados clasificando con error apenas muestras

accuracy
macro avg
weighted avg

confusion matrix

predicted class

precision

recall

fraud

flscore








support

random forest el poder del ensamble 

aquí podemos destacar que para la clase minoritaria es decir la que detecta los casos de fraude
tenemos un buen valor de recall de lo cual es un buen indicador y el fscore macro avg es de
 logramos construir un modelo de bosque aleatorio que a pesar de tener un conjunto de datos
de entrada muy desigual logra buenos resultados

comparamos con el baseline

si comparamos estos resultados con los del algoritmo de regresión logística

random forest nos dio mejores clasificaciones menos falsos positivos
general

 vemos que el
y mejores métricas en

resumen

avanzando en nuestro aprendizaje sobre diversos modelos que podemos aplicar a las problemáticas
que nos enfrentamos hoy sumamos a nuestro kit de herramientas el random forest vemos que
es un modelo sencillo bastante rápido y si bien perdemos la interpretabilidad maravillosa que nos
brindaba sólo árbol de decisión es el precio a pagar para evitar el overfitting y para ganar un
clasificador más robusto

los algoritmos treebased en inglés son muchos todos parten de la idea principal de árbol
de decisión y la mejoran con diferentes tipos de ensambles y técnicas tenemos que destacar a
 modelos que según el caso logran superar a las mismísimas redes neuronales son xgboost y
lightgbm

recursos y adicionales

puedes descargar la notebook para este ejercicio desde mi cuenta de github

 código en jupyter notebook en github
 dataset de kaggle

otros artículos sobre random forest en inglés

 random forest simple explanation
 an implementation of random forest in python

 httpwwwaprendemachinelearningcomregresionlogisticaconpythonpasoapaso
httpwwwaprendemachinelearningcomclasificacioncondatos desbalanceados
httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning

 httpwwwaprendemachinelearningcominterpretacion de modelosde machinelearning
httpwwwaprendemachinelearningcomqueesoverfittingyunderfittingycomosolucionarlo
dhttpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning
httpwwwaprendemachinelearningcomarbolde decisionenpythonclasificacionyprediccion

 httpwwwaprendemachinelearningcomunasencillaredneuronalenpythonconkerasytensorflow
 httpsgithubcomjbagnatomachinelearningblobmasterejerciciorandomforestipynb
ohttpswwwkagglecommlgulbcreditcardfrauddata

 httpsmediumcomqwilliamkoehrsenrandomforestsimpleexplanationadd
httpstowardsdatasciencecomanimplementationandexplanationoftherandomforestinpythonbfab

conjunto de entrenamiento test y
validación

vamos a comentar las diferencias entre los conjuntos de entrenamiento validación y test utilizados
en machine learning ya que suele haber bastante confusión en para qué es cada uno y cómo
utilizarlos adecuadamente

veremos que tenemos distintas técnicas de hacer la validación del modelo y aplicarlas con scikit
learn en python

un nuevo mundo

al principio de los tiempos sólo tenemos un conjunto pangea que contiene todo nuestro dato
disponible digamos que tenemos un archivo csv con registros

para entrenar nuestro modelo de machine learning y poder saber si está funcionando bien alguien
dijo separemos el conjunto de datos inicial en conjunto de entrenamiento train y conjunto de
pruebas test por lo general se divide haciendo y se toman muestras aleatorias no en
secuencia si no mezclado

para hacer el ejemplo sencillo supongamos que queremos hacer clasificación usando un algoritmo
supervisado con lo cual tendremos

 x train con registros para entrenar

 ytrain con las etiquetas de los resultados esperados de xtrain
 x test con registros para test

 ytest con las etiquetas de los resultados de xtest

nttpswwwaprendemachinelearningcomqueesmachinelearning

 httpswwwaprendemachinelearningcompasosmachinelearningconstruirmaquina
shttpswwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning
 httpswwwaprendemachinelearningcomaplicacionesdelmachinelearning

índice general

guardar csv y ver en excel 
otros ejemplos útiles de webscaping 
resumen emedrererderverdederverve verdevedederererer 
anexo ii machine learning en la nube aa 
machine learning en la nube google colaboratory con gpu 
machine learning desde el navegador 
la gpu en casa o en la nube a 
qué es google colab eee 
enlazar con google drive ae 
ejecutar una jupyter notebook de github 
instalar otras librerías python con pip 
resumen emedrererderverdederverve verdevedederererer 
anexo iii principal component analysis 
introducción a pca ee eee 
qué es principal component analysis 
cómo funciona pca eee ee 
selección de los componentes principales ñ 
pero porqué funciona pca 
ejemplo mínimo en python 
resumen emedrererderverdederverve verdevedederererer 

resultados de pca en el mundo real

conjunto de entrenamiento test y validación 

hágase el conjunto de test

lo interesante de esto es que una vez los separamos en registros para entrenar y para
probar usaremos sólo esos registros para alimentar el modelo al entrenar haciendo

modelofitxtrain y train

luego de entrenar nuestro modelo y habiendo decidido como métrica de negocio el accuracy el 
de aciertos obtenemos un sobre el set de entrenamiento y asumimos que ese porcentaje nos
sirve para nuestro objetivo de negocio

los registros que separamos en xtest aún nunca han pasado por el modelo de ml se
entiende esto porque eso es muy importante cuando usemos el set de test haremos

modelopredictxtest

como verás no estamos usando fit sólo pasaremos los datos sin la columna de ytest que
contiene las etiquetas además remarco que estamos haciendo predicción me refiero a que el modelo
no se está entrenando ni incorporando conocimiento el modelo se limita a ver la entrada y
escupir una salida

cuando hacemos el predict sobre el conjunto de test y obtenemos las predicciones las podemos
comprobar y contrastar con los valores reales almacenados en ytest y hallar así la métrica que
usamos los resultados que nos puede dar serán

conjunto de entrenamiento test y validación 

 si el accuracy en test es cercano al de entrenamiento dijimos por ejemplo en este caso
si estuviera entre ú quiere decir que nuestro modelo entrenado está generalizando bien
y lo podemos dar por bueno siempre y cuando estemos conformes con las métricas obtenidas

 si el accuracy en test es muy distinto al de entrenamiento tanto por encima como por debajo
nos da un ó un lejano al entonces es un indicador de que nuestro modelo no ha
entrenado bien y no nos sirve de hecho este podría ser un indicador de overfitting

para evaluar mejor el segundo caso es donde aparece el conjunto de validación

al séptimo día dios creo el crossvalidation

si el conjunto de train y test nos está dando métricas muy distintas esto es que el modelo no nos
sirve

para mejorar el modelo podemos pensar en tunear sus parámetros y volver a entrenar y probar
podemos intentar obtener más registros cambiar el preprocesado de datos limpieza balanceo de
clases selección de features generación de features de hecho podemos pensar que seleccio

 httpswwwaprendemachinelearningcomqueesoverfittingy underfittingycomosolucionarlo
shttpswwwaprendemachinelearningcomclasificacioncon datos desbalanceados

conjunto de entrenamiento test y validación 

namos un mal modelo y podemos intentar con distintos modelos de árbol de decisión redes
neuronales ensambles

la técnica de validación cruzada nos ayudará a medir el comportamiento dellos modelos
que creamos y nos ayudará a encontrar un mejor modelo rápidamente

repasemos antes de empezar hasta ahora contamos con conjuntos el de train y test el set de
validación no es realmente un tercer set si no que vive dentro del conjunto de train reitero el set
de validación no es un conjunto que apartemos de nuestro archivo esv original el set de validación
se utilizará durante iteraciones que haremos con el conjunto de entrenamiento

técnicas de validación cruzada

entonces volvamos a tener las cosas claras solo tenemos conjunto de train y test ok el de test
seguirá tratándose como antes lo apartamos y lo usaremos al final una vez entrenemos el modelo

o httpswwwaprendemachinelearningcomarbol dedecision enpythonclasificaciony prediccion
httpswwwaprendemachinelearningcomunasencillaredneuronalenpythonconkerasytensorflow
 httpswwwaprendemachinelearningcomrandomforestelpoderdelensamble

conjunto de entrenamiento test y validación 

dentro del conjunto de train y siguiendo nuestro ejemplo inicial tenemos registros la
validación más común utilizada y que nos sirve para entender el concepto es kfolds vamos a
comentarla

crossvalidation kfold con splits

lo que hacemos normalmente al entrenar el modelo es pasarle los registros y que haga el
fit con kfolds en este ejemplo de splits para entrenar en vez de pasarle todos los registros
directamente al modelo haremos así

e iterar veces
 apartaremos de muestras es decir 
 entrenamos al modelo con el restante de muestras 
 mediremos el accuracy obtenido sobre las que habíamos apartado

 esto quiere decir que hacemos entrenamientos independientes
 el accuracy final será el promedio de las accuracies anteriores

kfolds

iteracion 

en amarillo las muestras para entrenar y en verde el conjunto de validación

entonces fijémonos que estamos ocultando una quinta parte del conjunto de train durante cada
iteración esto es similar a lo que explique antes pero esta vez aplicado al momento de entrenamiento
al cabo de esas iteraciones obtenemos accuracies que deberían ser similares entre sí esto sería
un indicador de que el modelo está funcionando bien

ejemplo kfolds en python

veamos en código python usando la librería de data science scikitlearn como podemos hacer el
crossvalidation con kfolds

httpsscikitlearnorgstablemodulescrossvalidationhtmlikfold

conjunto de entrenamiento test y validación 

 from sklearn import datasets metrics
 from sklearnmodelselection import traintestsplit
 from sklearnmodelselection import crossvalscore
 from sklearnmodelselection import kfold
 from sklearnlinearmodel import logisticregression


iris datasetsloadiris


 x train x test ytrain ytest traintestsplitirisdata iristarget testsizy
 e randomstate



 kf kfoldnsplits



 clf logisticregression



 clffitx train ytrain



 score clfscorextrainytrain

 printmetrica del modelo score


 scores crossvalscoreclf x train ytrain cvkf scoringaccuracy

 printmetricas crossvalidation scores

 printmedia de crossvalidation scoresmean 

 preds clfpredictxtest

 scorepred metricsaccuracyscoreytest preds

 printmetrica en test scorepred
en el ejemplo vemos los pasos descritos anteriormente

e cargar el dataset

 dividir en train y test en 

 creamos un modelo de regresión logística podría ser otro y lo entrenamos con los datos de
train

 hacemos crossvalidation usando kfolds con splits

 comparamos los resultados obtenidos en el modelo inicial en el cross validation y vemos que
son similares

 finalmente hacemos predict sobre el conjunto de test y veremos que también obtenemos buen
accuracy

conjunto de entrenamiento test y validación 

más técnicas para validación del modelo
otras técnicas usadas y que nos provee sklearn para python son

stratified kfold

statified kfold es una variante mejorada de kfold que cuando hace los splits las divisiones del
conjunto de train tiene en cuenta mantener equilibradas las clases esto es muy útil porque
imaginen que tenemos que clasificar en sino y si una de las iteraciones del kfold normal
tuviera muestras con etiquetas sólo si el modelo no podría aprender a generalizar y aprenderá
para cualquier input a responder st esto lo soluciona el stratified kfold

leave p out

leave p out selecciona una cantidad p por ejemplo entonces se separarán de a muestras
contra las cuales validar y se iterará como se explico anteriormente si el valor p es pequeño
esto resultará en muchísimas iteraciones de entrenamiento con un alto coste computacional y
seguramente en tiempo si el valor p es muy grande podría contener más muestras que las
usadas para entrenamiento lo cual sería absurdo usar esta técnica con algo de sentido común y
manteniendo un equilibrio entre los scores y el tiempo de entreno

shufflesplit

shufflesplit primero mezcla los datos y nos deja indicar la cantidad de splits divisiones es decir
las iteraciones independientes que haremos y también indicar el tamaño del set de validación

 httpswwwaprendemachinelearningcomclasificacioncon datos desbalanceados
httpsscikitlearnorgstablemodulescrossvalidationhtmlistratifiedkfold
httpsscikitlearnorgstablemodulescrossvalidationhtmléleavepoutipo
shttpsscikitlearnorgstablemodulescrossvalidationhtmlirandompermutationscrossvalidationakashufflesplit

conjunto de entrenamiento test y validación 

series temporales atención al validar

el

para problemas de series temporales tenemos que prestar especial cuidado con los datos pues si
pasamos al modelo dato futuro antes de tiempo estaríamos haciendo data leakage esto es como
si le hiciéramos spoiler al modelo y le contaremos el final de la película antes de que la vea esto
causaría overfitting

al hacer el split inicial de datos estos deberán estar ordenados por fecha y no podemos
mezclarlos

para ayudarnos con el crossvalidation sklearn

nos provee de timeseriessplit
timeseriessplit

timeseriessplit es una variante adaptada de kfolds que evita la fuga de datos para hacerlo va

httpswwwwaprendemachinelearningcompronosticodeseriestemporalesconredes neuronalesenpython
 httpswwwaprendemachinelearningcomqueesoverfitting y underfittingy comosolucionarlo

 httpsscikitlearnorgstablemodulescrossvalidationhtml
httpsscikitlearnorgstablemodulescrossvalidationhtmlitimeseriessplit

conjunto de entrenamiento test y validación 

iterando los folds de a uno usando una ventana de tiempo que se desplaza y usando el fold más
reciente cómo el set de validación se puede entender mejor viendo una animación

timeseriessplit

en amarillo las muestras para entrenar y en verde el conjunto de validación

pero entonces cuando uso crossvalidation

es una buena práctica usar crossvalidation en nuestros proyectos de hecho usarlo nos ayudará a
elegir el modelo correcto y nos da mayor seguridad y respaldo ante nuestra decisión

pero siempre hay un pero

en casos en los que hacer sólo entrenamiento normal tome muchísimo tiempo y recursos podría
ser nuestra perdición imaginen que hacer un kfolds de implica hacer entrenos aunque un
poco más pequeños pero que consumirían mucho tiempo

entonces en la medida de lo posible siempre usar validación cruzada y vuelvo a reforzar el concepto
luego se probará el modelo contra el conjunto de pruebas test

para hacer tuneo de hiperparámetros como randomsearch gridsearch ó tuneo baye
siano es muy útil hacer crossvalidation

conjunto de entrenamiento test y validación 

si ya estoy conforme y quiero llevar el modelo a un
entorno de producción

supongamos que el entrenamiento haciendo cross validation y el predict en test nos están dando
buenos accuracy y similares y estamos conformes con nuestro modelo pues si lo queremos usar

en un entorno real y productivo antes de publicarlo es recomendado que agreguemos el

conjunto de test al modelo pues así estaremos aprovechando el de nuestros datos espero que
esto último también se entienda porque es super importante lo que estoy diciendo es que si al final
de todas nuestras iteraciones pre procesado de dato mejoras de modelo ajuste de hiperparámetros
y comparando con el conjunto de test estamos seguros que el modelo funciona correctamente es
entonces ahora que usaremos las muestras para entrenar al modelo y ese modelo final será
el que publicamos en producción

es una última iteración que debería mejorar el modelo final aunque este no lo podemos contrastar
contra nada excepto con su comportamiento en el entorno real

si esta última iteración te causara dudas no la hagas excepto que tu problema sea de tipo serie

s httpswwwaprendemachinelearningcomtupropioservicio demachinelearning

conjunto de entrenamiento test y validación 

temporal en ese caso sí que es muy importante hacerlo o quedaremos con un modelo que no es
el más actual

resumen

lo más importante que quisiera que quede claro en este capítulo es que entonces tenemos conjuntos
uno de train y otro de test el conjunto de validación no existe como tal si no que vive
temporalmente al momento de entrenar y nos ayuda a obtener al mejor modelo de entre los distintos
que probaremos para conseguir nuestro objetivo esa técnica es lo que se llama validación cruzada
ó en inglés crossvalidation

nota en los ejemplos de la documentación de sklearn podremos ver que usan las
palabras train y test pero conceptualmente se está refiriendo al conjunto de validación y
no al de test que usaremos al final esto es en parte el causante de tanta confusión con
este tema

tener en cuenta el tamaño de split es el usual pero puede ser distinto y esta proporción puede
cambiar sustancialmente las métricas obtenidas del modelo entrenado ojo con eso el tamaño ideal
dependerá del dominio de nuestro problema deberemos pensar en una cantidad de muestras para
test que nos aseguren que estamos el modelo creado está funcionando correctamente teniendo
 registros puede que con testear filas ya estemos conformes ó que necesitemos para
estar megaseguros por supuesto debemos recordar que las filas que estemos quitando para testear
no las estamos usando al entrenar

otro factor al hacer el experimento y tomar las muestras mezcladas mantener la semilla ó no
podremos reproducir el mismo experimento para comparar y ver si mejora o no este suele ser un
parámetro llamado randomstate y está bien que lo usemos para fijarlo

recomendaciones finales

 en principio separar train y test en una proporción de 
 hacer cross validation siempre que podamos

 no usar kfolds usar stratifiedkfolds en su lugar

 la cantidad de folds dependerá del tamaño del dataset que tengamos pero la cantidad
usual es pues es similar al que hacemos con traintest

 para problemas de tipo timeseries usar timeseriessplit

 si el accuracy métrica que usamos es similar en los conjuntos de train donde hicimos
cross validation y test podemos dar por bueno al modelo

httpswwwaprendemachinelearningcompronosticodeventasredesneuronalespythonembeddings

nota inicial

si has adquirido ó descargado este ejemplar primero que nada quiero agradecerte

este libro es un trabajo de gran ilusión y esfuerzo para mi ten en cuenta que lo fui construyendo en
mis tiempos libres entre el trabajo cursar un master cuidar de mis hijos pequeños y una pandemia
de contexto

escribir un artículo lleva desde la idea inicial en mi cabeza a investigar e informarme bien de cada
tema crear un ejercicio original en código python recopilar el conjunto de datos testear crear las
gráficas y redactar el texto y alguna cosilla más editar revisar corregir enlazar difundir pull
push

todos los artículos son versiones corregidas actualizadas y mejoradas de los originales publicados
hasta julio que podrás encontrar en el blog aprende machine learning

espero que sigas en contacto conmigo y si el libro es de tu agrado lo compartas con amigos

repositorio

el código completo y las jupyter notebooks las podrás ver y descargar desde mi repositorio github

 z

tu opinión

todos los comentarios para mejorar el libro son bienvenidos por lo que eres libre de enviarme
sugerencias ó correcciones por las vías que ofrece leanpub ó por twitter en ojbagnato ó por el
formulario de contacto del blog

httpswwwaprendemachinelearningcom
httpsgithubcomjbagnatomachinelearning
httpsleanpubcomaprendemlemailauthornew
httpstwittercomjbagnato
httpswwwaprendemachinelearningcomcontacto

conjunto de entrenamiento test y validación 

recursos adicionales
otros artículos interesantes en inglés

 documentación scikit learn sobre cross validation

 reasons why you should use cross validation
 random forest and kfold cross validation

y ejemplos en código python

httpsscikitlearnorgstablemodulescrossvalidationhtml
 httpstowardsdatasciencecomreasons whyyoushouldusecrossvalidationinyourdatascienceprojectae
s httpswwwkagglecomynourirandomforestkfoldcross validation

kmeans

kmeans es un algoritmo no supervisado de clustering se utiliza cuando tenemos un montón
de datos sin etiquetar el objetivo de este algoritmo es el de encontrar k grupos clusters entre
los datos crudos en este artículo repasaremos sus conceptos básicos y veremos un ejemplo paso a
paso en python

cómo funciona kmeans

el algoritmo trabaja iterativamente para asignar a cada punto las filas de nuestro conjunto de
entrada forman una coordenada uno de los k grupos basado en sus características son agrupados
en base a la similitud de sus features las columnas como resultado de ejecutar el algoritmo
tendremos

 los centroids de cada grupo que serán unas coordenadas de cada uno de los k conjuntos
que se utilizarán para poder etiquetar nuevas muestras

 etiquetas para el conjunto de datos de entrenamiento cada etiqueta perteneciente a uno de
los k grupos formados

los grupos se van definiendo de manera orgánica es decir que se va ajustando su posición en cada
iteración del proceso hasta que converge el algoritmo una vez hallados los centroids deberemos
analizarlos para ver cuales son sus características únicas frente a la de los otros grupos estos grupos
son las etiquetas que genera el algoritmo

casos de uso de kmeans

el algoritmo de clustering kmeans es uno de los más usados para encontrar grupos ocultos o

sospechados en teoría sobre un conjunto de datos no etiquetado esto puede servir para confirmar
 desterrar alguna teoría que teníamos asumida de nuestros datos y también puede ayudarnos a
descubrir relaciones asombrosas entre conjuntos de datos que de manera manual no hubiéramos
reconocido una vez que el algoritmo ha ejecutado y obtenido las etiquetas será fácil clasificar
nuevos valores o muestras entre los grupos obtenidos algunos casos de uso son

 segmentación por comportamiento relacionar el carrito de compras de un usuario sus tiempos
de acción e información del perfil

t httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningnosupervisado
httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearningclustering
httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning

kmeans 

 categorización de inventario agrupar productos por actividad en sus ventas
 detectar anomalías o actividades sospechosas según el comportamiento en una web reconocer
un troll o un bot de un usuario normal

datos de entrada para kmeans

las features o características que utilizaremos como entradas para aplicar el algoritmo kmeans
deberán ser de valores numéricos continuos en lo posible en caso de valores categóricos por ej
hombremujer o ciencia ficción terror novelaetc se puede intentar pasarlo a valor numérico
pero no es recomendable pues no hay una distancia real como en el caso de géneros de película
o libros además es recomendable que los valores utilizados estén normalizados manteniendo una
misma escala en algunos casos también funcionan mejor datos porcentuales en vez de absolutos
no conviene utilizar features que estén correlacionados o que sean escalares de otros

el algoritmo kmeans

el algoritmo utiliza una proceso iterativo en el que se van ajustando los grupos para producir el
resultado final para ejecutar el algoritmo deberemos pasar como entrada el conjunto de datos y un
valor de k el conjunto de datos serán las características o features para cada punto las posiciones
iniciales de los k centroids serán asignadas de manera aleatoria de cualquier punto del conjunto de
datos de entrada luego se itera en dos pasos

 paso de asignación de datos en este paso cada fila de nuestro conjunto de datos se asigna al
centroide más cercano basado en la distancia cuadrada euclideana se utiliza la siguiente fórmula
donde dist es la distancia euclideana standard

argmin distc x
cec

paso de actualización de centroid en este paso los centroid de cada grupo son recalculados esto
se hace tomando una media de todos los puntos asignados en el paso anterior

l
dy es i

el algoritmo itera entre estos pasos hasta cumplir un criterio de detención

 si no hay cambios en los puntos asignados a los grupos

kmeans 

 o si la suma de las distancias se minimiza
 se alcanza un número máximo de iteraciones

el algoritmo converge a un resultado que puede ser el óptimo local por lo que será conveniente
volver a ejecutar más de una vez con puntos iniciales aleatorios para confirmar si hay una salida
mejor

elegir el valor de k

este algoritmo funciona preseleccionando un valor de k para encontrar el número de clusters en
los datos deberemos ejecutar el algoritmo para un rango de valores k ver los resultados y comparar
características de los grupos obtenidos en general no hay un modo exacto de determinar el valor
k pero se puede estimar con aceptable precisión siguiendo la siguiente técnica una de las métricas
usada para comparar resultados es la distancia media entre los puntos de datos y su centroid
como el valor de la media diminuirá a medida de aumentemos el valor de k deberemos utilizar la
distancia media al centroide en función de k y entontrar el punto codo donde la tasa de descenso
se afila aquí vemos una gráfica a modo de ejemplo

elbow point example







elbow point k





average withincluster distance to centroid

 
number of clusters k

ejemplo kmeans con scikitlearn

como ejemplo utilizaremos de entradas un conjunto de datos que obtuve de un proyecto propio en
el que se analizaban rasgos de la personalidad de usuarios de twitter he filtrado a famosos del

rn 

o 




kmeans 

mundo en diferentes areas deporte cantantes actores etc basado en una metodología de psicología
conocida como ocean the big five tendemos como características de entrada

 usuario el nombre en twitter

 op openness to experience grado de apertura mental a nuevas experiencias curiosidad
arte

 co conscientiousness grado de orden prolijidad organización

 ex extraversion grado de timidez solitario o participación ante el grupo social

 ag agreeableness grado de empatía con los demás temperamento

 ne neuroticism grado de neuroticismo nervioso irritabilidad seguridad en sí mismo

 wordcount cantidad promedio de palabras usadas en sus tweets

 categoria actividad laboral del usuario actor cantante etc

utilizaremos el algoritmo kmeans para que agrupe estos usuarios no por su actividad laboral si no
por sus similitudes en la personalidad si bien tenemos columnas de entrada sólo utilizaremos 
en este ejemplo de modo que podamos ver en un gráfico tridimensional y sus proyecciones a d los
grupos resultantes pero para casos reales podemos utilizar todas las dimensiones que necesitemos
una de las hipótesis que podríamos tener es todos los cantantes tendrán personalidad parecida
y así con cada rubro laboral pues veremos si lo probamos o por el contrario los grupos no están
relacionados necesariamente con la actividad de estas celebridades

agrupar usuarios twitter de acuerdo a su
personalidad con kmeans

implementando kmeans en python con sklearn

comenzaremos importando las librerías que nos asistirán para ejecutar el algoritmo y graficar

import pandas as pd

import numpy as np

import matplotlibpyplot as plt

import seaborn as sb

from sklearncluster import kmeans

from sklearnmetrics import pairwisedistancesargminmin

zmatplotlib inline

from mpltoolkitsmplotd import axesd
pltrcparamsfigure figsize 
pltstyleuse ggplot

importamos el archivo csv para simplificar suponemos que el archivo se encuentra en el mismo

directorio que el notebook y vemos los primeros registros del archivo tabulados

 httpwwwaprendemachinelearningcomwpcontentuploadsanalisiscsv

kmeans

dataframe head 

dataframe pdreadcsvranalisiscsv



usuario op co ex ag ne wordcount categoria
 gerardpique 
 aguerosergiokun 
albertochicote 
 alejandrosanz 
 alfredocasero 

dataframe describe 

también podemos ver una tabla de información estadística que nos provee pandas dataframe

op co ex ag ne wordcount categoria
count 
mean 
std 
min 
 
 
 
max 

el archivo contiene diferenciadas categorías actividades

ooon lerona

para saber cuantos registros tenemos de cada uno hacemos

 actoractriz
 cantante

 modelo

 tv series
radio

 tecnología
 deportes

 politica

 escritor

aborales que son

kmeans

printdataframe groupby categoria size

categoria

v oauewwhh

dtype int











como vemos tenemos cantantes actores deportistas políticosetc

visualización de datos

veremos graficamente nuestros datos para tener una idea de la dispersión de los mismos

dataframe drop categoriahist
pltshow



a b

s



wordcount

kmeans 

en este caso seleccionamos dimensiones op ex y ag y las cruzamos para ver si nos dan alguna
pista de su agrupación y la relación con sus categorías
sbpairplotdataframedropna huecategoriasizevarsop
catter

ex agkindsy

categoria

ssescsccc
oaualn

revisando la gráfica no pareciera que hay algún tipo de agrupación o correlación entre los usuarios
y sus categorías

n

o

l













kmeans 

definimos la entrada

concretamos la estructura de datos que utilizaremos para alimentar el algoritmo como se ve sólo
cargamos las columnas op ex y ag en nuestra variable x

x nparraydataframe op exag
y nparraydataframe categoria
xshape

 python



ahora veremos una gráfica en d con colores representando las categorías
 python
fig plt figure
ax axesd fig
coloresblue red green blue cyan yellow orange black pink brown n
 purple
asignar
for row in y
asignar appendcolores row
axscatter x x x casignar s

d



l



kmeans 

veremos si con kmeans podemos pintar esta misma gráfica de otra manera con clusters
diferenciados

obtener el valor k

vamos a hallar el valor de k haciendo una gráfica e intentando hallar el punto de codo que
comentábamos antes este es nuestro resultado

nc range 

kmeans kmeansnclustersi for i in nc

kmeans

score kmeansifitxscorex for i in range len kmeans
score

pltplotncscore

pltxlabel number of clusters

pltylabel score

plttitle elbow curve

pltshow

qué es el machine learning

veamos algunas definiciones existentes sobre machine learning para intentar dar comprensión a
esta revolucionaria materia

definiendo machine learning

el machine learning traducido al español como aprendizaje automático ó aprendizaje de
máquinas es un subcampo de la inteligencia artificial que busca resolver el cómo construir
programas de computadora que mejoran automáticamente adquiriendo experiencia

esta definición implica que el programa que se crea con ml no necesita que el programador indique
explicitamente las reglas que debe seguir para lograr su tarea si no que este mejora automáticamente

en los últimos años han surgido grandes volúmenes de datos de diversas fuentes públicas big data y
el aprendizaje automático relacionado al campo estadístico consiste en extraer y reconocer patrones
y tendencias para comprender qué nos dicen los datos para ello se vale de algoritmos que pueden
procesar gygas yo terabytes en tiempos razonables y obtener información útil

z 

una definición técnica

podemos encontrar la siguiente definición técnica sobre aprendizaje automático

a computer program is said to learn from experience e with respect to some class of tasks
tand performance measure p ifits performance at tasks in t as measured by p improves
wwith experience e

la experiencia e hace referencia a grandes volúmenes de datos recolectados big data para la toma
de decisiones t y la forma de medir su desempeño p para comprobar que esos algoritmos mejoran
con la adquisición de más experiencia

diagrama de venn

drew conway creó un simpático diagrama de venn en el que inerrelaciona diversos campos aquí
copio su versión al español

kmeans 

elbow curve

number of clusters

realmente la curva es bastante suave considero a como un buen número para k según vuestro
criterio podría ser otro

ejecutamos kmeans

ejecutamos el algoritmo para clusters y obtenemos las etiquetas y los centroids

kmeans kmeansnclustersfitx
centroids kmeansclustercenters

printcentroids
 
 
 
 
 

ahora veremos esto en una gráfica d con colores para los grupos y veremos si se diferencian las
estrellas marcan el centro de cada cluster

n

o

o






kmeans

 predicting the clusters
labels kmeanspredictx
 getting the cluster centers
c kmeansclustercenters
coloresred green blue cyan yellow
asignar
for row in labels
asignar appendcolores row

fig plt figure
ax axesd fig
axscatter x x x casignar s

axscatterc c c marker ccolores s





aqui podemos ver que el algoritmo de kmeans con k ha agrupado a los usuarios twitter
por su personalidad teniendo en cuenta las dimensiones que utilizamos openess extraversion y

agreeablenes pareciera que no hay necesariamente una relación en los grupos con sus actividades

de celebrity haremos gráficas en dimensiones con las proyecciones a partir de nuestra gráfica

d para que nos ayude a visualizar los grupos y su clasificación

n

 a

 s

 a

kmeans

 getting the values and plotting it
f dataframe opvalues
f dataframe exvalues

pltscatterf f casignar s
pltscatterc c marker ccolores s
pltshow

l 
 
 
 
s 
 
 or p 
 
 í 
 
 
 r 
be o
 í 
 
 

 

 getting the values and plotting it
f dataframe opvalues
f dataframe agvalues

pltscatterf f casignar s
pltscatterc c marker ccolores s
pltshow

eo n 

o

o

kmeans



 
o
d o c
j f 
 e
o r 
 
c a 
 rik r 
 
 n c q r 




dataframe ex values

dataframe ag values

pltscatterf f casignar s

pltscatterc c marker

pltshow

cecolores s

n

o

l

kmeans 

 r 
 ooo o 
 o o 
 
l ji e 
 

en estas gráficas vemos que están bastante bien diferenciados los grupos podemos ver cada uno de
los clusters cuantos usuarios tiene

copy pddataframe
copy usuario dataframe usuario values
copy categoria dataframe categoria values
copylabel labels
pddataframe
cantidadgrupocolorcolores
cantidadgrupocantidad copy groupby labelsize
cantidadgrupo

cantidadgrupo

cantidad

n

o

o

kmeans 

y podemos ver la diversidad en rubros laborales de cada uno por ejemplo en el grupo rojo vemos
que hay de todas las actividades laborales aunque predominan de actividad y correspondiente a
actores y cantantes con y famosos

groupreferrerindex copylabel 
groupreferrals copygroupreferrerindex

diversidadgrupo pddataframe 
diversidadgrupocategoria
diversidadgrupocantidad groupreferralsgroupby categoria size 
diversidadgrupo

categoria cantidad

ojo nan
 
 
 
 

de categoría modelos hay sobre un total de buscaremos los usuarios que están más cerca a los
centroids de cada grupo que podríamos decir que tienen los rasgos de personalidad característicos
que representan a cada cluster

vemos el representante del grupo el usuario cercano a su centroid
closest pairwisedistancesargminminkmeansclustercenters x
closest

array posicion en el array de usuarios

ne

a

d

a

kmeans 

usersdataframe usuario values
for row in closest
printusers row

carmenelectra pabloiglesias judgejudy jpvarsky kobebryant

en los centros vemos que tenemos una modelo un político presentadora de tv locutor de radio y
un deportista

clasificar nuevas muestras

y finalmente podemos agrupar y etiquetar nuevos usuarios twitter con sus características y
clasificarlos vemos el ejemplo con el usuario de david guetta y nos devuelve que pertenece al
grupo verde

xnew nparray davidguetta

newlabels kmeanspredictxnew
printnewlabels



resumen

el algoritmo de kmeans nos ayudará a crear clusters cuando tengamos grandes grupos de datos
sin etiquetar cuando queramos intentar descubrir nuevas relaciones entre features o para probar o
declinar hipótesis que tengamos de nuestro negocio

atención puede haber casos en los que no existan grupos naturales o clusters que contengan
una verdadera razón de ser si bien kmeans siempre nos brindará k clusters quedará en nuestro
criterio reconocer la utilidad de los mismos o bien revisar nuestras features y descartar las que no
sirven o conseguir nuevas

también tener en cuenta que en este ejemplo estamos utilizando como medida de similitud entre
features la distancia euclideana pero podemos utilizar otras diversas funciones que podrían
arrojar mejores resultados como manhattan lavenshtein mahalanobis etc hemos visto
una descripción del algoritmo aplicaciones y un ejemplo python paso a paso que podrán descargar
también desde los siguientes enlaces

p httpseswikipediaorgwikidistanciaeuclidiana

 httpseswikipediaorgwikigeometrcadadeltaxista
httpsenwikipediaorgwikilevenshteindistance

 httpsenwikipediaorgwikimahalanobisdistance

kmeans 



 notebook jupiter online
 descargar archivo csv y notebook ejercicio kmeans
 visualizar y descargar desde jbagnato github



httpnbviewerjupyterorggithubjbagnatomachinelearningblobmasterejerciciokmeansipynb
httpwwwaprendemachinelearningcomwpcontentuploadsanalisiscsv
httpwwwaprendemachinelearningcomwpcontentuploadsejerciciokmeansipynb
httpsgithubcomjbagnatomachinelearningblobmasterejerciciokmeansipynb

 httpsgithubcomjbagnatomachinelearning

knearestneighbor

knearestneighbor es un algoritmo basado en instancia de tipo supervisado de machine

learning puede usarse para clasificar nuevas muestras valores discretos o para predecir regresión
valores continuos al ser un método sencillo es ideal para introducirse en el mundo del aprendizaje
automático sirve esencialmente para clasificar valores buscando los puntos de datos más similares
por cercanía aprendidos en la etapa de entrenamiento y haciendo conjeturas de nuevos puntos
basado en esa clasificación

a diferencia de kmeans que es un algoritmo no supervisado y donde la k significa la cantidad
de grupos clusters que deseamos clasificar en knearest neighbor la k significa la cantidad
de puntos vecinos que tenemos en cuenta en las cercanías para clasificar los n grupos que ya se
conocen de antemano pues es un algoritmo supervisado

qué es el algoritmo knearest neighbor 

es un método que simplemente busca en las observaciones más cercanas a la que se está tratando de
predecir y clasifica el punto de interés basado en la mayoría de datos que le rodean como dijimos
antes es un algoritmo

 supervisado esto brevemente quiere decir que tenemos etiquetado nuestro conjunto de datos
de entrenamiento con la clase o resultado esperado dada una fila de datos

 basado en instancia esto quiere decir que nuestro algoritmo no aprende explícitamente un
modelo como por ejemplo en regresión logística o árboles de decisión en cambio memoriza
las instancias de entrenamiento que son usadas como base de conocimiento para la fase de
predicción

dónde se aplica knearest neighbor

aunque sencillo se utiliza en la resolución de multitud de problemas como en sistemas de
recomendación búsqueda semántica y detección de anomalías

 httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearninginstancia
httpwwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado

 httpwwwaprendemachinelearningcomkmeansenpythonpasoapaso
httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningnosupervisado

 httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearningclustering

knearestneighbor 

pros y contras

como pros tiene sobre todo que es sencillo de aprender e implementar tiene como contras que
utiliza todo el dataset para entrenar cada punto y por eso requiere de uso de mucha memoria
y recursos de procesamiento cpu por estas razones knn tiende a funcionar mejor en datasets
pequeños y sin una cantidad enorme de features las columnas

cómo funciona knn

 calcular la distancia entre el item a clasificar y el resto de items del dataset de entrenamiento

 seleccionar los k elementos más cercanos con menor distancia según la función que se use

 realizar una votación de mayoría entre los k puntos los de una claseetiqueta que decidirán su clasificación final

teniendo en cuenta el punto veremos que para decidir la clase de un punto es muy importante
el valor de k pues este terminará casi por definir a qué grupo pertenecerán los puntos sobre
todo en las fronteras entre grupos por ejemplo y a priori yo elegiría valores impares de k para
desempatar si las features que utilizamos son pares no será lo mismo tomar para decidir valores
que esto no quiere decir que necesariamente tomar más puntos implique mejorar la precisión
lo que es seguro es que cuantos más puntos k más tardará nuestro algoritmo en procesar y
darnos respuesta las formas más populares de medir la cercanía entre puntos son la distancia
euclidiana la de siempre o la cosine similarity mide el ángulo de los vectores cuanto
menores serán similares recordemos que este algoritmo y prácticamente todos en ml funcionan
mejor con varias características de las que tomemos datos las columnas de nuestro dataset lo
que entendemos como distancia en la vida real quedará abstracto a muchas dimensiones que no
podemos visualizar fácilmente como por ejemplo en un mapa

un ejemplo knearest neighbor en python

exploremos el algoritmo con scikit learn

realizaremos un ejercicio usando python y su librería scikitlearn que ya tiene implementado el
algoritmo para simplificar las cosas veamos cómo se hace

el ejercicio app reviews

para nuestro ejercicio tomaremos registros con opiniones de usuarios sobre una app reviews
utilizaremos columnas de datos como fuente de alimento del algoritmo recuerden que sólo tomaré

qué es el machine learning 

diagrama de venn

en esta aproximación al ml podemos ver que es una intersección entre conocimientos de matemá
ticas y estadística con habilidades de hackeo del programador

aproximación para programadores

los programadores sabemos que los algoritmos de búsqueda pueden tomar mucho tiempo en
concluir y que cuanto mayor sea el espacio de búsqueda crecerán exponencialmente las posibilidades
de combinación de una respuesta óptima haciendo que los tiempos de respuesta tiendan al infinito
o que tomen más tiempo de lo que un ser humano pueda tolerar por quedarse sin vida o por
impaciencia

para poder resolver este tipo de situaciones surgen soluciones de tipo heurísticas que intentan dar
intuición al camino correcto a tomar para resolver un problema estos logran buenos resultados
en tiempos menores de procesamiento pero muchas veces su intuición es arbitraria y pueden fallar

d



l










knearestneighbor 

 features para poder graficar en dimensiones pero para un problema en la vida real conviene
tomar más características de lo que sea que queramos resolver esto es únicamente con fines de
enseñanza las columnas que utilizaremos serán wordcount con la cantidad de palabras utilizadas
y sentimentvalue con un valor entre y que indica si el comentario fue valorado como positivo
o negativo nuestras etiquetas serán las estrellas que dieron los usuarios a la app que son valores
discretos del al podemos pensar que si el usuario puntúa con más estrellas tendrá un sentimiento
positivo pero no necesariamente siempre es así

comencemos con el código

primero hacemos imports de librerías que utilizaremos para manejo de datos gráficas y nuestro
algoritmo

import pandas as pd

import numpy as np

import matplotlibpyplot as plt

from matplotlibcolors import listedcolormap
import matplotlibpatches as mpatches

import seaborn as sb

zmatplotlib inline
pltrcparamsfigure figsize 
pltstyleuse ggplot

from sklearnmodelselection import traintestsplit
from sklearnpreprocessing import minmaxscaler

from sklearnneighbors import kneighborsclassifier
from sklearnmetrics import classificationreport
from sklearnmetrics import confusionmatrix

cargamos el archivo entrada csv con pandas usando separador de punto y coma pues en las reviews
hay textos que usan coma con head vemos los primeros registros

dataframe pdreadcsvrreviewssentimentcsvsep
dataframe head

knearestneighbor



review title review text wordcount titlesentiment textsentiment star rating sentimentvalue
 sin conexión hola desde hace algo más de un mes me pone sin negative negative 
 faltan cosas han mejorado la apariencia pero no negative negative 
 es muy buena lo recomiendo andres e puto amooo nan negative 
 version antigua me gustana mas la version anterior esta es mas nan negative 
 esta bien sin ser la biblia esta bien negative negative 
 buena nada del otro mundo pero han mejorado mucho positive negative 
 de gran ayuda lo malo q necesita de pero la app es muy buena positive negative 
 muy buena estaba más acostumbrado al otro diseño pero e positive negative 
 ta to guapa va de escándalo positive negative 
 se han corregido han corregido muchos fallos pero el diseño es negative negative 

aprovechamos a ver un resumen estadístico de los datos

dataframe describe 

wordcount star rating sentimentvalue

count 

mean 
std 

min 

 
 
 

max 


























son registros las estrellas lógicamente vemos que van del al la cantidad de palabras van
de sóla hasta y las valoraciones de sentimiento están entre y con una media de 

y a partir del desvío estándar podemos ver que la mayoría están entre y 

un poco de visualización

veamos unas gráficas simples y qué información nos aportan

dataframehist
pltshow

knearestneighbor 

star rating sentimentvalue

 

wordcount

vemos que la distribución de estrellas no está balanceada esto no es bueno convendría tener
las mismas cantidades en las salidas para no tener resultados tendenciosos para este ejercicio lo
dejaremos así pero en la vida real debemos equilibrarlos la gráfica de valores de sentimientos
parece bastante una campana movida levemente hacia la derecha del cero y la cantidad de palabras
se centra sobre todo de a veamos realmente cuantas valoraciones de estrellas tenemos

 printdataframegroupby star ratingsize

star rating









dtype int

n e w

con eso confirmamos que hay sobre todo de y estrellas y aqui una gráfica más bonita

 sbfactorplot star ratingdatadataframekindcount aspect

knearestneighbor 

 





 t q t 
 

star rating

count

graficamos mejor la cantidad de palabras y confirmamos que la mayoría están entre y palabras

 sbfactorplotwordcountdatadataframekindcount aspect

iiiiiiiiiiii iiiiiiiiiiiiii

 
wordcount

count
 t

u

preparamos las entradas

creamos nuestro x e y de entrada y los sets de entrenamiento y test

dataframe wordcount sentimentvalue values
dataframe star ratingvalues

scaler minmaxscaler
xtrain scalerfittransformxtrain




 x train x test y train ytest traintestsplitx y randomstate


 x test scalertransformxtest

d



l

ne

a

knearestneighbor 

usemos knearest neighbor con scikit learn

definimos el valor de k en esto realmente lo sabemos más adelante ya veréis y creamos nuestro
clasificador

nneighbors 

knn kneighborsclassifier nneighbors

knnfitxtrain ytrain

printaccuracy of knn classifier on training set f
 format knnscorextrain y train

printaccuracy of knn classifier on test set f
 formatknnscorextest ytest

accuracy of knn classifier on training set 
accuracy of knn classifier on test set 

vemos que la precisión que nos da es de en el set de entrenamiento y del para el de test

nota como verán utilizamos la clase kneighborsclassifier de scikit learn puesto
que nuestras etiquetas son valores discretos estrellas del al pero deben saber que
también existe la clase kneighborsregressor para etiquetas con valores continuos

precisión del modelo

confirmemos la precisión viendo la confusión matrix y el reporte sobre el conjunto de test que
nos detalla los aciertos y fallos

pred knnpredictxtest
printconfusion matrixytest pred
printclassificationreportytest pred

httpscikitlearnorgstablemodulesgeneratedsklearnneighborskneighborsclassifierhtmlisklearnneighborskneighborsclassifier
httpscikitlearnorgstablemodulesgeneratedsklearnneighborskneighborsregressor htmlsklearnneighborskneighborsregressor
shttpwwwaprendemachinelearningcomclasificacioncondatos desbalanceados

n

o

l











knearestneighbor 

 
 
 
 
 

precision recall flscore support

 

 

 

 

 

avg total 

cómo se ve la puntuación f es del bastante buena nota recuerden que este es sólo un
ejercicio para aprender y tenemos muy pocos registros totales y en nuestro conjunto de test por
ejemplo de estrellas sólo tiene valoración y esto es evidentemente insuficiente

y ahora la gráfica que queríamos ver

ahora realizaremos la grafica con la clasificación obtenida la que nos ayuda a ver fácilmente en
donde caerán las predicciones nota al ser features podemos hacer la gráfica d y si fueran 
podría ser en d pero para usos reales podríamos tener más de dimensiones y no importaría poder
visualizarlo sino el resultado del algoritmo

h step size in the mesh

 create color maps
cmaplight listedcolormap ffaaaa ffcc ffffbbffffcfc
cmapbold listedcolormap ff ffffffoo ooffffffoo

 we create an instance of neighbours classifier and fit the data
clf kneighborsclassifier nneighbors weightsdistance
clffitx y

 plot the decision boundary for that we will assign a color to each
 point in the mesh xmin xmaxxymin y max
xxmin x max x min x max 
y min y max x min x max 
xx yy npmeshgridnparangexmin xmax h
nparangeymin ymax h
z clfpredictnpcxxravel yy ravel

knearestneighbor

 put the result into a color plot
 zreshapexxshape
pltfigure

 plot also the training points

edgecolork s
pltxlimxxmin xxmax
pltylimyymin yymax

patch mpatchespatchcolorff 
patch mpatchespatchcolorff
patch mpatchespatchcolorffff
patch mpatchespatchcolorffff
patch mpatchespatchcolorff 

ó

p

plttitleclass classification k i

 nneighbors weights

pltshow

pltpcolormeshxx yy z cmapcmaplight

pltscatterx x cy cmapcmapbold

label
label
label
label
label

 weights

tlegendhandlespatch patch patch patchpatch

s

knearestneighbor 

class classification k weights distance

erwne

vemos las zonas en las que se relacionan cantidad de palabras con el valor de sentimiento de la
review que deja el usuario se distinguen regiones que podríamos dividir así

class classification k weights distance

erwne

 o
 aíñzgaieni ai i

 s e

 


 



es decir que a ojo una review de palabras y sentimiento nos daría una valoración de zona

knearestneighbor 

celeste con estas zonas podemos intuir ciertas características de los usuarios que usan y valoran la
app

 los usuarios que ponen estrella tienen sentimiento negativo y hasta palabras

 los usuarios que ponen estrellas dan muchas explicaciones hasta palabras y su
sentimiento puede variar entre negativo y algo positivo

 los usuarios que ponen estrellas son bastante neutrales en sentimientos puesto que están en
torno al cero y hasta unas palabras

 los usuarios que dan estrellas son bastante positivos de en adelante aproximadamente
y ponen pocas palabras hasta 

elegir el mejor valor de k

sobre todo importante para desempatar o elegir los puntos
frontera

antes vimos que asignamos el valor nneighbors como valor de kk y obtuvimos buenos
resultados pero de donde salió ese valor pues realmente tuve que ejecutar este código que viene
a continuación donde vemos distintos valores k y la precisión obtenida

krange range 
scores 
for k in krange
knn kneighborsclassifier nneighbors k
knnfitxtrain ytrain
scores appendknnscorextest y test
pltfigure
pltxlabelk
pltylabel accuracy
pltscatterkrange scores
pltxticks

knearestneighbor 

en la gráfica vemos que con valores k a k es donde mayor precisión se logra

clasificar ó predecir nuevas muestras

ya tenemos nuestro modelo y nuestro valor de k ahora lo lógico será usarlo pues supongamos que
nos llegan nuevas reviews veamos como predecir sus estrellas de maneras la primera

printclfpredict 


este resultado nos indica que para palabras y sentimiento nos valorarán la app con estrellas
pero también podríamos obtener las probabilidades que de nos den o estrellas con predict

proba

printclfpredictproba 

 

aquí vemos que para las coordenadas hay probabilidades que nos den estrellas puedes
comprobar en el gráfico anterior que encajan en las zonas que delimitamos anteriormente

qué es el machine learning 

los algoritmos de ml intentan utilizar menos recursos computacionales para entrenar grandes
volúmenes de datos e ir aprendiendo por sí mismos podemos dividir el ml en grandes categorías
aprendizaje supervisado o aprendizaje no supervisado hay una tercer categoría llamada apren
dizaje por refuerzo pero no será tratada en este libro

entre los algoritmos más utilizados en inteligencia artificial encontramos

 arboles de decisión

 regresión lineal

 regresión logística

 k nearest neighbor

 pca principal component analysis

 svm

 gaussian naive bayes

 kmeans

 redes neuronales artificiales

 aprendizaje profundo ó deep learning

una mención especial a las redes neuronales artificiales

una mención distintiva merecen las rnas ya que son algoritmos que imitan al comportamiento de
las neuronas humanas y su capacidad de sinápsis para la obtención de resultados interrelacionando
diversas capas de neuronas para darle mayor poder de aprendizaje

aunque este código existe desde hace más de años en la última década han evolucionado
notoriamente en paralelo a la mayor capacidad tecnológica de procesamiento memoria ram y
disco la nube etc y están logrando impresionantes resultados para analizar textos y síntesis de
voz traducción automática de idiomas procesamiento de lenguaje natural visión artificial análisis
de riesgo clasificación y predicción y la creación de motores de recomendación

resumen

el machine learning es una nueva herramienta clave que posibilitará el desarrollo de un futuro
mejor para la humanidad brindando inteligencia a robots coches y hogares las smart cities el
iot internet of things ya se está volviendo una realidad y también las aplicaciones de machine
learning en asistentes como siri las recomendaciones de netflix o sistemas de navegación autónoma
en drones para los ingenieros o informáticos es una disciplina fundamental para modelar construir
y transitar este nuevo futuro

knearestneighbor 

resumen

en este ejercicio creamos un modelo con python para procesar y clasificar puntos de un conjunto de
entrada con el algoritmo knearest neighbor cómo su nombre en inglés lo dice se evaluán los k
vecinos más cercanos para poder clasificar nuevos puntos al ser un algoritmo supervisado debemos
contar con suficientes muestras etiquetadas para poder entrenar el modelo con buenos resultados
este algoritmo es bastante simple y como vimos antes necesitamos muchos recursos de memoria
y cpu para mantener el dataset vivo y evaluar nuevos puntos esto no lo hace recomendable para
conjuntos de datos muy grandes en el ejemplo sólo utilizamos dimensiones de entrada para poder
graficar y ver en dos dimensiones cómo se obtienen y delimitan los grupos finalmente pudimos
hacer nuevas predicciones y a raíz de los resultados comprender mejor la problemática planteada

recursos y enlaces

 descarga la jupyter notebook y el archivo de entrada csv
 ó puedes visualizar online
 o ver y descargar desde mi cuenta github

más artículos de interés sobre knearest neighbor en inglés

 implementing knn in scikit learn

 introduction to knn
 complete guide to knn

 solving a simple classification problem with python

httpwwwaprendemachinelearningcomwpcontentuploadsejercicioknearestneighboripynb

 httpwwwaprendemachinelearningcomwpcontentuploadsreviewssentimentcsv

httpnbviewer jupyterorggithubjbagnatomachinelearningblobmasterejercicioknearestneighboripynb
httpsgithubcomjbagnatomachinelearning
httpstowardsdatasciencecomimplementingknearestneighborswithscikitlearneeea

 httpstowardsdatasciencecomintroductiontoknearestneighborsbbbd
httpskevinzakkagithubioknearestneighbor

 httpstowardsdatasciencecomsolvingasimpleclassification problemwithpythonfruitslovers editiondabbd

naive bayes comprar casa o
alquilar

en este capítulo veremos un ejercicio práctico utilizando naive bayes intentando llevar los algo
ritmos de machine learning a un ejemplo de la vida real repasaremos la teoría del teorema de
bayes de estadística para poder tomar una decisión muy importante me conviene comprar
casa ó alquilar

veamos si la ciencia de datos nos puede ayudar a resolver el misterio si alquilo casa
estoy tirando el dinero a la basura ó es realmente conveniente pagar una hipoteca
durante el resto de mi vida

si bien tocaremos el tema superficialmente sin meternos en detalles como taza de interés de
hipotecas variablefija comisiones de bancos etc haremos un planteo genérico para obtener
resultados y tomar la mejor decisión dada nuestra condición actual en otros capítulos vimos diversos
algoritmos supervisados del aprendizaje automático que nos dejan clasificar datos yo obtener
predicciones o asistencia a la toma de decisiones árbol de decisión regresión logística y lineal
red neuronal por lo general esos algoritmos intentan minimizar algún tipo de coste iterando las
entradas y las salidas y ajustando internamente las pendientes ó pesos para hallar una salida
esta vez el algoritmo que usaremos se basa completamente en teoría de probabilidades y resultados
estadísticos será suficiente el teorema de bayes para obtener buenas decisiones veamos

los datos de entrada

importemos las librerías que usaremos y visualicemos la información que tenemos de entrada

httpswwwyoutubecomwatchvvxwouco

 httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado
httpwwwaprendemachinelearningcomqueesmachinelearning

 httpwwwaprendemachinelearningcomarbolde decisionenpythonclasificacionyprediccion
httpwwwaprendemachinelearningcomregresionlogisticaconpythonpasoapaso
httpwwwaprendemachinelearningcomregresionlinealenespanolconpython

 httpwwwaprendemachinelearningcomunasencillaredneuronalenpythonconkerasytensorflow
httpseswikipediaorgwikiteoremadebayes

ne



l









naive bayes comprar casa o alquilar 

import pandas as pd

import numpy as np

import matplotlibpyplot as plt
from matplotlib import colors
import seaborn as sb

zmatplotlib inline
pltrcparamsfigure figsize 
pltstyleuse ggplot

from sklearnmodelselection import traintestsplit
from sklearnmetrics import classificationreport
from sklearnmetrics import confusionmatrix

from sklearnnaivebayes import gaussiannb

from sklearn featureselection import selectkbest

carguemos la información sobre inmuebles del archivo csv

dataframe pdreadcsvrcompraralquilarcsv
dataframe head

ingresos qgastos comunes pago coche gastos otros ahorros vivienda estado civil hijos trabajo comprar
click to expand output double click to hide output

o uuu tuuu lo 
 o
 
 o o o
 o o 
 
 s 
 o o o
 o 
 

las columnas que tenemos son

 ingresos los ingresos de la familia mensual

 gastos comunes pagos de luz agua gas etc mensual

 pago coche si se está pagando cuota por uno o más coches y los gastos en combustible etc al
mes

 httpwwwaprendemachinelearningcomwpcontentuploadscompraralquilarcsv
 httpwwwaprendemachinelearningcomwpcontentuploadsbayesentradaspng

naive bayes comprar casa o alquilar 

 gastosotros compra en supermercado y lo necesario para vivir al mes
 ahorros suma de ahorros dispuestos a usar para la compra de la casa

 vivienda precio de la vivienda que quiere comprar esa familia

 estado civil

 osoltero
 casado
 divorciado

hijos cantidad de hijos menores y que no trabajan
 trabajo

 sin empleo

 autónomo freelance

 empleado

 empresario

 pareja autónomos

 pareja empleados

 pareja autónomo y asalariado

 parejaempresario y autónomo

 pareja empresarios los dos o empresario y empleado
 comprar no comprar comprar esta será nuestra columna de salida

algunos supuestos para el problema formulado

 está planteado en euros pero podría ser cualquier otra moneda

 notiene en cuenta ubicación geográfica cuando sabemos que dependerá mucho los precios de
los inmuebles de distintas zonas

 se supone una hipoteca fija a años con interés de mercado bajo

con esta información queremos que el algoritmo aprenda y que como resultado podamos consultar
nueva información y nos dé una decisión sobre comprar o alquilar casa

el teorema de bayes

el teorema de bayes es una ecuación que describe la relación de probabilidades condicionales

de cantidades estadísticas en clasificación bayesiana estamos interesados en encontrar la proba
bilidad de que ocurra una clase dadas unas características observadas datos lo podemos escribir
como p clase datos el teorema de bayes nos dice cómo lo podemos expresar en términos de
cantidades que podemos calcular directamente

httpseswikipediaorgwikiteoremadebayes
shttpseswikipediaorgwikiclasificadorbayesianoingenuo

naive bayes comprar casa o alquilar 

pdatosclase pclase

pclase datos 
pdatos



 clase es una salida en particular por ejemplo comprar

 datos son nuestras características en nuestro caso los ingresos gastos hijos etc
pclasedatos se llama posterior y es el resultado que queremos hallar
pdatosclase se llama verosimilitud en inglés likelihood

pclase se llama anterior pues es una probabilidad que ya tenemos

pdatos se llama probabilidad marginal

si estamos tratando de elegir entre dos clases como comprar ó alquilar entonces una manera de
tomar la decisión es calcular la tasa de probabilidades a posterior

httpwwwaprendemachinelearningcomwpcontentuploadsteoremabayespng

naive bayes comprar casa o alquilar 

pcomprar datos pdatoscomprar pcomprar

palquilar datos pdatosalquilar palquilar



con esta maniobra nos deshacemos del denominador de la ecuación anterior pdatos el llamado
probabilidad marginal

clasificador gaussian naive bayes

uno de los tipos de clasificadores más populares es el llamado en inglés gaussian naive bayes
classifier

veamos cómo es su fórmula para comprender este curioso nombre aplicaremos clases comprar
alquilar y tres características ingresos ahorros e hijos

 httpwwwaprendemachinelearningcomwpcontentuploadsteoremabayespng
 httpseswikipediaorgwikiclasificadorbayesianoingenuo

naive bayes comprar casa o alquilar 

pcomprarpingresoscomprarpahorroscomprar phijoscomprar

posteriorcomprar l o

probabilidad marginal

palquilarpingresosjalquilarpahorrosalquilar phijosalquilar

posterioralquilar 

probabilidad marginal



posterior de comprar es lo que queremos hallar pcomprardatos explicaremos los demás

 pcomprar es la probabilidad que ya tenemos es sencillamente el número de veces que se
selecciona comprar en nuestro conjunto de datos dividido el total de observaciones en
nuestro caso luego lo veremos en python son 

 plingresoscomprarpahorroscomprarphijoscomprar es la verosimilitud los nombres

gaussian y naive ingenuo del algoritmo vienen de dos suposiciones
 asumimos que las características de la verosimilitud no estan correlacionada entre ellas

esto seria que los ingresos sean independientes a la cantidad de hijos y de los ahorros
como no es siempre cierto y es una suposición ingenua es que aparece en el nombre
naive bayes

 asumimos que el valor de las características ingresos hijos etc tendrá una distribución
normal gaussiana esto nos permite calcular cada parte pingresoscomprar usando la

función de probabilidad de densidad normal
e probabilidad marginal muchas veces es difícil de calcular sin embargo por la ecuación que

vimos más arriba no la necesitaremos para obtener nuestro valor a posterior esto simplifica
los cálculos

fin de la teoría sigamos con el ejercicio ahora toca visualizar nuestras entradas y programar un
poquito

visualización de datos

veamos qué cantidad de muestras de comprar o alquilar tenemos

 httpwwwaprendemachinelearningcomwpcontentuploadsbayesposterioripng
 httpseswikipediaorgwikidistribucicbnnormal
httpseswikipediaorgwikifuncicbndedensidaddeprobabilidad

naive bayes comprar casa o alquilar 

printdataframe groupby comprarsize

comprar dtype int

esto son entradas en las que se recomienda comprar y en las que no hagamos un histograma
de las características quitando la columna de resultados comprar

dataframe drop comprar axishist
pltshow

ahorros estadocivil financiar







pareciera a grandes rasgos que la distribución de hijos e ingresos se parece un poco a una distribución
normal

preparar los datos de entrada

procesemos algunas de estas columnas por ejemplo podríamos agrupar los diversos gastos también
crearemos una columna llamada financiar que será la resta del precio de la vivienda con los ahorros
de la familia

 httpwwwaprendemachinelearningcomwpcontentuploadsbayeshistogrampng

naive bayes comprar casa o alquilar

ne

a

pagocoche

dataframe financiar dataframe vivienda dataframe ahorros



dataframe gastos dataframe gastoscomunes data frame gastosotros dataframe y

 dataframedrop gastoscomunes gastosotrospagocoche axishead



ingresos












ahorros





















vivienda estadocivil













o o



hijos trabajo comprar

e o o

o o











o



gastos












financiar























y ahora veamos un resumen estadístico que nos brinda la librería pandas con describe

 reduced dataframedrop gastoscomunes gastosotros pagocoche axis

 reduceddescribe

ingresos ahorros vivienda estado civil hijos trabajo comprar gastos financiar
count 
mean 
std 
min 
 
 
 
max 

httpwwwaprendemachinelearningcomwpcontentuploadsbayespreprocesapng
 httpwwwaprendemachinelearningcomwpcontentuploadsbayesstatspng

d



l

d



l

naive bayes comprar casa o alquilar 

feature selection ó selección de características

en este ejercicio haremos feature selection para mejorar nuestros resultados con este algoritmo

en vez de utilizar las columnas de datos de entrada que tenemos vamos a utilizar una clase de
sklearn llamada selectkbest con la que seleccionaremos las mejores características y usaremos
sólo esas

xdataframe drop comprar axis
ydataframe comprar 

bestselectkkbest k

xnew bestfittransformx y
xnewshape

selected bestgetsupport indicestrue
printxcolumns selected

index ingresos ahorros hijos trabajo financiar dtypeobject

ien u ísti u ue má 
bien entonces usaremos de las características que teníamos las que más aportan al momento
de clasificar veamos qué grado de correlación tienen

usedfeatures xcolumnsselected

colormap pltcmviridis

pltfigure figsize

plttitle pearson correlation of features y size

sbheatmap dataframe usedfeatures astype floatcorrlinewidthsvmax sqn

uaretrue cmapcolormap linecolorwhite annottrue

httpscikitlearnorgstablemodulesfeatureselectionhtml

instalar el ambiente de desarrollo
python

para programar tu propia máquina de inteligencia artificial necesitarás tener listo tu ambiente de
desarrollo local en tu computadora de escritorio o portátil en este capítulo explicaremos una manera
sencilla de obtener python y las librerías necesarias para programar como un científico de datos y
poder utilizar los algoritmos más conocidos de machine learning

por qué instalar python y anaconda en mi ordenador

python es un lenguaje sencillo rápido y liviano y es ideal para aprender experimentar practicar y
trabajar con machine learning redes neuronales y aprendizaje profundo

utilizaremos la suite gratuita de anaconda que nos facilitará la tarea de instalar el ambiente e
incluye las jupyter notebooks que es una aplicación web que nos ayudará a hacer ejercicios paso
a paso en machine learning visualizacion de datos y escribir comentarios tal como si se tratase de
un cuaderno de notas de la universidad

esta suite es multiplataforma y se puede utilizar en windows linux y macintosh
agenda
nuestra agenda de hoy incluye

 descargar anaconda

 instalar anaconda

 iniciar y actualizar anaconda

 actualizar paquete scikitlearn

 instalar librerías para deep learning

comencemos

 descargar anaconda

veamos como descargar anaconda a nuestro disco y obtener esta suite científica de python

naive bayes comprar casa o alquilar

pearson correlation of features

f 


financiar

aho financiar



con esto comprobamos que en general están poco correlacionadas sin embargo también tenemos 
valores de esperemos que el algoritmo sea lo suficientemente naive para dar buenos resultados


crear el modelo gaussian naive bayes con sklearn

primero vamos a dividir nuestros datos de entrada en entrenamiento y test

httpwwwaprendemachinelearningcomwpcontentuploadsbayescorrelationpng

ne

a

ne



l







naive bayes comprar casa o alquilar 

 split dataset in training and test datasets

xtrain x test traintestsplitdataframe testsize randomstate
ytrain x traincomprar

ytest x testcomprar

y creamos el modelo lo ponemos a aprender con fit y obtenemos predicciones sobre nuestro
conjunto de test

 instantiate the classifier
gnb gaussiannb
 train classifier
gnbfit
xtrainusedfeatures values
y train

y pred gnbpredictxtest usedfeatures 

print precisión en el set de entrenamiento f

 format gnbscorextrainusedfeatures ytrain
print precisión en el set de test f

 formatgnbscorextest usedfeatures ytest

precisión en el set de entrenamiento 
precisión en el set de test 

pues hemos obtenido un bonito de aciertos en el conjunto de test con nuestro querido
clasificador bayesiano también puedes ver los resultados obtenidos aplicando pca en este otro
capítulo

probemos el modelo comprar o alquilar

ahora hagamos predicciones para probar nuestra máquina

 en un caso será una familia sin hijos con de ingresos que quiere comprar una casa de
 y tiene sólo ahorrados

 el otro será una familia con hijos con ingresos por al mes en ahorros y consultan
si comprar una casa de 

 httpwwwaprendemachinelearningcomcomprendeprincipalcomponentanalysis

ne

a

naive bayes comprar casa o alquilar 

a ingresos ahorros hijos trabajo financiar
printgnbpredict 
 

resultado esperado alquilar comprar casa

 

los resultados son los esperados en el primer caso recomienda alquilar y en el segundo comprar
la casa 

resumen

a lo largo del artículo repasamos el teorema de bayes y vimos un ejemplo para aplicarlo en

una toma de decisiones pero no olvidemos que en el proceso también hicimos preprocesamiento
de los datos visualizaciones y selección de características durante diversas charlas que tuve con
profesionales del data science en mi camino de aprendizaje sale un mismo mensaje que dice no
es tan importante el algoritmo a aplicar si no la obtención y preprocesado de los datos que se van a
utilizar naive bayes como clasificador se utiliza mucho en nlp natural language processing
tanto en el típico ejemplo de detectar spam o no en tareas más complejas como reconocer
un idioma o detectar la categoría apropiada de un artículo de texto también puede usarse para
detección de intrusiones o anomalías en redes informáticas y para diagnósticos médicos dados unos
síntomas observados por último veamos los pros y contras de utilizar gaussian naive bayes

 pros es rápido simple de implementar funciona bien con conjunto de datos pequeños va bien
con muchas dimensiones features y llega a dar buenos resultados aún siendo ingenuo sin
que se cumplan todas las condiciones de distribución necesarias en los datos

 contras requiere quitar las dimensiones con correlación y para buenos resultados las entradas
deberían cumplir las suposiciones de distribución normal e independencia entre sí muy difícil
que sea así ó deberíamos hacer transformaciones en lo datos de entrada

recursos adicionales
 el código lo puedes ver en mi cuenta de github ó 
 lo puedes descargar desde aquí jupyter notebook ejercicio bayes python code

 descarga el archivo csv de entrada compraralquilarcsv

otros artículos de interés sobre bayes y python en inglés

 httpseswikipediaorgwikiteoremadebayes
ohttpwwwaprendemachinelearningcomprocesamientodellenguajenaturalnlp
httpsgithubcomjbagnatomachinelearning
httpwwwaprendemachinelearningcomwpcontentuploadsejerciciobayesipynb
 httpwwwaprendemachinelearningcomwpcontentuploadscompraralquilarcsv

naive bayes comprar casa o alquilar 

 naive bayes classifier from scratch

 naive bayes classification with sklearn
in depth naive bayes classification

 bayesian statistic for data science
feature selection 

 comprende principal component analysis





httpschrisalboncommachinelearningnaivebayesnaivebayesclassifierfromscratch
shttpsblogsicaracomnaivebayesclassifiersklearnpythonexampletipse
shttpsjakevdpgithubiopythondatasciencehandbooknaivebayeshtml
httpstowardsdatasciencecombayesianstatisticsfordatascienceecc
httpscikitlearnorgstablemodulesfeatureselectionhtml

 httpwwwaprendemachinelearningcomcomprendeprincipalcomponentanalysis

sistemas de recomendación

crea en python un motor de recomendación con collaborative
filtering

una de las herramientas más conocidas y utilizadas que aportó el machine learning fueron
los sistemas de recomendación son tan efectivos que estamos invadidos todos los días por
recomendaciones sugerencias y productos relacionados aconsejados por distintas apps webs y
correos

sin dudas los casos más conocidos de uso de esta tecnología son netflix acertando en recomendar
series y películas spotify sugiriendo canciones y artistas ó amazon ofreciendo productos de
venta cruzada sospechosamente muy tentadores para cada usuario

pero también google nos sugiere búsquedas relacionadas android aplicaciones en su tienda y
facebook amistades o las típicas lecturas relacionadas en los blogs y periódicos

todo ecommerce que se precie de serlo debe utilizar esta herramienta y si no lo hace estará
perdiendo una ventaja competitiva para potenciar sus ventas

qué son los sistemas ó motores de recomendación

i ión a v a 
los sistemas de recomendación a veces llamados en inglés recommender systems son
algoritmos que intentan predecir los siguientes ítems productos canciones etc que
querrá adquirir un usuario en particular

antes del machine learning lo más común era usar rankings ó listas con lo más votado ó más
popular de entre todos los productos entonces a todos los usuarios se les recomendaba lo mismo
es una técnica que aún se usa y en muchos casos funciona bien por ejemplo en librerías ponen
apartados con los libros más vendidos best sellers pero y si pudiéramos mejorar eso si hubiera
usuarios que no se guían como un rebaño y no los estamos reteniendo

los sistemas de recomendación intentan personalizar al máximo lo que ofrecerán a cada
usuario esto es ahora posible por la cantidad de información individual que podemos recabar de
las personas y nos da la posibilidad de tener una mejor tasa de aciertos mejorando la experiencia
del internauta sin ofrecer productos a ciegas

ohttpswwwaprendemachinelearningcomqueesmachinelearning

sistemas de recomendación 

tipos de motores

entre las estrategias más usadas para crear sistemas de recomendación encontramos

 itv sopularidad uctos s vendidos

popularity aconseja por la popularidad de los productos por ejemplo los más vendidos
globalmente se ofrecerán a todos los usuarios por igual sin aprovechar la personalización es
fácil de implementar y en algunos casos es efectiva

 contentbased a partir de productos visitados por el usuario se intenta adivinar qué busca
el usuario y ofrecer mercancías similares

 colaborative es el más novedoso pues utiliza la información de masas para identificar
perfiles similares y aprender de los datos para recomendar productos de manera individual

en este artículo comentaré mayormente el collaborative filtering y realizaremos un ejercicio en
python

cómo funciona collaborative filtering

para explicar cómo funciona collaborative filtering vamos a entender cómo será el dataset

ejemplo de dataset

necesitaremos ítems y las valoraciones de los usuarios los ítems pueden ser canciones películas
productos ó lo que sea que queremos recomendar

entonces nos quedará una matriz de este tipo donde la intersección entre fila y columna es una
valoración del usuario

sistemas de recomendación 

en esta gráfica educativa tenemos una matriz con productos a la izquierda y los ítems arriba
en este ejemplo los ítems serán frutas y cada celda contiene la valoración hecha por cada usuario
de ese ítem las casillas vacías significa que el usuario aún no ha probado esa fruta

entonces veremos que tenemos huecos en la tabla pues evidentemente no todos los usuarios tienen
o valoraron todos los ítems por ejemplo si los ítems fueran películas es evidente que un usuario
no habrá visto todas las películas del mundo entonces esos huecos son justamente los que con
nuestro algoritmo rellenaremos para recomendar ítems al usuario

una matriz con muchas celdas vacías se dice en inglés que es sparce y suele ser normal en
cambio si tuviéramos la mayoría de las celdas cubiertas con valoraciones se llamará dense

tipos de collaborative filtering

 userbased este es el que veremos a continuación
 se identifican usuarios similares
 se recomiendan nuevos ítems a otros usuarios basado en el rating dado por otros usuarios
similares que no haya valorado este usuario
 titembased
 calcular la similitud entre items

 encontrar los mejores items similares a los que un usuario no tenga evaluados y
recomendárselos

 httpdbaoraclecomoraclenewswhatssparsity htm

sistemas de recomendación 

predecir gustos userbased

collaborative filtering intentará encontrar usuarios similares para ofrecerle ítems bien valorados
para ese perfil en concreto lo que antes llamé rellenar los huecos en la matriz hay diversas
maneras de medir ó calcular la similitud entre usuarios y de ello dependerá que se den buenas
recomendaciones pero tengamos en cuenta que estamos hablando de buscar similitud entre gustos
del usuario sobre esos ítems me refiero a que no buscaremos perfiles similares por ser del mismo
sexo edad ó nivel educativo sólo nos valdremos de los ítems que ha experimentado valorado y
podría ser su secuencia temporal para agrupar usuarios parecidos

una de las maneras de medir esa similitud se llama distancia por coseno de los vectores y por

simplificar el concepto digamos que crea un espacio vectorial con n dimensiones correspondientes
a los n items y sitúa los vectores siendo su medida el valor rating de cada usuario a ese item
luego calcula el ángulo entre los vectores partiendo de la coordenada cero a poca distancia
entre ángulos se corresponde con usuarios con mayor similitud

este método no es siempre es perfecto pero es bastante útil y rápido de calcular

calcular los ratings

una vez que tenemos la matriz de similitud nos valdremos de otra operación matemática

calcular las recomendaciones

n

ru rusu ísu

u u 

formula para calcular los ratings faltantes sería algo así como matriz de similitud prodvectorial
ratings sumatoria de cada fila de ratings transpuesta

para

lo que haremos es cada rating se multiplica por el factor de similitud de usuario que dio el rating
la predicción final por usuario será igual a la suma del peso de los ratings dividido por la suma
ponderada

bueno no te preocupes que este cálculo luego lo verás en código y no tiene tanto truco

httpblogchristianperonecommachinelearningcosinesimilarityforvectorspacemodelspartiii

 httpsrealpythoncombuildrecommendationenginecollaborativefiltering
 httpswwwaprendemachinelearningcomwpcontentuploadsratingponderadopng

d



l

d



o

sistemas de recomendación 

ejercicio en python sistema de recomendación de
repositorios github

vamos a crear un motor de recomendación de repositorios github es la propuesta que hago en
el blog porque los recomendadores de música películas y libros ya están muy vistos

contaremos con un set de datos limitado pequeño pero podremos editar e ir agregando usuarios
y repositorios para mejorar las sugerencias

vamos al código

cargamos las librerías que utilizaremos

import pandas as pd

import numpy as np

from sklearnmetrics import meansquarederror

from sklearnmodelselection import traintestsplit
from sklearnneighbors import nearestneighbors
import matplotlibpyplot as plt

import sklearn

cargamos y previsualizamoás los archivos de datos csv que utilizaremos

dfusers pdreadcsv userscsv
dfrepos pdreadcsvreposcsv
dfratings pdreadcsvratingscsv
printdfusershead 
printdfrepos head 
printdfratingshead

userid username name
o iris isabel ruiz buriticá
 dianaciarke diana
 nateprewitt nate prewitt
 oldani ordanis sanchez
 waflessnet waflessnet



 httpswwwaprendemachinelearningcomwpcontentuploadsrecommenddfuserspng

n

sistemas de recomendación 

repold title categories stars
 airbnb javascript completar nan
 kamranahmedse developerroadmap roadmap to becoming a web developer in 
 microsoft vscode visual studio code 
 torvalds linux linux kernel source tree 
 ytdiorg youtubedl commandline program to download videos from y 
userid repold rating
o 
 
 
 
 

vemos que tenemos un archivo con la información de los usuarios y sus identificadores un archivo
con la información de los repositorios y finalmente el archivo ratings que contiene la valoración
por usuario de los repositorios como no tenemos realmente una valoración del al como
podríamos tener por ejemplo al valorar películas la columna rating es el número de usuarios que
tienen ese mismo repositorio dentro de nuestra base de datos sigamos explorando para comprende
un poco mejor

nusers dfratingsuseridunique shape 

nitems dfratingsrepolidunique shape 

l

print strnusers users

l

print strnitems items

 users
 items

vemos que es un dataset reducido pequeño tenemos usuarios y repositorios valorados

plthistdfratingsratingbins

 httpswwwaprendemachinelearningcomwpcontentuploadsrecommenddfrepospng
httpswwwaprendemachinelearningcomwpcontentuploadsrecommenddfratingspng

instalar el ambiente de desarrollo python 

nos dirigimos a la home de anaconda e iremos a la sección de download descargas

elegimos nuestra plataforma windows mac o linux

anaconda installers

windows macos é linux 

bit graphical installer mb bit graphical installer mb bit x installer mb

bit graphical installer mb bit command line installer mb bit power and power installer 
mb

bit graphical installer mb bit graphical installer mb

bit x installer mb
bit graphical installer mb bit command line installer mb

bit power and power installer 
mb

atención elegir la versión de python y no la de y seleccionar el instalador gráfico
graphical installer

con esto guardaremos en nuestro disco duro unos mb según sistema operativo y obtendremos
un archivo con el nombre similar a anacondamacosxxpkg

 instalar anaconda

en este paso instalaremos la app en nuestro sistema deberá tener permisos de administrador si
instala para todos los usuarios

ejecutamos el archivo que descargamos haciendo doble click
se abrirá un típico wizard de instalación

seguiremos los pasos podemos seleccionar instalación sólo para nuestro usuario seleccionar la ruta
en disco donde instalaremos y listo

al instalarse el tamaño total podrá superar gb en disco
 iniciar y actualizar anaconda

en este paso comprobaremos que se haya instalado correctamente y verificaremos tener la versión
más reciente

httpswwwanacondacomproductsindividualtdownload

o ojiaaada bafb




sistemas de recomendación 

 



tenemos más de valoraciones con una puntuación de y unas con puntuación en veamos
las cantidades exactas

dfratings groupby rating userid count

rating

















name userld dtype int

o i oaññswln 

plthistdfratingsgroupby repoid repoid countbins

 httpswwwaprendemachinelearningcomwpcontentuploadsrecommend ratingshistpng

sistemas de recomendación 

 



aquí vemos la cantidad de repositorios y cuantos usuarios los siguen la mayoría de repos los
tiene sólo usuario y no los demás hay unos que los tienen usuarios y unos que coinciden
 usuarios la suma total debe dar 

creamos la matriz usuariosratings
ahora crearemos la matriz en la que cruzamos todos los usuarios con todos los repositorios

dfmatrix pdpivottabledfratings valuesrating indexuserlid columnsrepx
oldfillna
dfmatrix

 httpswwwaprendemachinelearningcomwpcontentuploadsrecommendreposhistpng

rn 

sistemas de recomendación

repold 
userld

 
 
 
 

 

o a r n

 
 
 
 

 































vemos que rellenamos los huecos de la matriz con ceros y esos ceros serán los que deberemos

reemplazar con las recomendaciones

sparcity

veamos el porcentaje de sparcity

que tenemos
ratings df matrixvalues

sparsity floatlenratingsnonzero
sparsity ratingsshape ratingsshape
sparsity 

printsparsity fformat sparsity

sparsity 

esto serán muchos ceros que rellenar predecir

ohttpswwwaprendemachinelearningcomwpcontentuploadsusersrepomatrixpng

 httpswwwwquoracomwhatisaclear explanationofdatasparsity

d

a

ne

a

sistemas de recomendación 

dividimos en train y test set

separamos en train y test para más adelante poder medir la calidad de nuestras recomendaciones

ratingstrain ratingstest traintestsplitratings testsize randomstat x
e

printratingstrainshape

printratingstest shape

 
 

matriz de similitud distancias por coseno
ahora calculamos en una nueva matriz la similitud entre usuarios

simmatrix sklearnmetricspairwisecosinedistancesratings
printsimmatrixshape

 

pltimshowsimmatrix
pltcolorbar
pltshow

httpswwwaprendemachinelearningcomclasificacioncon datos desbalanceados

o o ojiduadabpaa t 

sistemas de recomendación 

o 



cuanto más cercano a mayor similitud entre esos usuarios

predicciones ó llamémosle sugeridos para ti

separar las filas y columnas de train y test
simmatrixtrain sim matrix
simmatrixtest simmatrix

userspredictions simmatrixtraindotratingstrain nparray npabssimmatriwv
xtrainsumaxist

pltrcparams figurefigsize 
pltimshowuserspredictions
pltcolorbar

pltshow

shttpswwwaprendemachinelearningcomwpcontentuploadssimilitudmatrixplotpng

o oiddaaaafsaa n 



d e o n 

sistemas de recomendación 









 





 

vemos pocas recomendaciones que logren puntuar alto la mayoría estará entre y puntos esto
tiene que ver con nuestro dataset pequeño

vamos a tomar de ejemplo mi usuario de github que es jbagnato

usuarioejemplo jbagnato
data dfusersdfusersusername usuario ejemplo
usuariover datailocuserid resta para obtener el index de pandas

userouserspredictionsargsort usuariover

 veamos los tres recomendados con mayor puntaje en la predic para este usuario
for i akepo in enumerateuser
selrepo dfreposdfreposrepolidarepo

l

printselrepotitle puntaje userspredictions usuariover arepo

 ytdlorg youtubedl
name title dtype object puntaje 
 dipanjans practicalmachinelearningwithpy
name title dtype object puntaje 
 abhat datasciencecheatsheet
name title dtype object puntaje 

vemos que los tres repositorios con mayor puntaje para sugerir a mi usuario son el de data
sciencecheatsheet con una puntuación de practicalmachinelearningwithpy con 
y youtubedl con no son puntuaciones muy altas pero son buenas sugerencias

validemos el error



sobre el test set comparemos la métrica mean squared error con el conjunto de entrenamiento

 httpswwwaprendemachinelearningcomwpcontentuploadsrecommendtrainpredictionspng
shttpsgithubcomjbagnatomachinelearning
 httpswwwstatisticshowtodatasciencecentralcommeansquarederror

ne



l









sistemas de recomendación 

def get msepreds actuals
if predsshape actualsshape
actuals actualst
preds predsactualsnonzeroflatten
actuals actualsactualsnonzero flatten
return meansquarederror preds actuals

getmseuserspredictions ratingstrain

 realizo las predicciones para el test set

userspredictionstest simmatrixdotratings nparray npabssimmatrixsum x
axist

userspredictionstest userspredictionstest

getmseuserspredictionstest ratingstest




vemos que para el conjunto de train y test el mae es bastante cercano un indicador de que no tiene
buenas predicciones sería si el mae en test fuera veces más ó la mitad del valor del de train

hay más

en la notebook completa en github encontrarás más opciones de crear el recomendador
utilizando knearest neighbors como estimador y también usando la similitud entre ítems ítem
based sin embargo para los fines de este artículo espero haber mostrado el funcionamiento básico
del collaborative filtering te invito a que luego lo explores por completo

resumen

vimos que es relativamente sencillo crear un sistema de recomendación en python y con machine
learning como muchas veces en data science una de las partes centrales para que el modelo
funcione se centra en tener los datos correctos y un volumen alto también es central el valor que
utilizaremos como rating siendo una valoración real de cada usuario ó un valor artificial que
creemos adecuado recuerda que me refiero a rating como ese puntaje que surge de la intersección
entre usuario e ítems en nuestro dataset luego será cuestión de evaluar entre las opciones de motores
userbased ítembased y seleccionar la que menor error tenga y no descartes probar en el mundo
real y ver qué porcentaje de aciertos o feedback te dan los usuarios reales de tu aplicación

shttpsgithubcomjbagnatomachinelearningblobmasterejerciciosistemasrecomendacionipynb
c httpswwwaprendemachinelearningcomclasificarconknearestneighborejemploenpython

sistemas de recomendación 

existen algunas librerías que se utilizan para crear motores de recomendación como la llamada
surprise
por último decir que como en casi todo el machine learning tenemos la opción de crear redes

neuronales con embeddings como recomendados y hasta puede que sean las que mejor funcionan
para resolver esta tarea

recursos del artículo
descarga los archivos csv y el notebook con el ejercicio python completo y adicionales

 userscsv

 teposcsv
 ratingscsv
e ejerciciosistemasderecomendación jupyter notebook





otros artículos de interés en inglés



build a recommendation engine
collaborative filtering and embeddings
how to build a simple songrecommendersystem
collaborative filtering with python

 machine learning for recommender system






s

 httpswwwaprendemachinelearningcompronosticodeventasredesneuronales pythonembeddings

ohttpsgithubcomjbagnatomachinelearningblobmasteruserscsv

 httpsgithubcomjbagnatomachinelearningblobmasterreposcsv

httpsgithubcomjbagnatomachinelearningblobmasterratingscsv

 httpsgithubcomjbagnatomachinelearningblobmasterejerciciosistemasrecomendacionipynb

httpsrealpythoncombuildrecommendation engine collaborativefiltering

 shttpstowardsdatasciencecomcollaborativefilteringandembeddings partbbce

 httpstowardsdatasciencecomhowtobuildasimplesongrecommenderfcbcc

 httpwwwsalemmaraficomcodecollaborativefilteringwithpython

 httpsmediumcomrecombeeblogmachinelearningforrecommendersystemspartalgorithmsevaluationandcoldstart
fded

breve historia de las redes
neuronales artificiales

arquitecturas y aplicaciones de las redes neuronales

vamos a hacer un repaso por las diversas estructuras inventadas mejoradas y utilizadas a lo largo de
la historia para crear redes neuronales y sacar el mayor potencial al deep learning para resolver
toda clase de problemas de regresión y clasificación

evolución de las redes neuronales en ciencias de la
computación

vamos a revisar las siguientes redesarquitecturas

 perceptron
 multilayer perceptron
 s
 neuronas sigmoidales
 redes feedforward
 backpropagation
 convolutional neural networks cnn recurent neural networks rnn
 long short term memory lstm
 deep belief networks dbn nace el deep learning
 restricted boltzmann machine
 encoder decoder autoencoder
 generative adversarial networks gan

si bien esta lista no es exhaustiva y no se abarcan todos los modelos creados desde los años he
recopilado las que fueron las redes y tecnologías más importantes desarrolladas para llegar al
punto en que estamos hoy el aprendizaje profundo

 httpwwwaprendemachinelearningcomaprendizajeprofundounaguiarapida
httpwwwaprendemachinelearningcomaplicacionesdelmachinelearning

breve historia de las redes neuronales artificiales 

el inicio de todo la neurona artificial

 perceptron

entre las décadas de y el científico frank rosenblatt inspirado en el trabajo de warren
mcculloch y walter pitts creó el perceptron la unidad desde donde nacería y se potenciarían las
redes neuronales artificiales un perceptron toma varias entradas binarias x x etc y produce una
sóla salida binaria para calcular la salida rosenblatt introduce el concepto de pesos w w
etc un número real que expresa la importancia de la respectiva entrada con la salida la salida de
la neurona será si la suma de la multiplicación de pesos por entradas es mayor o menor a un
determinado umbral sus principales usos son decisiones binarias sencillas o funciones lógicas como
or and

 httpwwwaprendemachinelearningcomwpcontentuploadsnetperceptronpng
httpseswikipediaorgwikifrankrosenblatt

instalar el ambiente de desarrollo python 

anaconda viene con una suite de herramientas gráficas llamada anaconda navigator iniciemos la
aplicación y veremos una pantalla como esta

o anaconda navigator

 anaconda navigator

 upgrade now

sign in to anaconda

m home m
applications on root v channels refresh
environments

o s e e





m projects beta jupyter q

n a
b ao notebook atconsole spyder

 

webbased interactive computing notebook
environment edit and run humanreadable

a community

docs while describing the data analysis

pyqt gui that supports inline figures proper
multiline editing with syntax highlighting
graphical calltips and more

scientific python development
environment powerful python ide with
advanced editing interactive testing
debugging and introspection features

launch launch launch
e e e
pe
documentation r
a
developer blog n
glueviz jupyterlab orange
feedback 
multidimensional data visualization across component based data mining framework
files explore relationships within and among data visualization and data analysis for
y a o related datasets novice and expert interactive workflows

entre otros íconos vemos que podemos lanzar las jupyter notebooks

para comprobar la instalación abrimos una terminal de maclinuxubuntu o la línea de comandos

de windows

escribimos

 conda v

y obtenemos la versión
conda 

luego tipeamos

 python v

y verificamos la versión de python de nuestro sistema

para asegurarnos de tener la versión más reciente de la suite ejecutaremos

breve historia de las redes neuronales artificiales 

 multilayer perceptron



como se imaginarán el multilayer perceptron es una amplicación del perceptrón de una única
neurona a más de una además aparece el concepto de capas de entrada oculta y salida pero con
valores de entrada y salida binarios no olvidemos que tanto el valor de los pesos como el de umbral
de cada neurona lo asignaba manualmente el científico cuantos más perceptrones en las capas
mucho más difícil conseguir los pesos para obtener salidas deseadas

los s aprendizaje automático

neuronas sigmoides

para poder lograr que las redes de neuronas aprendieran solas fue necesario introducir un nuevo
tipo de neuronas las llamadas neuronas sigmoides son similares al perceptron pero permiten que
las entradas en vez de ser ceros o unos puedan tener valores reales como ó ó lo que sea
también aparecen las neuronas bias que siempre suman en las diversas capas para resolver ciertas
situaciones ahora las salidas en vez de ser ó será dw x b donde d será la función sigmoide
definida como dz e esta es la primer función de activación

 httpwwwaprendemachinelearningcomwpcontentuploadsnetmultilayerpng

breve historia de las redes neuronales artificiales 

 e o 



imagen de la curva logística normalizada de wikipedia

con esta nueva fórmula se puede lograr que pequeñas alteraciones en valores de los pesos
deltas produzcan pequeñas alteraciones en la salida por lo tanto podemos ir ajustando muy
de a poco los pesos de las conexiones e ir obteniendo las salidas deseadas

redes feedforward

se les llama así a las redes en que las salidas de una capa son utilizadas como entradas en la próxima
capa esto quiere decir que no hay loops hacia atrás siempre se alimenta de valores hacia
adelante hay redes que veremos más adelante en las que sí que existen esos loops recurrent neural
networks además existe el concepto de fully connected feedforward networks y se refiere a que
todas las neuronas de entrada están conectadas con todas las neuronas de la siguiente capa

 backpropagation

gracias al algoritmo de backpropagation

se hizo posible entrenar redes neuronales de multiples
capas de manera supervisada al calcular el error obtenido en la salida e ir propagando hacia las capas
anteriores se van haciendo ajustes pequeños minimizando costo en cada iteración para lograr que

la red aprenda consiguiendo que la red pueda clasificar las entradas correctamente

 httpwwwaprendemachinelearningcomwpcontentuploadslogisticcurvepng
shttpseswikipediaorgwikipropagacicbnhaciaatrcas

breve historia de las redes neuronales artificiales 

 convolutional neural network



las convolutional neural networks

son redes multilayered que toman su inspiración del cortex vi
sual de los animales esta arquitectura es útil en varias aplicaciones principalmente procesamiento
de imágenes la primera cnn fue creada por yann lecun y estaba enfocada en el reconocimiento
de letras manuscritas la arquitectura constaba de varias capas que implementaban la extracción
de características y luego clasificación la imagen se divide en campos receptivos que alimentan
una capa convolutional que extrae features de la imagen de entrada por ejemplo detectar lineas
verticales vértices etc el siguiente paso es pooling que reduce la dimensionalidad de las features
extraídas manteniendo la información más importante luego se hace una nueva convolución y
otro pooling que alimenta una red feedforward multicapa la salida final de la red es un grupo de
nodos que clasifican el resultado por ejemplo un nodo para cada número del al es decir 
nodos se activan de a uno

esta arquitectura usando capas profundas y la clasificación de salida abrieron un mundo nuevo de
posibilidades en las redes neuronales las cnn se usan también en reconocimiento de video y tareas
de procesamiento del lenguaje natural

 httpwwwaprendemachinelearningcomwpcontentuploadsnetconvolutionalpng
 httpwwwaprendemachinelearningcomcomofuncionanlas convolutionalneuralnetworks visionporordenador
httpsenwikipediaorgwikiyannlecun

breve historia de las redes neuronales artificiales 

 long short term memory recurrent neural network



aqui vemos que la red lstm tiene neuronas ocultas con loops hacia atrás en azul esto permite
que almacene información en celdas de memoria

las long short term memory son un tipo de recurrent neural network esta arquitectura permite
conexiones hacia atrás entre las capas esto las hace buenas para procesar datos de tipo time
series datos históricos en se crearon las lstm que consisten en unas celdas de memoria
que permiten a la red recordar valores por períodos cortos o largos una celda de memoria contiene
compuertas que administran cómo la información fluye dentro o fuera la puerta de entrada
controla cuando puede entran nueva información en la memoria la puerta de olvido controla
cuanto tiempo existe y se retiene esa información la puerta de salida controla cuando la información
en la celda es usada como salida de la celda la celda contiene pesos que controlan cada compuerta el
algoritmo de entrenamiento conocido como backpropagationthroughtime optimiza estos pesos
basado en el error de resultado las lstm se han aplicado en reconocimiento de voz de escritura
texttospeech y otras tareas

 httpwwwaprendemachinelearningcomwpcontentuploadsnetistmpng

breve historia de las redes neuronales artificiales 

se alcanza el deep learning

 deep belief networks dbn



la deep belief network utiliza un autoencoder con restricted boltzmann machines para preentre
nar a las neuronas de la red y obtener un mejor resultado final

antes de las dbn en los modelos con profundidad decenas o cientos de capas eran
considerados demasiado difíciles de entrenar incluso con backpropagation y el uso de las redes
neuronales artificiales quedó estancado con la creación de una dbn que logro obtener un mejor
resultado en el dataset mnist se devolvió el entusiasmo en poder lograr el aprendizaje profundo
en redes neuronales hoy en día las dbn no se utilizan demasiado pero fueron un gran hito en la
historia en el desarrollo del deep learning y permitieron seguir la exploración para mejorar las redes
existentes cnn lstm etc las deep belief networks demostraron que utilizar pesos aleatorios
al inicializar las redes son una mala idea por ejemplo al utilizar backpropagation con descenso
por gradiente muchas veces se caía en mínimos locales sin lograr optimizar los pesos mejor será
utilizar una asignación de pesos inteligente mediante un preentrenamiento de las capas de la red
se basa en el uso de la utilización de restricted boltzmann machines y autoencoders para
preentrenar la red de manera no supervisada ojo luego de preentrenar y asignar esos pesos
iniciales deberemos entrenar la red de forma habitual supervisada con backpropagation ese
preentrenamiento es una de las causas de la gran mejora en las redes neuronales permitió el deep

 httpwwwaprendemachinelearningcomwpcontentuploadsdeepbeleifpng

 httpstensorflowrstudiocomtensorflowarticlestutorialmnistbeginnershtml
httpstowardsdatasciencecomdeeplearning meetsphysicsrestrictedboltzmann machinespartidfcc
httpstowardsdatasciencecomthevariationalautoencoderasatwoplayergameparticfb

 httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningnosupervisado

 httpwwwaprendemachinelearningcomaplicaciones delmachinelearningsupervisado

breve historia de las redes neuronales artificiales 

learning pues para asignar los valores se evalúa capa a capa de a una y no sufre de cierto sesgo
que causa el backpropagation al entrenar a todas las capas en simultáneo

 generative adversarial networks

httpwwwaprendemachinelearningcomwpcontentuploadsganpng

las gan entrenan dos redes neuronales en simultáneo la red de generación y la red de discrimi
nación a medida que la máquina aprende comienza a crear muestras que son indistinguibles de
los datos reales

estas redes pueden aprender a crear muestras de manera similar a los datos con las que las
alimentamos la idea detrás de gan es la de tener dos modelos de redes neuronales compitiendo
uno llamado generador toma inicialmente datos basura como entrada y genera muestras el
otro modelo llamado discriminador recibe a la vez muestras del generador y del conjunto de
entrenamiento real y deberá ser capaz de diferenciar entre las dos fuentes estas dos redes juegan
una partida continua donde el generador aprende a producir muestras más realistas y el
discriminador aprende a distinguir entre datos reales y muestras artificiales estas redes son
entrenadas simultáneamente para finalmente lograr que los datos generados no puedan diferenciarse
de datos reales sus aplicaciones principales son la de generación de imágenes artificiales realistas
pero también la de mejorar imágenes ya existentes o generar textos captions en imágenes o
generar textos siguiendo un estilo determinado y hasta para el desarrollo de moléculas para industria
farmacéutica

breve historia de las redes neuronales artificiales 

resumen

hemos recorrido estos primeros casi años de avances en las redes neuronales en la historia de la
inteligencia artificial se suele dividir en etapas del al en donde se pasó del asombro de estos
nuevos modelos hasta el escepticismo el retorno de un invierno de años cuando en los ochentas
surgen mejoras en mecanismos y maneras de entrenar las redes backpropagation y se alcanza
una meseta en la que no se puede alcanzar la profundidad de aprendizaje seguramente también
por falta de poder de cómputo y una tercer etapa a partir de en la que se logra superar esa
barrera y aprovechando el poder de las gpu y nuevas técnicas se logra entrenar cientos de capas
jerárquicas que conforman y potencian el deep learning y dan una capacidad casi ilimitada a estas
redes como último comentario me gustaría decir que recientemente feb surgieron nuevos
estudios de las neuronas humanas biológicas en las que se está redescubriendo su funcionamiento
y se está produciendo una nueva revolución pues parece que es totalmente distinto a lo que hasta
hoy conocíamos esto puede ser el principio de una nueva etapa totalmente nueva y seguramente
mejor del aprendizaje profundo el machine learning y la inteligencia artificial

más recursos

 cheat sheets for ap

 neural networks and deep learning
 from perceptrons to deep networks

 neural networks architectures

 a beginners guide to machine learning
 a guide on time series prediction using lstm

 convolutional neural networks in python with keras
 history of neural networl







 httpwwwaprendemachinelearningcomcrearunaredneuronalenpythondesdecero

 httpsmediumcomintuitionmachineneuronsaremorecomplexthanwhatwehaveimaginedbddadcd
 httpwwwaprendemachinelearningcomqueesmachinelearning

 httpsbecominghumanaicheatsheetsforaineuralnetworksmachinelearning deeplearningbigdatacbb
ottpneuralnetworksanddeeplearningcomchaphtmlperceptrons

 httpswwwtoptalcommachinelearninganintroductionto deeplearningfromperceptronstodeepnetworks
httpsmlcheatsheetreadthedocsioenlatestarchitectureshtml
shttpswwwibmcomdeveloperworkslibraryccbeginnerguide machinelearningaicognitiveindexhtml
httpsblogstatsbotcotimeseries predictionusingrecurrentneuralnetworksistmsfacaf

 httpswwwdatacampcomcommunitytutorialsconvolutionalneuralnetworkspython

httpsmediumcom okarthikeyanahistoryofneuralnetworkdfc

aprendizaje profundo una guía
rápida
deep learning y redes neuronales sin código

explicaré brevemente en qué consiste el deep learning ó aprendizaje profundo utilizado en
machine learning describiendo sus componentes básicos

nos centraremos en aprendizaje profundo aplicando redes neuronales artificiales

cómo funciona el deep learning mejor un ejemplo

el aprendizaje profundo es un método del machine learning que nos permite entrenar
una inteligencia artificial para obtener una predicción dado un conjunto de entradas esta
inteligencia logrará un nivel de cognición por jerarquías se puede utilizar aprendizaje
supervisado o no supervisado

explicaré cómo funciona el deep learning mediante un ejemplo teórico de predicción sobre quién
ganará el mundial de futbol utilizaremos aprendizaje supervisado mediante algoritmos de redes
neuronales artificiales para lograr las predicciones de los partidos de fútbol podemos tener las
siguientes entradas

 cantidad de partidos ganados

 cantidad de partidos empatados

 cantidad de partidos perdidos

 cantidad de goles a favor

 cantidad de goles en contra

 racha ganadora del equipo cant max de partidos ganados seguidos sobre el total jugado

y podríamos tener muchísimas entradas más la puntuación media de los jugadores del equipo o el
score que da la fifa al equipo como en cada partido tenemos a rivales deberemos incluir estos 
datos de entrada por cada equipo es decir entradas del equipo y otras del equipo dando un
total de entradas la predicción de salida será el resultado del partido local empate o visitante

httpwwwaprendemachinelearningcombrevehistoria delasredesneuronalesartificiales
httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado
httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningnosupervisado

a httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearningredneuronal

aprendizaje profundo una guía rápida 

creamos una red neuronal

en la programación tradicional escribiríamos código en donde indicamos reglas por ejemplo si
goles de equipo mayor a goles de equipo entonces probabilidad de local aumenta es decir que
deberíamos programar artesanalmente unas reglas de inteligencia bastante extensa e interrelacionar
las variables para posibles resultados para evitar todo ese enredo y hacer que nuestro código
sea escalaba y flexible a cambios recurrimos a las redes neuronales para describir una arquitectura
de interconexiones y capas y dejar que este modelo aprenda por sí mismo y descubra él mismo
relaciones entre variables que nosotros desconocemos

vamos a crear una red neuronal con valores de entrada input layer y con neuronas de salida
output layer las neuronas que tenemos en medio se llaman hidden layers y podemos tener
muchas cada una con una distinta cantidad de neuronas todas las neuronas estarán interconectadas
unas con otras en las distintas capas como vemos en el dibujo las neuronas son los círculos blancos

esquema de capas en una red neuronal

capa de capa capa capa de
entrada oculta oculta salida




 la capa de entrada recibe los datos de entrada y los pasa a la primer capa oculta
 las capas ocultas realizarán cálculos matemáticos con nuestras entradas uno de los desafíos

aprendizaje profundo una guía rápida 

al crear la red neuronal es decidir el número de capas ocultas y la cantidad de neuronas de

cada capa

 la capa de salida devuelve la predicción realizada en nuestro caso de resultados discretos
las salidas podrán ser para local para empate y para visitante

partidos
ganados

partidos
empatados

partidos
perdidos

goles
a favor

goles
en contra

racha
ganadora

red neuronal para la predicción
mundial rusia 

m v
wo 
n

k

xy n local
p m
y aó m c x a
n h 
oo 
á m w l á

 ya am w 
 q lx visitante
y ak whw 

 para simplificar ilustramos sólo entradas del equipo faltan otras entradas del rival equipo 

la cantidad total de capas en la cadena le da profundidad al modelo de aquí es que surge la
terminología de aprendizaje profundo

cómo se calcula la predicción

cada conexión de nuestra red neuronal está asociada a un peso este peso dictamina la importancia
que tendrá esa relación en la neurona al multiplicarse por el valor de entrada los valores iniciales
de peso se asignan aleatoriamente spoiler más adelante los pesos se ajustarán solos

o ia ek an 

aoooo aha
o i aada ik w dn ho yo

instalar el ambiente de desarrollo python

 conda update conda

s o jbagnato conda update conda conda conda update conda x

 conda update conda

fetching package metadata eee

solving package specifications 

package plan for installation in environment anaconda

the following packages will be updated

conda pyheda py
pycosat py pyhcce

proceed yn b

debemos poner y para confirmar y se descargarán luego ejecutamos

 conda update anaconda

para confirmar que todo funciona bien crearemos un archivo de texto para escribir un breve script

de python nombra al archivo versionespy y su contenido será

 scipy

import scipy

printscipy s scipy version

 numpy

import numpy

printnumpy s numpy version

 matplotlib

import matplotlib

printmatplotlib s matplotlib version
 pandas

import pandas

printpandas s pandas version

 statsmodels

import statsmodels

print statsmodels s statsmodelsversion
 scikitlearn

import sklearn

printsklearn s sklearn version

en la linea de comandos en el mismo directorio donde está el archivo escribiremos

aprendizaje profundo una guía rápida 

distribución de pesos en una red neuronal



imitando a las neuronas biológicas cada neurona tiene una función de activación esta función
determinará si la suma de sus valores recibidos previamente multiplicados por el peso de la
conexión supera un umbral que hace que la neurona se active y dispare un valor hacia la siguiente
capa conectada hay diversas funciones de activación conocidas que se suelen utilizar en estas
redes cuando todas las capas finalizan de realizar sus cómputos se llegará a la capa final con una
predicción por ejemplo si nuestro modelo nos devuelve está prediciendo que ganará

local con probabilidades será empate o que gane visitante 

entrenando nuestra red neuronal

entrenar nuestra ia puede llegar a ser la parte más difícil del deep learning necesitamos

 gran cantidad y diversidad de valores en nuestro conjunto de datos de entrada
 gran poder de cálculo computacional

 httpsenwikipediaorgwikiactivationfunction

aprendizaje profundo una guía rápida 

en nuestro ejemplo de predicción de partidos de futbol para el mundial deberemos crear una
base de datos con todos los resultados históricos de los equipos de fútbol en mundiales en partidos
amistosos en clasificatorios los goles las rachas a lo largo de los años etc

para entrenar nuestra máquina deberemos alimentarla con el conjunto de datos de entrada y
comparar el resultado local empate visitante contra la predicción obtenida como nuestro modelo
fue inicializado con pesos aleatorios y aún está sin entrenar las salidas obtenidas seguramente serán
erróneas una vez que tenemos nuestro conjunto de datos comenzaremos un proceso iterativo
usaremos una función para comparar cuan buenomalo fue nuestro resultado contra el resultado
real esta función es llamada función coste idealmente queremos que nuestro coste sea cero
es decir sin error cuando el valor de la predicción es igual al resultado real del partido a medida
que entrena el modelo irá ajustando los pesos de interconexiones de las neuronas de manera
automática hasta obtener buenas predicciones a ese proceso de ir hacia atrás y venir por las capas
de neuronas se le conoce como backpropagation más detalle a continuación

cómo reducimos la función coste y mejoramos las
predicciones

para poder ajustar los pesos de las conexiones entre neuronas haciendo que el coste se aproxime a
cero usaremos una técnica llamada gradient descent esta técnica permite encontrar el mínimo de
una función en nuestro caso buscaremos el mínimo en la función coste funciona cambiando los
pesos en pequeños incrementos luego de cada iteración del conjunto de datos al calcular la derivada
o gradiente de la función coste en un cierto conjunto de pesos podremos ver en qué dirección
descender hacia el mínimo global aquí se puede ver un ejemplo de descenso de gradiente en 
dimensiones imaginen la dificultad de tener que encontrar un mínimo global en dimensiones

 httpsenwikipediaorgwikilossfunction
 httpsenwikipediaorgwikigradientdescent

aprendizaje profundo una guía rápida 

 gradiente
peso linicial 



coste mínimo
global

para minimizar la función de coste necesitaremos iterar por el conjunto de datos cientos de miles
de veces más por eso es tan importante contar con una gran capacidad de cómputo en
el ordenador en el que entrenamos la red la actualización del valor de los pesos se realizará
automáticamente usando el descenso de gradiente

esta es parte de la magia del aprendizaje profundo automático una vez que finalizamos de
entrenar nuestro predictor de partidos de futbol del mundial sólo tendremos que alimentarlo con
los partidos que se disputarán y podremos saber quién ganará el mundial

resumen

 el aprendizaje profundo utiliza algoritmos de redes neuronales artificiales que imitan el
comportamiento biológico del cerebro

 hay tipos de capas de neuronas de entrada ocultas y de salida

 las conexiones entre neuronas llevan asociadas un peso que denota la importancia del valor
de entrada en esa relación

 las neuronas aplican una función de activación para estandarizar su valor de salida a la
próxima capa de neuronas

aprendizaje profundo una guía rápida 

 para entrenar una red neuronal necesitaremos un gran conjunto de datos

 lterar el conjunto de datos y comparar sus salidas producirá una función coste que indicará
cuán alejado está nuestra predicción del valor real

 luego de cada iteración del conjunto de datos de entrada se ajustarán los pesos de las neuronas
utilizando el descenso de gradiente para reducir el valor de coste y acercar las predicciones a
las salidas reales

crear una red neuronal en python
desde cero

programaremos una red neuronal artificial en python sin utilizar librerías de terceros entrenare
mos el modelo y en pocas lineas el algoritmo podrá conducir por sí mismo un coche robot

para ello explicaremos brevemente la arquitectura de la red neuronal explicaremos el concepto
forward propagation y a continuación el de backpropagation donde ocurre la magia y aprenden
las neuronas

el proyecto

coche robot

este amigable coche robot arduino será a quien le implantaremos nuestra red neuronal para que
pueda conducir sólo evitando los obstáculos



vamos a crear una red neuronal que conduzca un coche de juguete arduino que más adelante

construiremos y veremos en el mundo real nuestros datos de entrada serán

 httpwwwaprendemachinelearningcombrevehistoriadelasredesneuronalesartificiales

 shttpwwwaprendemachinelearningcomprogramauncochearduinoconinteligenciaartificial
 httpneuralnetworksanddeeplearningcomchaphtml

 httpwwwaprendemachinelearningcomprogramauncochearduinoconinteligenciaartificial
 httpwwwaprendemachinelearningcomprogramauncochearduinoconinteligenciaartificial

crear una red neuronal en python desde cero 

 sensor de distancia al obstáculo

 si es no hay obstáculos a la vista

 si es se acerca a un obstáculo

 si es está demasiado cerca de un obstáculo
 posición del obstáculo izquierdaderecha

 el obstáculo es visto a la izquierda será 

 visto a la derecha será 

las salidas serán

 girar

 derecha izquierda 
e dirección

 avanzar retroceder 

la velocidad del vehículo podría ser una salida más por ejemplo disminuir la velocidad si nos
aproximamos a un objeto y podríamos usar más sensores como entradas pero por simplificar
el modelo y su implementación mantendremos estas entradas y salidas para entrenar la red
tendremos las entradas y salidas que se ven en la tabla

entrada entrada salida giro salida acción de la

sensor posición dirección salida

distancia obstáculo

 avanzar

 avanzar

 avanzar

 giro a la
izquierda

 giro a la
derecha

 avanzar

 retroceder

 retroceder

 retroceder

 avanzar

 avanzar

 avanzar

esta será la arquitectura de la red neuronal propuesta

crear una red neuronal en python desde cero 



en la imagen anterior y durante el ejemplo usamos la siguiente notación en las neuronas

 xi son las entradas
 ai activación en la capa 
 yi son las salidas

y quedan implícitos pero sin representación en la gráfica

 oj los pesos de las conexiones entre neuronas será una matriz que mapea la capa j a la j

 recordemos que utilizamos neurona extra en la capa y una neurona extra en la capa a
modo de bias no están en la gráfica para mejorar la precisión de la red neuronal dandole
mayor libertad algebraica

los cálculos para obtener los valores de activación serán
a gotx
a gotx
a gotx

nota la t indica matriz traspuesta para poder hacer el producto

en las ecuaciones la g es una función sigmoide que refiere al caso especial de función logística y
definida por la fórmula

gz ez

 httpwwwaprendemachinelearningcomwpcontentuploadsredneuronalcochepng
ohttpswwwquoracomwhatisbiasinartificialneuralnetworkshare
 httpseswikipediaorgwikifuncicbnsigmoide

crear una red neuronal en python desde cero 

funciones sigmoide

una de las razones para utilizar la función sigmoide función logística es por sus propiedades

matemáticas en nuestro caso sus derivadas cuando más adelante la red neuronal haga backpropa
gation para aprender y actualizar los pesos haremos uso de su derivada en esta función puede ser
expresada como productos de f y f entonces f t ft ft por ejemplo la función tangente y
su derivada arcotangente se utilizan normalizadas donde su pendiente en el origen es y cumplen
las propiedades

b
b
b

b
b
b
b

 e a 



imagen de la curva logística normalizada de wikipedia

forward propagation ó red feedforward

con feedforward nos referimos al recorrido de izquierda a derecha que hace el algoritmo de la
red para calcular el valor de activación de las neuronas desde las entradas hasta obtener los valores
de salida si usamos notación matricial las ecuaciones para obtener las salidas de la red serán

x x x x
zlayer x
alayer gzlayer
zlayer oalayer
y gtelayers

httpseswikipediaorgwikifuncicbnlogcadstica
 httpwwwaprendemachinelearningcomwpcontentuploadslogisticcurvepng

crear una red neuronal en python desde cero 

resumiendo tenemos una red tenemos entradas éstas se multiplican por los pesos de las
conexiones y cada neurona en la capa oculta suma esos productos y les aplica la función de
activación para emitir un resultado a la siguiente conexión concepto conocido en biología como
sinápsis química

como dijimos los pesos iniciales se asignan con valores entre y de manera
aleatoria el desafío de este algoritmo será que las neuronas aprendan por sí mismas
a ajustar el valor de los pesos para obtener las salidas correctas

backpropagation cómputo del gradiente

al hacer backpropagtion es donde el algoritmo itera para aprender esta vez iremos de derecha

a izquierda en la red para mejorar la precisión de las predicciones el algoritmo de backpropagation
se divide en dos fases propagar y actualizar pesos

fase propagar

esta fase implica pasos

 hacer forward propagation de un patrón de entrenamiento recordemos que es este es un
algoritmo supervisado y conocemos las salidas para generar las activaciones de salida de la red

 hacer backward propagation de las salidas activación obtenida por la red neuronal usando las
salidas y reales para generar los deltas error de todas las neuronas de salida y de las neuronas
de la capa oculta

fase actualizar pesos

para cada sinápsis de los pesos
 multiplicar su delta de salida por su activación de entrada para obtener el gradiente del peso
 substraer un porcentaje del gradiente de ese peso

el porcentaje que utilizaremos en el paso tiene gran influencia en la velocidad y calidad del
aprendizaje del algoritmo y es llamado learning rate ó tasa de aprendizaje si es una tasa muy
grande el algoritmo aprende más rápido pero tendremos mayor imprecisión en el resultado si es
demasiado pequeño tardará mucho tiempo y podría no acercarse nunca al valor óptimo

httpseswikipediaorgwikisinapsisqucadmica
httpneuralnetworksanddeeplearningcomchaphtml
shttpwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado

crear una red neuronal en python desde cero 

 gradiente
peso inicial 

jw

coste minimo
 global



en esta gráfica vemos cómo utilizamos el gradiente paso a paso para descender y minimizar el coste
total cada paso utilizará la tasa de aprendizaje learning rate que afectará la velocidad y calidad
de la red

deberemos repetir las fases y hasta que la performance de la red neuronal sea satisfactoria si
denotamos al error en el layer como di para nuestras neuronas de salida en layer la activación
menos el valor actual será usamos la forma vectorial

d alayer y
d ot d gzlayer
g zlayer alayer alayer

al fin aparecieron las derivadas nótese que no tendremos delta para la capa puesto que son los
valores x de entrada y no tienen error asociado el valor del costo que es lo que queremos minimizar
de nuestra red será

j alayer diayer 

usamos este valor y lo multiplicamos al learning rate antes de ajustar los pesos esto nos asegura
que buscamos el gradiente iteración a iteración apuntando hacia el mínimo global

selfweightsi learningrate layertdotdelta

nota el layer en el código es realmente a

 httpwwwaprendemachinelearningcomwpcontentuploadsdescensoporgradientepng

tuitea sobre el libro

por favor ayuda a juan ignacio bagnato hablando sobre el libro en twitter

el hashtag sugerido para este libro es aprendeml
descubre lo que otra gente dice sobre el libro haciendo clic en este enlace para buscar el hashtag en
twitter

aprendeml

instalar el ambiente de desarrollo python 
 python versionespy
y deberemos ver una salida similar a esta

scipy 
numpy 
matplotlib 
pandas 
statsmodels 
sklearn 

dd aañprraw lh 

 actualizar libreria scikitlearn

en este paso actualizaremos la librería más usada para machine learning en python llamada scikit
learn

en la terminal escribiremos

 conda update scikitlearn

 jpagnato conda update scikitlearn conda conda update scikitlearn x
the following new packages will be installed

imageio pyhd
libcxx heds
libexxabi hebd
libgfortran hfe

the following packages will be updated

astropy nppy pyhab

 bottleneck nppy pyhfa
hspy nppy nppy

 llvmlite py pyhdfed

 matplotlib nppy nppy
numba nppy nppyhe

 numexpr nppy nppy

 numpy py pyha
pandas nppy pyha

 pytables nppy nppy
pywavelets nppy pyhdea

 scikitimage nppy pyhee 
scikitlearn nppy nppy
scipy nppy nppy

 statsmodels nppy pyhddbf

proceed yn h

deberemos confirmar la actualización poniendo y en la terminal

podemos volver a verificar que todo es correcto ejecutando

n

o ad




























crear una red neuronal en python desde cero

el código de la red neuronal



veamos el código recuerda que lo puedes ver y descargar desde la cuenta de github del libro
primero declaramos la clase neuralnetwork

import numpy as np

def sigmoidx

return npexpx

def sigmoidderivadax

return sigmoidxsigmoidx

def tanhx

return nptanhx

def tanhderivadax
return x

class neuralnetwork

def init self layers activationtanh

if activation sigmoid

selfactiva
selfactiva
elif activation
selfactiva
selfactiva

tion sigmoid

tionprime sigmoidderivada
 tanh

tion tanh

tionprime tanhderivada

 inicializo los pesos

selfweights
selfdeltas
 capas 



 rando de pesos varia entre 

 asigno valores aleatorios a capa de entrada y capa oculta
for i in range
r nprandomrandom layersi layersi 

 lenlayers 

selfweightsappendr

 asigno aleatorios a capa de salida

r nprandomrandom layersi layersi 

httpsgithubcomjbagnatomachinelearning

crear una red neuronal en python desde cero 

def

ea

def

selfweightsappendr

fitself x y learningrate epochs

 agrego columna de unos a las entradas x

 con esto agregamos la unidad de bias a la capa de entrada
ones npatleastdnponesxshape

x npconcatenate onest x axis

for k in rangeepochs

i nprandom randintxshape

xi

a

for in range lenselfweights
dotvalue npdotal selfweights
activation selfactivationdotvalue
aappendactivation
 calculo la diferencia en la capa de salida y el valor obtenido
error yi a
deltas error selfactivationprimea

 empezamos en el segundo layer hasta el ultimo
 una capa anterior a la de salida
for in rangelena 
deltasappenddeltas dotselfweightstsel f activation primy

selfdeltasappenddeltas

 invertir
 levelutputlevelhidden levelhiddenleveloutput
deltasreverse 

 backpropagation
 multiplcar los delta de salida con las activaciones de entrada
h para obtener el gradiente del peso
 actualizo el peso restandole un porcentaje del gradiente
for i in range lenselfweights
layer npatleastdai
delta npatleastaddeltasi
selfweightsi learningrate layertdotdelta

if k printepochs k

predictself x

o i añfmsyos e















crear una red neuronal en python desde cero 

ones npatleastdnponesxshape
a npconcatenate nponest nparrayx axis
for in range lenselfweights

a selfactivationnpdota selfweights
return a

def printweightsself
printlistado pesos de conexiones
for i in range lenselfweights
printselfweightsi

def getdeltasself
return selfdeltas

y ahora creamos una red a nuestra medida con neuronas de entrada ocultas y de salida
deberemos ir ajustando los parámetros de entrenamiento learning rate y la cantidad de iteraciones
epochs para obtener buenas predicciones

 funcion coche evita obstáculos
nn neuralnetwork activation tanh
x nparray sin obstaculos

 sin obstaculos
 sin obstaculos
 obstaculo detectado a derecha
 obstaculo a izq
 demasiado cerca a derecha
 demasiado cerca a izq
y nparray avanzar
 avanzar
 avanzar
 giro izquierda
 giro derecha
 retroceder
 retroceder

nnfitx y learningrateepochs

index

for e in x
printxeyyindex networknnpredicte
indexindex

la salidas obtenidas son comparar los valores y con los de network

n

o

l

n

o

l









crear una red neuronal en python desde cero 

x y network 
x y network 
x y network 
x y network 
x y network 
x y network e e
x y network 

como podemos ver son muy buenos resultados aquí podemos ver como el coste de la función se
va reduciendo y tiende a cero

import matplotlibpyplot as plt

deltas nngetdeltas

valores

index

for arreglo in deltas
valores appendarreglo arreglo 
indexindex

pltplotrange lenvalores valores colorb
pltylim 

pltylabel cost

pltxlabel epochs

plttightlayout

pltshow

crear una red neuronal en python desde cero 



 

cost





 
epochs



y podemos ver los pesos obtenidos de las conexiones con nnprintweights pues estos valores
serán los que usaremos en la red final que en el próximo capítulo implementaremos en arduino
para que un cocherobot conduzca sólo evitando obstáculos

resumen

creamos una red neuronal en pocas líneas de código python

 comprendimos cómo funciona una red neuronal básica

 el porqué de las funciones sigmoides y sus derivadas que 
 nos permiten hacer backpropagation
 hallar el gradiente para minimizar el coste
 reducir el error iterando y obtener las salidas buscadas

 logrando que la red aprenda por sí misma en base a un conjunto de datos de entrada y sus
salidas como buen algoritmo supervisado que es

en el próximo capitulo finalizaremos el proyecto al aplicar esta red que construimos en el mundo
real y comprobar si un coche arduino será capaz de conducir por sí mismo y evitar obstáculos

 httpwwwaprendemachinelearningcomwpcontentuploadsdescensogradientepng
ohttpwwwaprendemachinelearningcomprogramauncochearduinoconinteligenciaartificial
 httpwwwaprendemachinelearningcomprogramauncochearduinoconinteligenciaartificial

crear una red neuronal en python desde cero 

recursos

 en donde se enseña la

 el código utilizado es una adaptación del original del bogotobogo
función xor
 pueden descargar el código de este artículo en un jupyter notebook aquí

ó pueden acceder a mi github

ovisualizar online

httpwwwbogotobogocompythonpythonneuralnetworksbackpropagation forxorusingonehiddenlayerphp
ttpwwwaprendemachinelearningcomwpcontentuploadsredneuronaldesdeceroipynb
httpnbviewer jupyterorggithubjbagnatomachinelearningblobmasterredneuronaldesdeceroipynb
httpsgithubcomjbagnatomachinelearning

programa un coche robot arduino
que conduce con ia

el machine learning nos permitirá utilizar redes neuronales para que un coche arduino

conduzca sólo evitando obstáculos

 httpwwwaprendemachinelearningcombrevehistoria delasredesneuronalesartificiales

programa un coche robot arduino que conduce con ia

t

con intelicencia
artificial

programa un coche robot arduino que conduce con ta 

en el capítulo anterior creamos una red neuronal desde cero en python en este artículo
mejoraremos esa red y copiaremos sus pesos a una red con propagación hacia adelante en arduino
que permitirá que el coche robot conduzca sólo sin chocar

la nueva red neuronal

por simplificar el modelo de aprendizaje en el capítulo anterior teníamos una red de tres capas
con neuronas de entrada ocultas y de salida giro y dirección para este ejercicio haremos que
la red neuronal tenga salidas una para cada motor además las salidas serán entre y apagar o
encender motor también cambiaremos las entradas para que todas comprendan valores entre y
 y sean acordes a nuestra función tangente hiperbólica aquí vemos los cambios en esta tabla

entrada entrada salida salida salida salida
sensor posición motor motor motor motor 
distancia obstáculo

 

 

 

 

 

 

 

 

 

 httpwwwaprendemachinelearningcomcrearunaredneuronalenpythondesdecero
httpwwwaprendemachinelearningcomcrearunaredneuronalenpythondesdecero
 httpseswikipediaorgwikitangentehiperbcblica

o ia eu n


























programa un coche robot arduino que conduce con ia 

siendo el valor de los motores y 

acción motor motor motor motor 
avanzar 
retroceder 
giro derecha 
giro izquierda 

para instanciar nuestra red ahora usaremos este código

 red coche para evitar obstáculos

nn neuralnetwork activation tanh

x nparray 
 
 
 






d

 sin obstaculos

 sin obstaculos

sin obstaculos

obstaculo detectado a derecha
obstaculo a izq

obstaculo centro

demasiado cerca a derecha
demasiado cerca a izq

h h h ohoheoh e

demasiado cerca centro

 las salidas y se corresponden con encender o no los motores

y mnparray 









d



 avanzar

 avanzar

avanzar

giro derecha

giro izquierda cambie izq y derecha
avanzar

retroceder

retroceder

h h h h ohoh o e

retroceder

nnfitx y learningrateepochs

def valnnx

return intabsroundx

index
for e in x

prediccion nnpredicte

printxe esperadoyindex obtenido valnnprediccionvalnnpr

ediccionvalnnprediccionvalnnprediccion 

indexindex

ne



o

d

a

instalar el ambiente de desarrollo python 

 python versionespy

 instalar librerías para deep learning

en este paso instalaremos las librerías utilizadas para aprendizaje profundo específicamente serán
keras y la famosa y querida tensorflow de google

para ello ejecutaremos en nuestra línea de comandos

 conda install c condaforge tensorflow

 pip install keras

y crearemos un nuevo script para probar que se instalaron correctamente le llamaremos versiones 
deeppy y tendrá las siguientes lineas

 tensorflow

import tensorflow

printtensorflow s tensorflow version 
 keras

import keras

printkeras s keras version 

ejecutamos en línea de comandos
 python versionesdeeppy

en la terminal veremos la salida
tensorflow 

using tensorflow backend

keras 

ya tenemos nuestro ambiente de desarrollo preparado para el combate

resumen

para nuestra carrera en machine learning y el perfeccionamiento como data scientist necesitamos
un buen entorno en el que programar y cacharrear lease probar cosas y divertirse para ello
contamos con la suite de herramientas gratuitas de anaconda que nos ofrece un entorno amable
y sencillo en el que crear nuestras máquinas en código python

programa un coche robot arduino que conduce con ia 

aquí podemos ver el código python completo modificado de la jupyter notebook y también
vemos la gráfica del coste que disminuye a medida que se entrena tras iteraciones



 

 

cost

 





 
epochs sa

no es impresionante cómo con apenas datos de entrada podemos enseñar a un robot
a conducir

el coche arduino

en mi caso es un coche arduino elegoo uno v de motores si eres maker te resultará fácil
construir el tuyo o puede que ya tengas uno en casa para programarlo el coche puede ser cualquier
otro de hecho podría ser de motores y modificando apenas la red funcionaría en caso de querer
construirlo tu mismo explicaré brevemente los requerimientos

necesitaremos

 una placa arduino uno y una placa de expansión de o
 o puede ser una placa arduino mega
 el controlador de motor ln

ohttpsgithubcomjbagnatomachinelearningblobmasterredneuronalcocheipynb

a httpwwwaprendemachinelearningcomwpcontentuploadsgraficacostemilpng

 httpswwwamazonesgpproductbpzhmtrefaslitlieutfétagaprendemlcamp creativelinkcode
ascreativeasinbpzhmtérlinkidaffceffaccae

programa un coche robot arduino que conduce con ia 

 motores dc o podrían ser y sus ruedas

servo motor sg

sensor ultrasónico

 baterias para alimentar los motores y la placa obviamente
chasis para el coche

 cables

circuito del coche

no entraré en detalle ya que va más allá de los alcances de este libro pero básicamente tenemos el
siguiente circuito ignorar el bluetooth y los sensores infrarrojos

fritzing

montar el coche

utilizaremos un servomotor en la parte delantera del coche que moverá al sensor ultrasónico de
distancia de izquierda a derecha a modo de radar para detectar obstáculos

más allá de eso es un coche pondremos las ruedas y las placas arduino encima del chasis el
objetivo de este capítulo es enseñar a programar una red neuronal en la ide de arduino

este es el video tutorial oficial de ensamblaje de elegoo de este coche

 httpwwwaprendemachinelearningcomwpcontentuploadswireconnectpng
 httpswwwyoutubecomwatchvwyuydvulistplkfeyzkrtzzoglvieipjosgonnkkeindexfragsplcwn

programa un coche robot arduino que conduce con ia 

así nos quedará montado

copiar la red neuronal

una vez obtenida la red neuronal python haremos copiar y pegar de la matriz de pesos en el
código arduino reemplazaremos las lineas y 

shttpwwwaprendemachinelearningcomwpcontentuploadscochearduinoiapng

programa un coche robot arduino que conduce con ia 

 jupyter red neuronaldesdecero q logout
fe edit view inset cel kemel hep trstod pyhon 
b m han ec ce e notily
disabled eho mi
 o 
epochs
he en s
he enb 
in def to strname w he n 
 strwtolistreplace replace meiv
return eloat namesstrwshapelostrimshap de to 
he t
executodin sms inished 
in obtenermos los pesos enfrenados para poder usarlos en el c
pesos nngetweigl 
int inputnodes incluye neurona de
 reemplazar estas lineas en tu codigo arduino int hiddennodes ncluye neurona de bias
 float hiddenweights int outputniodes 
 loat outputweighte ee

 con lo pesos entrenados
na

tostrhiddenveights pesos
printtostroutputweights pesos

 hiddenfhi ddennodes
 output outputnodes

 
 
enecuted in tims finished 

lee el artículo completo en ywwaprendemachineleamingcom

e 

copiamos los pesos que obtenemos en la jupyter notebook de nuestra red neuronal en el código
arduino reemplazando las variables por los nuevos valores

vittar a

el código arduino

el código arduino controlará el servo motor con el sensor de distancia que se moverá de izquierda
a derecha y nos proveerá las entradas de la red distancia y direcciónó giro

el resto lo hará la red neuronal en realidad la red ya aprendió en python es decir sólo
hará multiplicaciones y sumas de los pesos para obtener salidas realizará el camino forward
propagation y las salidas controlarán directamente los motores

hay código adicional para darle ciclos de tiempo a las ruedas a moverse variable accionencurso y
dar más o menos potencia a los motores al cambiar de dirección son relativamente pocas líneas de
código y logramos que la red neuronal conduzca el coche

nota para el movimiento del servomotor se utiliza una librería servo estándard

aquí vemos el código arduino completo tal vez la parte más interesante sea la función conducir

 httpwwwaprendemachinelearningcomwpcontentuploadscopyjupyterarduinogif

oo idíawn 

eobb b w wwwwwwwwwwlbnnnnnnnnennrhehrshehelrera ra
v d oooizaamaww nh ocggia afefg nh e o oxooiddaualmaw haeo

programa un coche robot arduino que conduce con ia

include servo library

servo myservo create servo object to control servo
int echo a
int trig a

define ena 
define enb 
define in 
define in 
define in 
define in 

a i i i i o i i

network configuration

aaa

const int inputnodes incluye neurona de bias
const int hiddennodes incluye neurona de bias
const int outputnodes 

int i j

double accum

double hiddenhiddennodes

double output outputnodes 
float hiddenweights 



 x
 y
 x
 

float outputweights n

end network configuration

aaa

void stop 
digitalwriteena low desactivamos los motores
digitalwriteenb low desactivamos los motores
serial printinstop

medir distancia en centimetros
int distancetest 

 n
 n
 on

 
a i i i i o i i

programa un coche robot arduino que conduce con ia

digitalwritetrig low
delaymicroseconds 
digitalwritetrig high
delaymicroseconds
digitalwritetrig low
float fdistance 
fdistance fdistance 
return intfdistance

void setup 
myservoattach
serialbegin
pinmodeecho input
pinmodetrig output
pinmode in output

pinmode in output
pinmode in output
pinmode in output
pinmodeena output
pinmodeenb output
stop
myservowrite
delay 

unsigned long previousmillis 
const long interval 
int gradosservo 

ultrasonico

bool clockwise true
const long angulomin 
const long angulomax 
double ditanciamaxima 
a actuar la nn
int incrementos 
vo
int accionencurso 
int multiplicador interval
mpo a que el coche pueda girar

const int speed 
vez

pulseinecho high

 para medir
 intervalos
 posicion

 sentido de



 attach servo on pin to servo object

posicion inicial en el centro

ciclos de tiempo
cada x milisegundos

del servo que mueve el sensor 

giro del servo

 distancia de lejania desde la que empieza

 incrementos por ciclo de posicion del ser 

 cantidad de ciclos ejecutando una accion

 multiplica

la cant de ciclos para dar tie x

 velocidad del coche de las ruedas a la

programa un coche robot arduino que conduce con ia

void loop 
unsigned long currentmillis millis

if currentmillis previousmillis interval 
previousmillis currentmillis

aaaobadadoaadaoeekaooeasaebssasob
manejar giro de servo
roassoeaaooeadaoeeassoasssedass 
ifgradosservoangulomin gradosservoangulomax 
clockwiseclockwise cambio de sentido
gradosservo constraingradosservo angulomin angulomax

ifclockwise
gradosservogradosservoincrementos
else
gradosservogradosservoincrementos

ifaccionencurso 
accionencursoaccionencurso

jelse
aa i i i i a i a i a

llamamos a la funcion de conduccion
aaa

conducir



myservowrite gradosservo 

usa la red neuronal ya entrenada
void conducir

double testinput 
double entradaentrada

aaa i i i i i i

obtener distancia del sensor
aaa

double distance doubledistancetest
distance doubleconstraindistance ditanciamaxima



entrada ditanciamaximadoubledistance uso una funcion lin

programa un coche robot arduino que conduce con ia 

neal para obtener cercania
accionencurso entrada multiplicador si esta muy cerca del y
obstaculo necestia mas tiempo de reaccion

aaa i i i i i i
obtener direccion segun angulo del servo

aaa
entrada mapgradosservo anculomin anculomax 
entrada doubleconstrainentrada 

aa i i i i a i a i a
llamamos a la red feedforward con las entradas

rrr

serialprintentrada

serialprintlnentrada

serialprintentrada

serial printlnentrada

testinput bias unit
testinput entrada
testinput entrada

inputtooutputtestinput testinput testinput input to ann to obt 
ain output

int out roundabsoutput
int out roundabsoutput
int out roundabsoutput
int out roundabsoutput

serialprintsalida
serialprintinout
serialprintsalida
serialprintinout
serialprintinoutput
serialprintsalida
serialprintlnout
serialprintsalida

serialprintinout

aaa i i i i i i

impulsar motores con la salida de la red
aaa

int carspeed speed hacia adelante o atras

programa un coche robot arduino que conduce con ia



ifoutut outut si es giro necesita doble fuerza los y

motores

carspeed speed 



analogwriteena carspeed
analogwriteenb carspeed
digitalwritein out high
digitalwritein out high
digitalwritein out high
digitalwritein out high

void inputtooutputdouble in double in double in



double testinput 
testinput in
testinput in
testinput in

aaa i i i i i i

calcular las activaciones en las capas ocultas
roassoeaaooeadaoeeassoasssedass 

for i i hiddennodes i 
accum hiddenweightsinputnodesi 
for j j inputnodes j 
accum testinputj hiddenweightsji 

hiddeni expaccum sigmoid
hiddeni tanhaccum tanh

aaa i i i i i i

calcular activacion y error en la capa de salida
roassoeaaooeadaoeeassoasssedass 

for i i outputnodes i 
accum utputweightshiddennodesi
for j j hiddennodes j 
accum hiddenj outputwweightsji 

outputi tanhaccum tanh

programa un coche robot arduino que conduce con ia 

el coche en acción

conecta tu coche sube el código y pruébalo



veamos un video del coche funcionando con su propia inteligencia artificial en el planeta tierra

resumen

aplicamos machine learning y sus redes neuronales a un objeto del mundo real y vimos cómo
funciona haciendo que el coche evite obstáculos y tome las decisiones por sí mismo sin haberle
dado instrucciones ni código explícito

mejoras a futuro

tengamos en cuenta que estamos teniendo como entradas los datos proporcionados por un sólo
sensor de distancia y un servo motor que nos indica si está a izquierda o derecha podríamos tener
más sensores de distancia infrarrojos medir velocidad luz sonido en fin si tuviéramos que
programar manualmente ese algoritmo tendría una complejidad enorme y sería muy difícil de
mantener o modificar en cambio hacer que una red neuronal aprenda sería muy sencillo tan sólo
agregaríamos features columnas a nuestro código y volveríamos a entrenar nuevamente la red con
las salidas deseadas voila copiar y pegar los pesos obtenidos en arduino y nuestro coche tendría
la inteligencia de manejarse por sí mismo nuevamente

recursos adicionales
 descarga el código python con la nueva red neuronalp

 descarga el código arduino

 si quieres comprar el mismo coche arduino que yo click aqui elegoo uno proyecto kit
de coche robot inteligente v te recuerdo que puedes construir tu propio coche arduino
si te das maña y utilizar este código de hecho puede funcionar con un coche de motores
modificando las salidas de la red en vez de tener tener 

 aquí puedes ver a otro maker que hizo un coche arduino con red neuronal que escapa de la

luz

 httpswwwyoutubecomwatchvbedqjktmusrfeatureemblogo

shttpsgithubcomjbagnatomachinelearningblobmasterredneuronalcocheipynb

 httpsgithubcomjbagnatomachinelearningtreemasternnobstaclecar

ohttpswwwamazonesgpproductbpzhmtrefaslitlieutfétagaprendemlcampcreativelinkcode
ascreativeasinbpzhmtérlinkidaffceffaccae

 httpswwwinstructablescomidarduinoneuralnetworkrobot

instalar el ambiente de desarrollo python

otros artículos de interés en inglés

usar anaconda navigator
instalar pip

instalar tensorflow
instalación de keras

httpsdocsanacondacomanacondanavigator
httpsrecursospythoncomguiasymanualesinstalacion y utilizaciondepipenwindowslinuxysx
httpswwwtensorfloworginstall

thttpskerasioinstallation

una sencilla red neuronal con keras
y tensorflow

crearemos una red neuronal artificial muy sencilla en python con keras y tensorflow para
comprender su uso implementaremos la compuerta xor y compararemos las ventajas del
aprendizaje automático frente a la programación tradicional

requerimientos para el ejercicio

puedes simplemente leer el código y comprenderlo o si quieres ejecutarlo deberás tener un ambiente
de desarrollo python como anaconda para ejecutar el jupyter notebook también funciona con
python en línea de comandos

las compuertas xor

para el ejemplo utilizaremos las compuertas xor si no las conoces o no las recuerdas funcionan
de la siguiente manera tenemos dos entradas binarias ó y la salida será sólo si una de las
entradas es verdadera y la otra falsa es decir que de cuatro combinaciones posibles sólo dos
tienen salida y las otras dos serán como vemos aquí

una red neuronal artificial sencilla con python y
keras

veamos el código completo en donde creamos una red neuronal con datos de entrada las 
combinaciones de xor y sus salidas ordenadas luego analizamos el código linea a linea

 httpwwwaprendemachinelearningcombrevehistoria delasredesneuronalesartificiales
ttpseswikipediaorgwikipuerta xor
httpwwwaprendemachinelearningcominstalarambientededesarrollopythonanacondaparaaprendizajeautomatico
ttpdataspeakslucadcompythonparatodosjupyternotebookhtml

httpseswikipediaorgwikipuerta xor

n

o

l

















una sencilla red neuronal con keras y tensorflow 

import numpy as np
from kerasmodels import sequential
from keraslayerscore import dense

 cargamos las combinaciones de las compuertas xor
trainingdata nparray float

 y estos son los resultados que se obtienen en el mismo orden
targetdata nparray float

model sequential

model adddense inputdim activationrelu

model adddense activationsigmoid

model compile lossmeansquarederror
optimizeradam
metricsbinaryaccuracy

model fittrainingdata targetdata epochs

 evaluamos el modelo
scores model evaluatetrainingdata targetdata

printwns f model metricsnames scores
print model predicttrainingdataround

keras y tensorflow what

utilizaremos keras que es una librería de alto nivel para que nos sea más fácil describir las capas
de la red que creamos y en background es decir el motor que ejecutará la red neuronal y la entrenará
estará la implementación de google llamada tensorflow que es la mejor que existe hoy en día

analicemos la red neuronal que hicimos

importamos las clases que utilizaremos

httpskerasio
httpswwwtensorfloworg

ne

a

d



o

ne

a

una sencilla red neuronal con keras y tensorflow 

import numpy as np
from kerasmodels import sequential
from keraslayerscore import dense

utilizaremos numpy para el manejo de arrays de keras importamos el tipo de modelo sequential y
el tipo de capa dense que es la normal creamos los arrays de entrada y salida

 cargamos las combinaciones de las compuertas xor
trainingdata nparray float

 y estos son los resultados que se obtienen en el mismo orden
targetdata nparray float

como se puede ver son las cuatro entradas posibles de la función xor y sus
cuatro salidas ahora crearemos la arquitectura de nuestra red neuronal

model sequential
model adddense inputdim activationrelu
model adddense activationsigmoid

creamos un modelo vació de tipo sequential este modelo indica que crearemos una serie de capas
de neuronas secuenciales una delante de otra agregamos dos capas dense con modeladdy
realmente serán capas pues al poner inputdim estamos definiendo la capa de entrada con 
neuronas para nuestras entradas de la función xor y la primer capa oculta hidden de neuronas
como función de activación utilizaremos relu que sabemos que da buenos resultados podría ser
otra función esto es un mero ejemplo y según la implementación de la red que haremos deberemos
variar la cantidad de neuronas capas y sus funciones de activación agregamos una capa con 
neurona de salida y función de activación sigmoid

visualización de la red neuronal

veamos que hemos hecho hasta ahora

 httpwwwaprendemachinelearningcombrevehistoriadelasredesneuronalesartificiales

una sencilla red neuronal con keras y tensorflow 

n








a entrenar la red

antes de de entrenar la red haremos unos ajustes de nuestro modelo
model compile lossmeansquarederror
optimizeradam

metricsbinaryaccuracy

con esto indicamos el tipo de pérdida loss que utilizaremos el optimizador de los pesos de las
conexiones de las neuronas y las métricas que queremos obtener ahora sí que entrenaremos la red

model fittrainingdata targetdata epochs

indicamos con modelfit las entradas y sus salidas y la cantidad de iteraciones de aprendizaje
epochs de entrenamiento este es un ejemplo sencillo pero recuerda que en modelos más grandes
y complejos necesitarán más iteraciones y a la vez será más lento el entrenamiento

resultados del entrenamiento

si vemos las salidas del entrenamiento vemos que las primeras linea pone

 httpwwwaprendemachinelearningcomwpcontentuploadsredneuronalxorpng

ne



o

d



l





una sencilla red neuronal con keras y tensorflow 

epoch 

 s msstep loss binaryaccuracy n


epoch 

 s usstep loss binaryaccuracy

y 

con esto vemos que la primer iteración acertó la mitad de las salidas pero a partir de la segunda
sólo acierta de cada luego en la epoch recupera el de aciertos ya no es por suerte
si no por haber ajustado correctamente los pesos de la red

epoch 
 s usstep loss binaryaccurac y
y 

epoch 
 s usstep loss binaryaccurac y
y 

epoch 
 s msstep loss binaryaccuracy y

y en mi caso en la iteración aumenta los aciertos al son de y en la iteración logra el
 de aciertos y se mantiene así hasta finalizar como los pesos iniciales de la red son aleatorios
puede que los resultados que tengas en tu ordenador sean ligeramente distintos en cuanto a las
iteraciones pero llegarás a la precisión binaria binaraaccuracy de 

evaluamos y predecimos

primero evaluamos el modelo

scores model evaluatetrainingdata targetdata
printwns f model metricsnames scores

y vemos que tuvimos un de precisión recordemos lo trivial de este ejemplo y hacemos las 
predicciones posibles de xor pasando nuestras entradas

print model predicttrainingdataround

y vemos las salidas que son las correctas

bo n 

o 








una sencilla red neuronal con keras y tensorflow 

afinando parámetros de la red neuronal

recordemos que este es un ejemplo muy sencillo y con sólo entradas posibles pero si tuviéramos
una red compleja deberemos ajustar muchos parámetros repasemos

 cantidad de capas de la red en nuestro caso son 

 cantidad de neuronas en cada capa nosotros tenemos de entrada en capa oculta y de
salida

 funciones de activación de cada capa nosotros utilizamos relu y sigmoid

 al compilar el modelo definir las funciones de pérdida optimizer y métricas

 cantidad de iteraciones de entrenamiento

puedes intentar variar la cantidad de neuronas de entrada probar con o con y ver qué resultados
obtienes revisar si necesitas más menos iteraciones para alcanzar el de aciertos realmente
podemos apreciar que hay muchos metaparámetros para ajustar si hiciéramos la combinatoria de
todos ellos tendríamos una cantidad enorme de ajustes posibles

guardar la red y usarla de verdad

si esto fuera un caso real en el cual entrenamos una red la ajustamos y obtenemos buenos resultados
ahora deberíamos guardar esa red en un archivo ya que esa red óptima tiene los pesos que estábamos
buscando sería lento entrenar cada vez la red antes de publicar en producción lo que hacemos
es guardar esa red y en otro código cargaríamos la red desde el fichero y la utilizamos como si
fuera una variable u objeto más del script pasándole entradas y obteniendo las predicciones para
guardar y cargar nuestra red utilizaremos el siguiente código

 serializar el modelo a json

modeljson modeltojson

with open modeljson w as jsonfile
jsonfilewritemodeljson

 serializar los pesos a hdf

model saveweights modelh

printmodelo guardado

 mas tarde

 cargar json y crear el modelo
jsonfile open modeljson r
loadedmodeljson jsonfileread
jsonfileclose

n

o

l






una sencilla red neuronal con keras y tensorflow 

loadedmodel modelfromjsonloadedmodeljson
 cargar pesos al nuevo modelo

loadedmodel loadweights model h
printcargado modelo desde disco

 compilar modelo cargado y listo para usar

loadedmodel compile lossmeansquarederror optimizeradam metricsbinaryayx
ccuracy

luego de esto ya usaríamos normalmente loadedmodelpredict y listo

vale la pena una red neuronal

hh porqué no programar con ifthenelse

luego de visto todo esto no conviene hacer una programación tradicional en vez de entrenar una
red neuronal pues siempre dependerá del caso por ejemplo para la función xor tendríamos algo
así

function predecirxorentrada entrada

ifentrada entrada 
return 

jelse ifentrada entrada 
return 

jelse ifentrada entrada 
return 

jelse ifentrada entrada 
return 

vemos que es una función con ifs que evalúa cada condición se podría mejorar lo sé pero
que pasaría si en vez de entradas tuviéramos más parámetros pues seguramente la cantidad
de ifs anidados aumentaría creando un código caótico y propenso a errores difícil de mantener
piénsalo un momento no quiere decir que haya que reemplazar todo el código del mundo con redes
neuronales pero sí pensar en qué casos las redes neuronales nos brindan una flexibilidad y un poder
de predicción increíbles y que justifican el tiempo de desarrollo

una sencilla red neuronal con keras y tensorflow 

resumen

hemos creado nuestra primera red neuronal artificial con capas para recrear la función xor
utilizamos la librería keras y a través de ella tensorflow como backend y creamos el modelo
entrenamos los datos y obtuvimos un buen resultado este es el puntapié inicial para seguir viendo
diversas arquitecturas de redes neuronales e ir aprendiendo a entrenarlas con python

código python

pueden ver el código en la cuenta de github aqui o pueden descargar el código de la notebook
jupyter desde aqui

otros recursos keras y tensorflow

 documentación keras en inglés

 dnn and cnn of keras with minst data in python

 keras ultimate beginers guide to deep leamning in python
 keras and convolutional neural networks



 httpsgithubcomjbagnatomachinelearningblobmasterredneuronalxoripynb

j httpwwwaprendemachinelearningcomwpcontentuploadsredneuronalxoripynb

 httpskerasio
httpscharleshsliaowordpresscomdnnandcnnoftensorflowkeraswithmnistdata
shttpselitedatasciencecomkerastutorial deeplearninginpython

s httpswwwpyimagesearchcomkerasandconvolutionalneuralnetworkscnns

pronóstico de series temporales con
redes neuronales

veremos qué son las series temporales y cómo predecir su comportamiento utilizando redes
neuronales con keras y tensorflow repasaremos el código completo en python y la descarga
del archivo csv del ejercicio propuesto con los datos de entrada

qué es una serie temporal y qué tiene de especial

una serie temporal es un conjunto de muestras tomadas a intervalos de tiempo regulares es
interesante analizar su comportamiento al mediano y largo plazo intentando detectar patrones y
poder hacer pronósticos de cómo será su comportamiento futuro lo que hace especial a una time
series a diferencia de un problema de regresión normal son dos cosas

 es dependiente del tiempo esto rompe con el requerimiento que tiene la regresión lineal

de que sus observaciones sean independientes
 suelen tener algún tipo de estacionalidad ó de tendencias a crecer ó decrecer pensemos en
cuánto más producto vende una heladería en sólo meses al año que en el resto de estaciones

ejemplo de series temporales

 capturar la temperatura humedad y presión atmosférica de una zona a intervalos de 
minutos

 valor de las acciones de una empresa en la bolsa minuto a minuto

 ventas diarias mensuales de una empresa

 producción en kg de una cosecha cada semestre

creo que con eso ya nos hacemos una idea como también pueden entrever las series temporales
pueden ser de sóla variable ó de múltiples

vamos a comenzar con la práctica cargando un dataset que contiene información de casi años de
ventas diarias de productos los campos que contiene son fecha y la cantidad de unidades vendidas

cargar el ejemplo con pandas

aprovecharemos las bondades de pandas para cargar y tratar nuestros datos comenzamos impor
tando las librerías que utilizaremos y leyendo el archivo csv

 httpwwwaprendemachinelearningcomunasencillaredneuronalenpythonconkerasy tensorflow
s httpwwwaprendemachinelearningcomregresionlinealenespanolconpython
nhttpsrawgithubusercontentcomjbagnatomachinelearningmastertimeseriescsv

rn 

o







s s

o

pronóstico de series temporales con redes neuronales 

import pandas as pd

import numpy as np

import matplotlibpylab as plt
zmatplotlib inline

pltrcparamsfigure figsize 
pltstyleusefast

from kerasmodels import sequential
from keraslayers import denseactivationflatten

from sklearnpreprocessing import minmaxscaler

df pdreadcsv timeseriescsv parsedates headernoneindexcol squeex
zetrue names fecha unidades 

dfhead

fecha

 
 
 
 
 
name unidades dtype int

notemos una cosa antes de seguir el dataframe que cargamos con pandas tiene como indice nuestra
primera columna con las fechas esto nos permite hacer filtrados por fecha directamente y algunas
operaciones especiales

por ejemplo podemos ver de qué fechas tenemos datos con

printdfindexmin
printdfindexmax

 
 

presumiblemente tenemos las ventas diarias de y de hasta el mes de noviembre y ahora
veamos cuantas muestras tenemos de cada año

printlendf
printlendf

análisis exploratorio de datos

veremos de qué se trata el eda este paso inicial tan importante y necesario para comenzar
un proyecto de machine learning aprendamos en qué consiste el eda y qué técnicas utilizar
veremos un ejemplo práctico y la manipulación de datos con python utilizando la librería pandas
para analizar y visualizar la información en pocos minutos

qué es el eda

eda es la sigla en inglés para exploratory data analysis y consiste en una de las primeras tareas que
tiene que desempeñar el científico de datos es cuando revisamos por primera vez los datos que nos
llegan por ejemplo un archivo csv y deberemos intentar comprender de qué se trata vislumbrar
posibles patrones y reconocer distribuciones estadísticas que puedan ser útiles en el futuro

lo ideal es que tengamos un objetivo que nos hayan adjuntado con los datos que indique lo
que se quiere conseguir a partir de ellos por ejemplo nos pasan un excel y nos dicen queremos
predecir ventas a días clasificar casos malignosbenignos de una enfermedad queremos
identificar audiencias que van a realizar recompra de un producto queremos hacer pronóstico
de fidelizaciónabandonos de clientes quiero detectar casos de fraude en mi sistema en tiempo

real

eda deconstruido

al llegar un archivo lo primero que deberíamos hacer es intentar responder

e cuántos registros hay
son pocos

son muchos y no tenemos capacidad cpuram suficiente para procesarlo
están todas las filas completas ó tenemos campos con valores nulos

 en caso que haya demasiados nulos queda el resto de información inútil
 qué datos son discretos y cuáles continuos
 muchas veces sirve distinguir el tipo de datos texto int double float
 si es un problema de tipo supervisado

httpswwwaprendemachinelearningcompronosticodeventasredesneuronalespythonembeddings
thttpswwwaprendemachinelearningcomarboldedecisionenpythonclasificaciony prediccion
httpswwwaprendemachinelearningcomkmeansenpythonpasoapaso

t httpswwwaprendemachinelearningcomregresionlogisticaconpythonpasoapaso
shttpswwwaprendemachinelearningcomclasificacioncon datosdesbalanceados

pronóstico de series temporales con redes neuronales 




como este comercio cierra los domingos vemos que de no tenemos días como erróneamente
podíamos presuponer en nos falta el último mes que será lo que trataremos de pronosticar

visualización de datos

veamos algunas gráficas sobre los datos que tenemos pero antes aprovechemos los datos estadísticos
que nos brinda pandas con describe

dfdescribe

count 

d



l

mean 
std 
min 
 
 

 
max 
name unidades dtype float

son un total de registros la media de venta de unidades es de y un desvío de es decir que
por lo general estaremos entre y unidades

de hecho aprovechemos el tener indice de fechas con pandas y saquemos los promedios mensuales

meses dfresamplemmean

ne



l

meses

fecha

pronóstico de series temporales con redes neuronales



































freq m name
















unidades dtype

float

y visualicemos esas medias mensuales

pltplotmeses values
pltplotmeses values























 ohttpwwwaprendemachinelearningcomwpcontentuploadsvtasmensualpng

n

pronóstico de series temporales con redes neuronales 

vemos que en en azul tenemos un inicio de año con un descenso en la cantidad de unidades
luego comienza a subir hasta la llegada del verano europeo en donde en los meses junio y julio
tenemos la mayor cantidad de ventas finalmente vuelve a disminuir y tiene un pequeño pico en
diciembre con la navidad

también vemos que naranja se comporta prácticamente igual es decir que pareciera que
tenemos una estacionalidad podríamos aventurarnos a pronosticar que el verano de también
tendrá un pico de ventas

veamos la gráfica de ventas diarias en unidades en junio y julio

verano df
pltplotveranovalues
verano df
pltplotveranovalues






o







p iw i 

cómo hacer pronóstico de series temporales

una vez que tenemos confirmado que nuestra serie es estacionaria podemos hacer pronóstico
existen diversos métodos para hacer pronóstico en nuestro caso las ventas parecen comportarse
bastante parecidas al año con lo cual un método sencillo si por ejemplo quisiéramos proveer el stock
que necesitaría este comercio sería decir si en en diciembre vendimos promedio unidades

 httpwwwaprendemachinelearningcomwpcontentuploadsvtasveranocomparapng

pronóstico de series temporales con redes neuronales 

pronostico que en diciembre será similar otro método muy utilizado en estadística es el llamado
arima el cual no explicaré aquí pero les dejo un enlace por si están interesados aquí un
gráfica que encontré en twitter sobre la evolución del forecasting

the evolution of forecasting

improvements i furecast muel dsprac mes nene ls a tuscesmental crnje e fe approac ferecastrg te cortreton af demanó modelng ans mactre

paro mo faracasing marve an saatiasica decraro fg anr horo dectars ponony demenr modeingl legeing mil decreass enory ad inst estas by 

no forecasting naive forecasting statistical forecasting demand planning demand modeling machine learning

fa a norecast curve
él elc
serard qa w
s

e
e trerd de e

w ncwq meges

m dere n excal

e error

 lase yaa
las mortrs
dara vs wl n
ajar b motit

accuracy 



z toolsgroup



copyriya tootstraup al mprts reserved

nosotros que somos unos alumnos tan avanzados y aplicados utilizaremos machine learning
en concreto una red neuronal para hacer el pronóstico curiosamente crear esta red es algo
relativamente sencillo y en poco tiempo estaremos usando un modelo de lo más moderno para
hacer las predicciones

httpsenwikipediaorgwikiautoregressiveintegratedmovingaverage
 httpswwwanalyticsvidhyacomblogtimeseriesforecastingcodespython
httpwwwaprendemachinelearningcomwpcontentuploadsevoluciontecnicaspronosticoipg

pronóstico de series temporales con redes neuronales 

pronóstico de ventas diarias con redes neuronal

usaremos una arquitectura sencilla de red neuronal mlp por sus siglas multilayered perceptron
con pocas neuronas y como método de activación tangente hiperbólica pues entregaremos valores
transformados entre y 

vamos al ejemplo

preparamos los datos

este puede que sea uno de los pasos más importantes de este ejercicio

lo que haremos es alterar nuestro flujo de entrada del archivo csv que contiene una columna con
las unidades despachadas y lo convertiremos en varias columnas y porqué hacer esto en realidad
lo que haremos es tomar nuestra serie temporal y la convertiremos en un problema de tipo
supervisado para poder alimentar la red neuronal y poder entrenarla con backpropagation
como es habitual para hacerlo debemos tener unas entradas y unas salidas

tomaremos días previos para obtener el octavo podríamos intentar entrenar a la red con 
días o también podríamos tener sola salida ó hasta atrevernos intentar predecir más de un día
futuro eso lo dejo a ustedes cómo actividad extra pero quedémonos con este modelado

 entradas serán columnas que representan las ventas en unidades de los días anteriores
 salida el valor del vo día es decir las ventas en unids de ese día

para hacer esta transformación usaré una función llamada seriestosupervised creada y explicada
en este blog la verás en el código a continuación

antes de usar la función utilizamos el minmaxscaler para transformar el rango de nuestros

valores y escalarlos entre y pues sabemos que a nuestra red neuronal le favorece para realizar
los cálculos

entonces aqui vemos cómo queda nuestro set de datos de entrada

 httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado

 httpneuralnetworksanddeeplearningcomchaphtml
httpsmachinelearningmasterycomconverttimeseries supervisedlearningproblempython
 httpsscikitlearnorgstablemodulesgeneratedsklearnpreprocessingminmaxscalerhtml

o ia ek an 

v www lynnnnennnennhhhhlsehehh e
iddarobd e oonniaoaddvmeh y y e q koiaauduaapébek wnhehoo

pronóstico de series temporales con redes neuronales

pasos

 convert series to supervised learning
def seriestosuperviseddata nin nout dropnantrue
nvars if typedata is list else datashape
df pddataframedata
cols names list list
 input sequence tn t
for i in rangenin 
colsappenddfshifti
names varzatd j i for j in rangenvars
 forecast sequence t t tn
for i in range nout
colsappend dfshifti
ifi
names varzat j for j in rangenvars
else
names vardtd j i for j in rangenvars
 put it all together
agg pdconcatcols axis
aggcolumns names
 drop rows with nan values
if dropnan
agg dropna inplacetrue
return agg

 load dataset

values dfvalues

 ensure all data is float

values valuesastypefloat

 normalize features

scaler minmaxscaler featurerange 
valuesvaluesreshape esto lo hacemos porque tenemos sola dimension
scaled scalerfittransformvalues

 frame as supervised learning

reframed seriestosupervisedscaled pasos 
reframedhead

oo aod ka n 

c
d o

pronóstico de series temporales con redes neuronales

varit varit

vart

vart

varit

vart

varit



vart

 

 
 
 
 





































usaremos como entradas las columnas encabezadas como vart a t y nuestra salida lo que
sería el valor y de la función será el vart la última columna

creamos la red neuronal artificial



antes de crear la red neuronal subdividiremos nuestro conjunto de datos en train y en test
atención algo importante de este procedimiento a diferencia de otros problemas en los que
podemos mezclar los datos de entrada es que en este caso nos importa mantener el orden
temporal en el que alimentaremos la red por lo tanto haremos una división de los primeros 
días consecutivos para entrenamiento de la red y los siguientes para su validación esta es una
proporción que elegí y que me pareció conveniente pero propongo al lector variar esta proporción
por ejemplo a y comparar resultados

 split into train and test sets

values reframedvalues

ntraindays pasos

train valuesntraindays 

test valuesntraindays 

 split into input and outputs

xtrain ytrain trainf

 train 

xval y val test test 

 reshape input to be d samples timesteps features
xtrain xtrainreshape xtrainshape x trainshape

xval xval reshape xval shape xval shape

printxtrainshape ytrainshape xvalshape yvalshape

 

hemos transformado la entrada en un arreglo con forma esto al castellano significa algo
así como entradas con vectores de x

 httpwwwaprendemachinelearningcomwpcontentuploadsserietosupervisedejemplopng

d



l

d



o

pronóstico de series temporales con redes neuronales 
la arquitectura de la red neuronal será

 entrada inputs como dijimos antes

 capa oculta con neuronas este valor lo escogí yo pero se puede variar

 la salida será sola neurona

 como función de activación utilizamos tangente hiperbólica
entre y 

 utilizaremos como optimizador adam

 puesto que utilizaremos valores

 y métrica de pérdida loss mean absolute error

 como la predicción será un valor continuo y no discreto para calcular el acuracy utilizaremos
mean squared error y para saber si mejora con el entrenamiento se debería ir reduciendo
durante el transcurso de las epochs

def crearmodeloff
model sequential
model adddensepasos inputshapepasosactivationtanh
model addflatten
model adddense activationtanh
model compile lossmeanabsoluteerroroptimizeradam metricsmse
model summary 
return model

entrenamiento y resultados

veamos cómo se comporta nuestra máquina al cabo de épocas
epochs
model crearmodeloff

historymodel fitxtrainytrainepochsepochsvalidationdataxvalyvalbatchx
sizepasos

en pocos segundos vemos una reducción del valor de pérdida tanto del set de entrenamiento como
del de validación

ohttpseswikipediaorgwikitangentehiperbcblica

 httpswwwquoracomcanyouexplainbasicintuitionbehind adamamethodforstochasticoptimization
httpswwwstatisticshowtodatasciencecentralcomabsoluteerror
httpsmediumcomhumaninamachine worldmaeandrmsewhichmetricisbettereacbded

eo n 

o

pronóstico de series temporales con redes neuronales

epoch 
 

ederror valloss valmeansquarederror 

visualizamos al conjunto de validación recordemos que eran días

resultsmodel predictxval

plt scatterrangelenyvalyvalc
pltscatter rangelenresultsresultscr
plttitle validate



 s usstep loss meansquar x

pltshow
validate



 
 s

 
 d 
 



 

 



 

 

 

en la gráfica vemos que los puntitos verdes intentan aproximarse a los rojos cuanto más cerca
ó superpuestos mejor tip si aumentamos la cantidad de epochs mejora cada vez más

veamos y comparemos también cómo disminuye el loss tanto en el conjunto de train como el de
validate esto es bueno ya que indica que el modelo está aprendiendo a su vez pareciera no haber

overfitting

 pues las curvas de train y validate siguen la misma tendencia

httpwwwaprendemachinelearningcomwpcontentuploadsrnvalidationplotpng
 httpwwwaprendemachinelearningcomqueesoverfitting y underfittingycomosolucionarlo

n

o

l





pronóstico de series temporales con redes neuronales

validate loss























pronóstico de ventas futuras







ahora que tenemos nuestra red y la damos por buena probaremos a realizar una nueva predicción

en este caso usaremos los últimos días de noviembre para calcular la primer semana de
diciembre veamos

ultimosdias
ultimosdias

fecha
























 httpwwwaprendemachinelearningcomwpcontentuploadsvisualicelosspng

df

análisis exploratorio de datos 

 cuál es la columna de salida binaria multiclase

 esta balanceado el conjunto salida
cuales parecen ser features importantes cuales podemos descartar
siguen alguna distribución
hay correlación entre features características
en problemas de nlp es frecuente que existan categorías repetidas ó mal tipeadas ó con
mayusculasminúsculas singular y plural por ejemplo abogado y abogadas avogado
pertenecerían todos a un mismo conjunto
estamos ante un problema dependiente del tiempo es decir un timeseries
si fuera un problema de visión artificial tenemos suficientes muestras de cada clase y
variedad para poder hacer generalizar un modelo de machine learning
cuales son los outliers unos pocos datos aislados que difieren drásticamente del resto y
contaminan ó desvían las distribuciones

 podemos eliminarlos es importante conservarlos

 son errores de carga o son reales
tenemos posible sesgo de datos por ejemplo perjudicar a clases minoritarias por no
incluirlas y que el modelo de ml discrimine en sus predicciones

puede ocurrir que tengamos set de datos incompletos y debamos pedir a nuestro clienteproveedor
ó interesado que nos brinde mayor información de los campos que aporte más conocimiento ó que
corrija campos

también puede ocurrir que nos pasen múltiples fuentes de datos por ejemplo un csv un excel y el
acceso a una base de datos entonces tendremos que hacer un paso previo de unificación de datos

qué sacamos del eda

el eda será entonces una primer aproximación a los datos atención si estamos mas o menos bien
preparados y suponiendo una muestra de datos suficiente puede que en unas horas tengamos
ya varias conclusiones como por ejemplo

esto que quiere hacer el cliente con estos datos es una locura imposible
no tenemos datos suficientes ó son de muy mala calidad pedir más al cliente
un modelo de tipo arbol es lo más recomendado usar

 reemplazar arbol por el tipo de modelo que hayamos descubierto como mejor opción
no hace falta usar machine learning para resolver lo que pide el cliente esto es muy
importante

t httpswwwaprendemachinelearningcomprocesamientodellenguajenaturalnlp
httpswwwaprendemachinelearningcompronosticodeseriestemporalesconredesneuronalesenpython
httpswwwaprendemachinelearningcomcomofuncionanlasconvolutionalneuralnetworksvisionporordenador
httpswwwaprendemachinelearningcomdetecciondeoutliersenpythonanomalia
ohttpswwwaprendemachinelearningcomclasificacioncondatosdesbalanceados

 httpswwwaprendemachinelearningcomarboldedecisionenpythonclasificaciony prediccion

ar n 

l

pronóstico de series temporales con redes neuronales 

 
 
 
 
name unidades dtype int

y ahora seguiremos el mismo preprocesado de datos que hicimos para el entrenamiento escalando
los valores llamando a la función seriestosupervised pero esta vez sin incluir la columna de salida
y pues es la que queremos hallar por eso verán en el código que hacemos drop de la última
columna

values ultimosdiasvalues

values valuesastypefloat

 normalize features

valuesvaluesreshape esto lo hacemos porque tenemos sola dimension
scaled scalerfittransformvalues

reframed seriestosupervisedscaled pasos 

reframed dropreframedcolumns axis inplacetrue

reframedhead

varit varit varit varit vart varit varit

 

 
 
 
 
 
 



de este conjunto ultimosdias tomamos sólo la última fila pues es la que correspondería a la última
semana de noviembre y la dejamos en el formato correcto para la red neuronal con reshape

 httpwwwaprendemachinelearningcomwpcontentuploadssettestparapronosticopng

ne

a

d



l






d

a

pronóstico de series temporales con redes neuronales 

values reframedvalues

xtest values 
xtest xtestreshape xtestshape xtestshape

xtest

array 
 dtypefloat

ahora crearemos una función para ir rellenando el desplazamiento que hacemos por cada
predicción esto es porque queremos predecir los primeros días de diciembre entonces para el
 de diciembre ya tenemos el set con los últimos días de noviembre pero para pronosticar el 
de diciembre necesitamos los días anteriores que incluyen al de diciembre y ese valor lo
obtenemos en nuestra predicción anterior y así hasta el de diciembre

def agregarnuevovalorxtestnuevovalor
for i in rangextestshape
xtest i xtest i
xtest xtest shape nuevovalor
return xtest

results

for i in range
parcialmodel predictxtest
resultsappendparcial 
printxtest
xtestagregarnuevovalor xtest parcial 

ya casi lo tenemos ahora las predicciones están en el dominio del al y nosotros lo queremos
en nuestra escala real de unidades vendidas entonces vamos a retransformar los datos con el
objeto scaler que creamos antes y si método inversetransform

adimen x for x in results
inverted scalerinversetransformadimen
inverted

n

 a

eo n 







prediccionsemanadiciembre

prediccionsemanadiciembre

pronóstico de series temporales con redes neuronales

array 






 




 



prediccionsemanadiciembre pddataframe inverted

prediccionsemanadiciembreplot











 

columns pronostico

tocsv pronosticocsv



ya podemos crear un nuevo dataframe pandas por si quisiéramos guardar un nuevo csv con el
pronóstico y lo visualizamos

 pronostico

 



a partir de los últimos días de noviembre y utilizando nuestra red neuronal hicimos el
pronóstico de venta de unidades para la primer semana de diciembre

resumen

durante este nuevo capítulo del aprendizaje automático diferenciamos lo que son las series
temporales y su predicción de los problemas de regresión estándar aprovechamos la capacidad

 httpwwwaprendemachinelearningcomwpcontentuploadspronosticounidadespng

pronóstico de series temporales con redes neuronales 

de las redes neuronales de generalizar y lograr predecir ventas futuras uno de los pasos

más importantes al realizar el preprocesado consiste en convertir nuestra serie en un modelo de
aprendizaje supervisado donde tenemos valores de entrada y salida para poder entrenar la red
y finalizamos realizando pronóstico de una semana utilizando la red neuronal creada

propongo al lector hacer diversas pruebas para mejorar las predicciones alterando parámetros del
ejercicio

 variar la cantidad de epochs
 probar otro optimizador distinto a adam ó configurar valores distintos de learning rate

 cambiar la arquitectura de la red neuronal
 cambiar la cantidad de neuronas de la capa oculta

 agregar más capas ocultas 
 probar utilizando mas de días previos para predecir o probar con menos días

 se puede probar de intentar predecir más de día por vez sin iterar el resultado como hice con
la función agregarnuevovalor 

el próximo capítulo retoma este ejercicio pero aplicando embeddings que puede mejorar la precisión
de las predicciones poniendo en juego el día de la semana y mes que estamos pronosticando
considerándolos como datos adicionales de entrada a la red neuronal para preservar mejor la
estacionalidad

nota recordemos que el futuro es impredecible por lo que debo decir al científico de datos
cuidado sobre todo si debemos predecir resultados de series con comportamiento errático como los
valores de la bolsa y también cautela en asuntos sensibles sobretodo relacionados con la salud

recursos adicionales
 acceder a la jupyter notebook con el ejercicio completo
github
 descargar el archivo de entrada csv con la serie temporal usada en el ejercicio
 herramientas para valorar y mejorar el modelo interpretación de modelos de ml

y siempre algún bonus track en



artículos recomendados en inglés

 how to choose the right forecasting technique

 multivariate time series forecasting with lstms in keras
 time series analysis tutorial using financial data
 a comprehensive beginners guide to create a time series forecast

 httpwwwaprendemachinelearningcomguiadeaprendizaje

 httpwwwaprendemachinelearningcomqueesoverfitting y underfittingycomosolucionarlo
 httpwwwaprendemachinelearningcomaplicaciones delmachinelearningsupervisado
httpsgithubcomjbagnatomachinelearningblobmasterseriestemporalesconrrnnipynb
httpsrawgithubusercontentcomjbagnatomachinelearningmastertimeseriescsv

 httpwwwaprendemachinelearningcominterpretaciondemodelosdemachinelearning
httpshbrorghowtochoosetherightforecastingtechnique

 httpsmachinelearningmasterycommultivariatetimeseriesforecastingistmskeras

 httpstowardsdatasciencecomtimeseriesanalysistutorialusingfinancial datad bf

 httpswwwwanalyticsvidhyacomblogtimeseriesforecastingcodespython

pronóstico de ventas con redes
neuronales parte 

mejora del modelo de series temporales con múltiples
variables y embeddings

este capítulo es la continuación de pronóstico de series temporales con redes neuronales en
python en donde vimos cómo a partir de un archivo de entrada con las unidades vendidas
por una empresa durante años anteriores podíamos estimar las ventas de la próxima semana
continuaremos a partir de ese modelo y haremos propuestas para mejorar la predicción

breve repaso de lo que hicimos

en el modelo del capitulo anterior creamos una red neuronal mlp multilayered perceptron
feedforward de pocas capas y el mayor trabajo que hicimos fue en los datos de entrada puesto
que sólo tenemos un archivo csv con columnas fecha y unidades vendidas lo que hicimos fue
transformar esa entrada en un problema de aprendizaje supervisado para ello creamos un
nuevo archivo de entrada con columnas en donde poníamos la cantidad de unidades vendidas
en los días anteriores y de salida la cantidad de unidades vendidas en la fecha actual de esa
manera alimentamos la red y ésta fue capaz de realizar pronósticos aceptables sólo utilizamos la
columna de unidades pero no utilizamos la columna de fecha podría ser la columna de fecha un
dato importante podría mejorar nuestra predicción de ventas

mejoras al modelo de series temporales

esto es lo que haremos propongo nuevos modelos con redes neuronales feedforward para
intentar mejorar los pronósticos de ventas

 un primer modelo tomando la fecha como nueva variable de entrada valiosa y que aporta
datos

 un segundo modelo también usando la fecha como variable adicional pero utilizándo embed
dings y a ver si mejora el pronóstico

 httpwwwaprendemachinelearningcompronosticodeseriestemporalesconredesneuronalesenpython
aohttpwwwaprendemachinelearningcombrevehistoria delasredesneuronalesartificiales

a httpwwwaprendemachinelearningcompasosmachinelearningconstruirmaquina

 httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado
ashttpwwwaprendemachinelearningcomaprendizajeprofundounaguiarapida

pronóstico de ventas con redes neuronales parte 

por lo tanto explicaremos qué son los embeddings utilizados en variables categóricas se utiliza
mucho en problemas de procesamiento del lenguaje natural nlp para modelar

para estos modelos propuestos haremos la transformación a problema de aprendizaje supervi
sado para ello usaremos la misma función seriestosupervised de la web machinelearningmas
tery como en el artículo anterior

primer mejora serie temporal de múltilples variables

puede que el ejemplo clásico para comprender lo que son las series temporales de múltiples
variables sea el pronóstico del tiempo en donde tenemos varias columnas de entrada con
la temperatura la presión atmosférica humedad con esas tres variables tendremos una mejor
predicción de la temperatura de mañana que si tan sólo usásemos una sola feature

fecha como variable de entrada

como solamente tenemos un dato de entrada las unidades vendidas en el día intentaremos
enriquecer a la red con más entradas para ello usaremos la fecha pero cómo bueno aprovecha
remos que podemos saber cada día que hubo ventas si fue un lunes martes por ejemplo algunos
comercios venden más los viernes y sábados también como vimos que en cuanto a estacionalidad
en verano europeo subían las ventas la red neuronal debería percatarse de eso y mejorar su
puntería entendiendo que eso ocurre en los meses y lo hará no agregaré los años pues sólo
tenemos y muy pocos pero si tu cuentas con un dataset con muchos años sería bueno
agregar como entrada también los años

en limpio usaremos el día como variable categórica con valores de a indicando día de semana
y usaremos el número de mes como otra variable categórica la intuición es que la red entenderá
las estacionalidades dadas entre semana y mensuales

segunda mejora embeddings en variables categóricas

bien para el segundo modelo utilizaremos embeddings en las variables categóricas es decir en la
columna de día y de mes los valores de día van del al representando los días de la semana pero
no quiere decir que el día vale más que el día son identificadores no tendría sentido decir que
jueves es mayor que domingo sin embargo la red neuronal esto no lo sabe y podría interpretar
erróneamente esos valores categóricos con los meses lo mismo van del al pero no quiere
decir que diciembre valga más que agosto y de hecho sabemos en la práctica para este ejercicio
que realmente en julio y agosto es cuando más aumentan las ventas para intentar resolver esta
problemática es que aparecen los embeddings

o httpwwwaprendemachinelearningcomprocesamientodellenguajenaturalnlp
shttpsmachinelearningmasterycomconverttimeseries supervisedlearningproblempython

pronóstico de ventas con redes neuronales parte 

qué son los embeddings

la traducción al español de embed es incrustar y esto a simple vista no nos ayuda mucho google
lo traduce en uno de sus tutoriales como incorporaciones los embeddings son una manera de dar
valoración útil a datos categóricos para ello asignaremos una profundidad a cada identificador
es decir un vector con valores continuos inicialmente aleatorios esos valores se ajustarán
con backpropagation al igual que nuestra red neuronal y finalmente nuestros datos categóricos
quedan enriquecidos y dejan de ser lunes para ser unos vectores con valores que significan algo
qué significan para simplificar podemos decir que esos vectores acercan identificadores similares
entre sí y distancia a los opuestos un ejemplo cuando se utiliza en natural language processing
nlp con un gran número de palabras los embeddings logran hacer que palabras sobre sentimientos
positivos alegríafelicidad queden cercanas pero distanciadas de las que significan sentimientos

e

negativos odiotristeza

 en el cual se crea un

otro caso de uso también muy conocido llamado filtrado colaborativo
motor de recomendaciones de películas entonces se sitúan en un espacio vectorial las películas
infantiles en un extremo y las de adultos en otro a su vez otra coordenada indica si la película es

más comercialtaquillera ó más artística

o httpsdevelopersgooglecommachinelearningcrashcourseembeddingsvideolecture
a httpwwwaprendemachinelearningcomaprendizajeprofundounaguiarapida

ao httpwwwaprendemachinelearningcomprocesamientodellenguajenaturalnlp

 httpsdevelopersgooglecommachinelearningcrashcourseembeddingsvideolecture

pronóstico de ventas con redes neuronales parte 

éxito
s

 batman el
shrek
 la guerra de caballero de la el tigre y
las galaxias noche asciende el dragón

adulto

las trillizas
de belleville

en este caso simplificado los embeddings se pueden ver como vectores de coordenadas xy que
acercan películas similares en dimensiones y a su vez quedan distanciadas de identificadores
opuestos

sobre la dimensionalidad de los embeddings

es bueno usar muchas dimensiones profundidad para modelar nuestras variables categóricas pero
ojo si tiene demasiadas puede ocurrir overfitting entonces habrá que hacer prueba y error hay
una regla que dice que hay que usar una cuarta parte del tamaño de la variable categórica si
tenemos identificadores usaremos como profundidad en el curso de fastai recomiendan
un máximo de ó la cantidad de elementos categóricos más uno dividido dos si es menor a 
pero siempre dependerá del caso

conclusión de embeddings

al asignarle vectores con valor numérico continuo a entradas categóricas estos terminan funcio
nando como una mini red neuronal dentro de la red principal aprenden con backpropagation

ohttpwwwaprendemachinelearningcomqueesoverfitting y underfittingycomosolucionarlo
 httpcourse fastailessonslessonhtml
httpneuralnetworksanddeeplearningcomchaphtml

pronóstico de ventas con redes neuronales parte 

y resuelven como valores continuos esos identificadores discretos acentuando su valor intrínseco

una ventaja de usar embeddings es que se pueden reutilizar una vez entrenados podemos
guardarlos para utilizarlos en otra red gracias a esto es que encontramos archivos de embeddings
con millones de palabras ya entrenadas por google listos para descargar y usar

quiero python

dejaré enlace a los códigos pues esta vez me quiero centrar más en las comparaciones y conclusiones
de cada modelo y no tanto en su implementación aquí los enlaces de las notebooks

 código modelo red neuronal con una variable
 código modelo serie temporal multiples variables
 código modelo series temporales con embeddings





comparemos los resultados de los modelos

para intentar que las comparaciones sean lo más justas posibles utilizaremos en las redes neuro
nales las mismas funciones de activación tanh misma optimización adam y métricas loss y
score meanabsoluteerror y meansquarederror además en todos los casos ejecutaremos
 epochs

por comodidad llamaremos a los modelos
 serie temporal de variable st

 serie temporal de multiples variables stmv
 serie temporal con embeddings ste

comparemos las métricas

vemos los valores finales de las métricas tras las epochs

shttpsgithubcommmihaltzwordvecgooglenewsvectors

 httpsnbviewerjupyterorggithubjbagnatomachinelearningblobmasterseriestemporalesconrrnnipynb
httpsnbviewerjupyterorggithubjbagnatomachinelearningblobmasterseriestemporalesmultivariateipynb
 httpsnbviewerjupyterorggithubjbagnatomachinelearningblobmasterseriestemporalesembeddingsipynb
httpseswikipediaorgwikitangentehiperbcblica

 shttpswwwquoracomcanyouexplainbasicintuitionbehind adamamethodforstochasticoptimization

 httpswwwstatisticshowtodatasciencecentralcomabsoluteerror
httpsmediumcomhumaninamachineworldmaeandrmsewhichmetricisbettereacbded

pronóstico de ventas con redes neuronales parte 

modelo loss valloss mse valmse
sti 
stmv 
ste 

el modelo st y stmv quedan prácticamente iguales es decir que tras agregar las variables de
fecha no pareciera haber mejoría en las métricas sin embargo el modelo con embeddings sí que
logra una mejora algo más evidente el validationloss pasa de a y el validationmse de
 a 

comparemos gráficas de pérdida loss

en las tres gráficas vemos que la métrica de loss en los sets de entrenamiento y validación descienden
y se mantiene estables bueno en la del segundo modelo stmv la curva de validación es algo errática
las curvas del modelo y se mantienen sobre el mientras que la del er modelo desciende
algo más en torno del 

validate loss



 

 

 

 
t t t t t t t t t
 

modelo st en azul el entrenamiento y naranja el set de validación

análisis exploratorio de datos 

 es todo tan aleatorio que no habrá manera de detectar patrones
 hay datos suficientes y de buena calidad como para seguir a la próxima etapa

a estas alturas podemos saber si nos están pidiendo algo viable ó si necesitamos más datos para
comenzar

repito el eda debe tomar horas ó puede que un día pero la idea es poder sacar algunas conclusiones
rápidas para contestar al cliente si podemos seguir o no con su propuesta

luego del eda suponiendo que seguimos adelante podemos tomarnos más tiempo y analizar en
mayor detalle los datos y avanzar a nuevas etapas para aplicar modelos de machine learning

técnicas para eda

vamos a lo práctico que herramientas tenemos hoy en día la verdad es que como cada conjunto
u único uir u

de datos suele ser único el eda se hace bastante a mano pero podemos seguir unos pasos

ordenados para acercarnos a ese objetivo que nos pide el cliente en pocas horas

a nivel programación y como vamos a utilizar python encontramos a la conocida librería pandas
que nos ayudará a manipular datos leer y transformarlos

otra de las técnicas que más nos ayudaran en el eda es la visualización de datos que también
podemos hacer con pandas matplotlib yo plotly

finalmente podemos decir que nuestra intuición basada en experiencia previa no en corazonadas
y nuestro conocimiento de casos similares también nos pueden aportar pistas para saber si estamos
ante datos de buena calidad por ejemplo si alguien quiere hacer reconocimiento de imágenes de
tornillos y tiene imágenes y con mala resolución podremos decir que no tenemos muestras
suficientes dado nuestro conocimiento previo de este campo

vamos a la práctica

un eda de pocos minutos con pandas

vamos a hacer un ejemplo en pandas de un eda bastante sencillo pero con fines educativos

vamos a leer un csv directamente desde una url de github que contiene información geográfica
básica de los países del mundo y vamos a jugar un poco con esos datos

httpswwwaprendemachinelearningcompasos machinelearningconstruirmaquina

pronóstico de ventas con redes neuronales parte 

validate loss








 vv

 
modelo stmv en azul el entrenamiento y naranja el set de validación

validate loss













 

modelo ste en azul el entrenamiento y naranja el set de validación

pronóstico de ventas con redes neuronales parte 

comparemos las gráficas de accuracy

utilizamos la métrica de mse y vemos que nuevamente el modelo y se comportan similar y se
sitúan sobre el y el modelo con embeddings desciende hasta el indicando una mejora

accuracy



















comparamos los pronósticos y sus aciertos

vamos a hacer una comparación visual de los pronósticos realizados sobre el set de validación y
marcar los aciertos en azul los puntos reales y en naranja las predicciones

pronóstico de ventas con redes neuronales parte 











 
modelo st con aciertos pero pronóstico conservador





 

modelo stmv bastante similar al modelo pero con algo mayor de amplitud

pronóstico de ventas con redes neuronales parte 











 

modelo ste los embeddings proveen mayor flexibilidad a la curva de pronóstico y aciertos

podemos ver que la primera red es más conservadora manteniéndoselo en la media de 
unidades sin picos bruscos el segundo modelo tiene algo más de amplitud en sus predicciones y
la red neuronal que mejor se comporta es la tercera que evidentemente gracias a los embeddings
logra pronosticar mejor los valores y vemos picos más alejados de la media de que son buenos
aciertos

resumen

como primer conclusión podemos decir que mejoran las predicciones al agregar más variables de
entrada a la red realmente notamos mejoría con nuestro modelo al usar embeddings en la red
neuronal

nota recordemos que hay muchos de los parámetros para tunear que hemos fijado arbitraria
mente al igual que en artículo anterior animo al lector a variar esos parámetros en los modelos
para mejorarlos empezando por la cantidad de epochs aumentar a ó la variable de pasos
que está en probar con ó con 

nota en el modelo de múltiples variables hicimos un truco tomando como variable adicional
la fecha pero realmente estaría bien tener otras variables con datos útiles

podemos ver cómo el machine learning puede a partir de relativamente pocos datos sacar de su
galera nuevas herramientas y adaptar y mejorar el modelo en este caso sabemos que podemos
utilizar las variables categóricas a través de embeddings y mejorar sustancialmente los resultados
obtenidos en una red neuronal

pronóstico de ventas con redes neuronales parte 

recursos adicionales



código modelo red neuronal con una variable

código modelo serie temporal multiples variables



código modelo series temporales con embeddings
archivo csv de entrada utilizado en los modelos

otros artículos recomendados en inglés

 neural networks embeddings explained
 multivariate time series forecasting with lstms in keras
 an introduction to deep learning for tabular data



 httpsnbviewerjupyterorggithubjbagnatomachinelearningblobmasterseriestemporalesconrrnnipynb
httpsnbviewerjupyterorggithubjbagnatomachinelearningblobmasterseriestemporalesmultivariateipynb
 httpsnbviewerjupyterorggithubjbagnatomachinelearningblobmasterseriestemporalesembeddingsipynb
 httpsrawgithubusercontentcomjbagnatomachinelearningmastertimeseriescsv

 httpstowardsdatasciencecomneuralnetworkembeddingsexplaineddef
shttpsmachinelearningmasterycommultivariatetimeseriesforecastingistmskeras

 httpswwwfastaicategoricalembeddings

crea tu propio servicio de machine
learning con flask

dale vida a tu ia

ya tienes tu modelo probado funciona bien y está listo para entrar en acción entonces cómo lo
desplegamos si es una solución que quieres ofrecer al público desde la nube puedes implementar
tu propio servicio online y ofrecer soluciones de machine learning

veamos cómo hacerlo

implementar modelos de machine learning

muchas veces el modelo creado por el equipo de machine learning será una pieza más de un
sistema mayor como por ejemplo una app un chatbot algún sistema de marketing un sistema de
monitoreo de seguridad y si bien el modelo puede correr en python o r es probable que interactúe
con otro stack distinto de desarrollo por ejemplo una app android en java ó kotlin algún sistema
php en la nube ó hasta podría ser aplicaciones de escritorio o crm entonces nuestro modelo deberá
ser capaz de interactuar y servir a los pedidos de esas otras herramientas

podríamos reescribir nuestro código en otro lenguaje javascript java c ó si nuestro proceso
ejecutara batch podría ser una tarea cron del sistema operativo que ejecute automáticamente
cada x tiempo y deje un archivo de salida csv ó entradas en una base de datos

pero si nuestro modelo tiene que servir a otros sistemas en tiempo real y no podemos reescribirlo
incluso por temas de mantenimiento futuro actualización ó imposibilidad de recrear módulos
completos de python podemos desplegar nuestro modelo en una api accesible desde un servidor
que podría ser público ó privado y mediante una clave secreta si hiciera falta

servir mediante una api

una api es la manera más flexible que hay para ofrecer servicios online en la actualidad sin
meterme en profundidad en el tema podemos decir que lo que hacemos es publicar un punto de
entrada desde donde los usuarios clientes apps u otras máquinas harán peticiones de consulta
inserción actualización o borrado de los datos a los que tienen acceso en nuestro caso lo típico
será ofrecer un servicio de machine learning de predicción ó clasificación entonces nos llegarán en

 httpswwwaprendemachinelearningcomprincipalesalgoritmosusados enmachinelearning

d



l





crea tu propio servicio de machine learning con flask 

la petición get ó post las entradas que tendrá el modelo nuestras features ó lo que normalmente
son las columnas de un csv que usamos para entrenar y nuestra salida podría ser el resultado de
la predicción ó una probabilidad ó un número por ej cantidad de ventas pronosticadas para ese
día

para crear una api podemos utilizar diversas infraestructuras ya existentes en el mercado que
ofrecen google amazon microsoft u otros ó podemos levantar nuestro propio servicio con flask
flask es un web framework en python que simplifica la manera de publicar nuestra propia api hay
otros como django falcon y más

instalar flask

veamos rápidamente como instalar y dejar montado flask

 instalar anaconda en el servidor ó en nuestra máquina local para desarrollo para servidores
también puedes usar la versión de miniconda

 prueba ejecutar el comando conda en el terminal para verificar que esté todo ok

e crear un nuevo environment en el que trabajaremos conda create name miambiente
python

e activa el ambiente creado con source activate miambiente

 instalar los paquetes python que utilizaremos pip install flask gunicorn nota para
usuarios windows utilizar waitress

hagamos un hello world con flask crea un archivo de texto nuevo llamado miserverpy

creando un servidor flask

la

from flask import flask
app flask name

gapproute users
def helloworldnombrenone

returnhola format nombre

guarda el archivo y escribe en la terminal

 httpswwwaprendemachinelearningcominstalarambiente de desarrollopythonanacondaparaaprendizajeautomatico
ohttpscondaiominicondahtml

 httpsdocspylonsprojectorgprojectswaitressenlatest
httpsgithubcomjbagnatomachinelearningblobmasterapimlmiserverpy

crea tu propio servicio de machine learning con flask 

gunicorn bind 

 miserver app

una vez iniciado verás algo así

s s jbagnato gunicorn bind mi serverapp python gunicorn bind mi serverapp
jupyter jupyternotebook pyihon gunicorn 

entonces abre tu navegador web favorito y entra en la ruta httplocalhost users juan

 localhostusersjuan ú
 múútzílíoítlot
hola juan

con eso ya tenemos nuestro servidor ejecutando en breve haremos cambios para poder servir
nuestro modelo de machine learning desde flask al mundo 



nota usuarios windows seguir estas instrucciones para el módulo waitress

 httpswwwaprendemachinelearningcompasosmachinelearningconstruirmaquina
 httpsstackoverflowcomquestionsservingflaskappwithwaitressonwindows

ae o n 

o






























crea tu propio servicio de machine learning con flask 

crear el modelo de ml

hagamos un ejemplo de un modelo de ml basándonos en el ejercicio de pronóstico de series
temporales que hace un pronóstico de ventas con redes neuronales con embeddings esta vez no
usaremos una notebook de jupyter si no archivos de texto plano python

import pandas as pd
import numpy as np
from sklearnpreprocessing import minmaxscaler

from utiles import 

df pdreadcsv timeseriescsv parsedates headernoneindexcol names xn
 fecha unidades

dfweekdayxweekday for x in dfindex

dfmonthxmonth for x in dfindex

printdfhead

epochs
pasos

scaler minmaxscaler featurerange 
reframed transformar df scaler

reordenadoreframed weekday month vartvartvartvartx
vartvartvartvart 
reordenado dropna inplacetrue

trainingdata reordenadodrop vart axis
targetdatareordenado vart

cant lendfindex

validdata trainingdatacantcant
validtargettargetdatacantcant

trainingdata trainingdatacant

targetdatatargetdatacant

printtrainingdatashape targetdatashape validdatashape validtargetshape
printtrainingdatahead 

model crearmodeloembeddings

crea tu propio servicio de machine learning con flask 

continuas trainingdata vartvartvart vartvartx
 vartvart

validcontinuas valid data vartvartvartvartvarx
tvartvart

history modelfit trainingdataweekdaytrainingdatamonthcontinuas tarx
getdata epochsepochs

validationdata validdata weekdayvaliddatamonthvalixn
dcontinuasvalidtarget

results model predict validdataweekdayvaliddatamonthvalidcontinuas

print resultados escaladosresults 
inverted scalerinversetransformresults
print resultadosinverted 

ya logramos entrenar un nuevo modelo del que estamos conformes ahora veamos cómo guardarlo
para poder reutilizarlo en la api

guardar el modelo serialización de objetos en python

el proceso de serialización consiste en poder transformar nuestro modelo ml que es un objeto
python en ceros y unos que puedan ser almacenados en un archivo y que luego al momento de la
carga vuelva a regenerar ese mismo objeto con sus características

aunque existen diversas maneras de guardar los modelos comentemos rápidamente las que
usaremos

 pickle de python para almacenar objetos en nuestro caso un transformador que debemos
mantener para reconvertir los resultados escalados al finalizar de entrenar

 hpy para el modelo keras podemos guardar el modelo completo ó los pesos asociados a la
red

oo idíawn 

análisis exploratorio de datos 
import pandas as pd
import numpy as np
import matplotlibpyplot as plt
import statsmodelsapi as sm
url httpsrawgithubusercontentcomloreylistofcountriesmastercsvcountriey
scsv
df pdreadcsvurl sep
printdfhead
alpha alpha area capital continent currency code
 ad and aandorra la vella eu eur
 ae are abu dhabi as aed
 af afg kabul as afn
 ag atg st johns nan xcd
 ai aia the valley nan xxcd
currency name eqivalent fips code fips geoname id languages
 euro nan an ca
 dirham nan ae araefaenhiur
 afghani nan af faafpsuzaftk
 dollar nan ac enag
 dollar nan av enali
name neighbours numeric phone population
 andorra esfr 
 uunited arab emirates saom 
 afghanistan tmcnirtjpkuz 
 aantigua and barbuda nan 
 anguilla nan 
postal code format postal code regex tild
 adaae advds ad
 nan nan ae
 nan nan af
 nan nan ag
 nan nan ai

veamos los datos básicos que nos brinda pandas
cantidad y nombre de columnas

print cantidad de filas y columnasdfshape

printnombre columnas dfcolumns

n

o ad












crea tu propio servicio de machine learning con flask 

import pickle

definimos funciones de guardar y cargar
def saveobject filename object

with openfilename wb as file
pickledumpobject file

def loadobject filename
with openfilename rb as f
loaded pickleload f
return loaded

 guardamos los objetos que necesitaremos mas tarde
saveobject scalertimeseriespk scaler
model saveweights pesosh

 cargamos cuando haga falta

loadedscaler loadobject scalertimeseriespkl
loadedmodel crearmodeloembeddings
loadedmodel loadweights pesos h

podemos comprobar a ver las predicciones sobre el set de validación antes y después de guardar los
objetos y veremos que da los mismos resultados

crear una api con flask

ahora veamos el código con el que crearemos la api y donde incorporaremos nuestro modelo

utilizaremos los siguientes archivos

e serverpy el servidor flask

e testapipy ejemplo de request post para probar la api

e utilespy las funciones comunes al proyecto

 apitrain modelpy entreno y creación del modelo una red neuronal con embeddings del
ejercicio de timeseries

e timeseriescsv archivo con datos para el ejercicio

vamos a la acción

shttpsgithubcomjbagnatomachinelearningblobmasterapimlserverpy
httpsgithubcomjbagnatomachinelearningblobmasterapimltestapipy

 httpsgithubcomjbagnatomachinelearningblobmasterapimlutilespy

 httpsgithubcomjbagnatomachinelearningblobmasterapimlapitrainmodelpy

 httpswwwaprendemachinelearningcompronosticodeventasredesneuronales pythonembeddings
ohttpsgithubcomjbagnatomachinelearningblobmasterapimltimeseriescsv

o i añfmsyos e






























crea tu propio servicio de machine learning con flask 

 crearemos un método inicial que será invocado desde la url predict
 cargaremos el modelo que entrenamos previamente
 responderemos peticiones en formato json

filename serverpy

la

import pandas as pd

from sklearnexternals import joblib

from flask import flask jsonify request

from utiles import 

app flask name

gapproutepredict methodspost
def predict
api request

m

try

regjson requestgetjson
input pdreadjsonregjson orientrecords

except exception as e

raise e

if inputempty

returnbadrequest 

else

load the saved model

printcargar el modelo
loadedmodel crearmodeloembeddings
loadedmodel loadweights pesos h

printhacer pronosticos
continuas inputvartvartvart vartvartx

 vartvart

uas

predictions loadedmodel predict inputweekday inputmonth continxn

printtransformando datos

loadedscaler loadobject scalertimeseriespkl
inverted loadedscalerinversetransformpredictions
inverted invertedastypeint

o i añeaod
















crea tu propio servicio de machine learning con flask 

finalpredictions pddataframe inverted
finalpredictionscolumns ventas

printenviar respuesta

responses jsonifypredictionsfinal predictionstojsonorientrecords
responses statuscode 

printfin de peticion

return responses
muy bien podemos ejecutar nuestra api desde la terminal para testear con
gunicorn bind server app

nota usuarios windows realizar la función similar de waitress

y ahora hagamos una petición para probar nuestra api con un archivo python y veamos la salida

import json

import requests
import pandas as pd
import pickle

from utiles import 

setting the headers to send and accept json responses
header contenttype applicationjson xn
accept applicationjson

 creamos un dataset de pruebas

df pddataframe unidades 
weekday 
month 

loadedscaler loadobject scalertimeseriespkl
reframed transformar df loadedscaler
reordenadoreframed weekday month vartvartvartvartx

vartvartvart 
reordenado dropna inplacetrue

crea tu propio servicio de machine learning con flask 

converting pandas dataframe to json

la

data reordenadotojsonorientrecords
printjson para enviar en post data

post predict

la

resp requestsposthttplocalhostpredict x
data jsondumpsdata n
headers header

printstatus resp statuscode

the final response we get is as follows

la

printrespuesta de servidor
printresp json

este ejemplo nos devuelve de salida

predictions ventas

actualizar el modelo según sea necesario

no olvidar que si nuestro modelo es dependiente del tiempo ó de datos históricos ó datos nuevos
que vamos recopilando nuevas muestras deberemos reentrenar el modelo

eso se podría automatizar reejecutando el archivo de entrenamiento y sobreescribiendo el archivo
de los pesos modelo hpy que habíamos generado antes cada x días ó a raíz de otro evento que en
nuestro negocio sea significativo y sea el detonador del reentreno

resumen

en este artículo vimos que nuestro modelo de ml puede llegar a ser una pequeña pieza de un puzzle
mayor y puede ofrecer soluciones a usuarios finales ó a otros subsistemas para poder ofrecer sus
servicios podemos contar con diversas soluciones siendo una de ellas el despliegue de una api
podemos crear una api fácil y rápidamente con el web framework de flask ya puedes ofrecer tus
modelos al mundo

notas finales recuerda ejecutar los archivos en el siguiente orden

crea tu propio servicio de machine learning con flask 

 copia el archivo timeseriescsv con los datos del ejercicio en tu server

 entrena el modelo y crea los archivos necesarios con python apitrainmodel py

 inicia el server desde una terminal con gunicorn como vimos anteriormente usuarios
windows con waitress

 ejecuta el archivo de pruebas desde otra terminal con python testapipy

recursos adicionales

descarga los archivos creados en este artículo
 archivos en github

otros artículos relacionados en inglés

 flask with embedded machine learning

 httpsgithubcomjbagnatomachinelearningtreemasterapiml
 httpswwwbogotobogocompythonflaskpythonflaskembeddingmachinelearningphp

clasificación de imágenes en python

crearemos una convolutional neural network con keras y tensorflow en python para reconoci
miento de imágenes

en este capítulo iremos directo al grano veremos el código que crea la red neuronal para visión
artificial en el próximo capítulo explicaré bien los conceptos utilizados pero esta vez haremos un
aprendizaje topdown 

ejercicio clasificar imágenes de deportes

para el ejercicio se me ocurrió crear mi propio dataset mnist con imágenes de deportes para
ello seleccioné los deportes más populares del mundo según la sabiduría de internet que
son fútbol basket golf fútbol americano tenis fórmula ciclismo boxeo beisball y natación
enumerados sin orden particular entre ellos obtuve entre y imágenes de cada deporte
a partir de videos de youtube usando ffmpeg las imágenes están en tamaño diminuto de x
pixeles en color y son un total de si bien el tamaño en pixeles es pequeño es suficiente
para que nuestra red neuronal pueda distinguirlas increíble no



el objetivo es que nuestra máquina red neuronal convoluciona aprenda a clasificar

por sí sóla dada una nueva imagen de qué deporte se trata

 httpwwwaprendemachinelearningcomcomofuncionanlas convolutionalneuralnetworks visionporordenador
httpsclasstroomsynonymcomdifferencebetweentopdownteachingbottomupteachinghtml
shttpsdeeplearninglaptopcomblogmnist

 httpsfimpegorg

 httpwwwaprendemachinelearningcomcomofuncionanlasconvolutionalneuralnetworks visionporordenador

clasificación de imágenes en python 

ejemplo de imágenes de los deportes más populares del mundo

dividiremos el set de datos en para entrenamiento y para test a su vez el conjunto de
entrenamiento también lo subdividiremos en otro para entrenamiento y validación en
cada iteración epoch de aprendizaje

 httpsgsaiblogspotcomepochbatchanditerationhtml

clasificación de imágenes en python 



una muestra de las imágenes del dataset que he titulado sportsmnist contiene más de 
imágenes de los deportes más populares del mundo

clasificación de imágenes en python 

requerimientos técnicos

necesitaremos tener python y como lo haremos en una notebook jupyter recomiendo tener
instalada una suite como anaconda que nos facilitará las tareas además instalar keras y
tensorflow como backend

necesitarás descargar el archivo zip con las imágenes están comprimidas y decomprimirlas en el
mismo directorio en donde ejecutarás la notebook con el código al descomprimir se crearán 
subdirectorios con las imágenes uno por cada deporte

 descarga las imágenes mnistdeportes mb no olvides descomprimir el zip
 descarga la jupyter notebook con el código python

vamos al código python

por más que no entiendas del todo el código sigue adelante intentaré explicar brevemente
qué hacemos paso a paso y en el próximo capítulo se explicará cada parte de las cnn
convolutional neural networks también encontrarás al final varios enlaces con información
adicional que te ayudará esto es lo que haremos hoy

importar librerías

cargar las imágenes en memoria

crear dinámicamente las etiquetas de resultado

dividir en sets de entrenamiento validación y test preprocesamiento de datos
crear el modelo de la cnn

ejecutar nuestra máquina de aprendizaje entrenar la red

revisar los resultados obtenidos

sampona

empecemos a programar

 importar librerías

cargaremos las libs que utilizaremos para el ejercicio

 ohttpdataspeakslucadcompythonparatodosjupyternotebookhtml
ohttpswwwanacondacomdownload

 httpsgithubcomjbagnatomachinelearningrawmastersportimageszip
httpsgithubcomjbagnatomachinelearningblobmasterejerciciocnnipynb

 httpwwwaprendemachinelearningcomcomofuncionanlas convolutionalneuralnetworksvisionporordenador

n

o

o







n

o

o














clasificación de imágenes en python 

import numpy as np

import os

import re

import matplotlibpyplot as plt

matplotlib inline

from sklearnmodelselection import traintestsplit
from sklearnmetrics import classificationreport
import keras

from kerasutils import tocategorical

from kerasmodels import sequential inputmodel

from keraslayers import dense dropout flatten

from keraslayers import convd maxpoolingd

from keraslayersnormalization import batchnormalization
from keraslayersadvancedactivations import leakyrelu

cargar las imágenes

recuerda tener descomprimidas las imágenes y ejecutar el código en el mismo directorio
donde descomprimiste el directorio llamado sportimages contiene subdirectorios uno por cada
deporte este proceso pltimreadfilepath cargará a memoria en un array las mil imágenes por
lo que puede tomar varios minutos y consumirá algo de memoria ram de tu ordenador

dirname ospathjoinosgetcewd sportimages

imgpath dirname ssep

images 
directories 
dircount 
prevroot
cant

printleyendo imagenes de imgpath

for root dirnames filenames in oswalkimgpath
for filename in filenames
if research x jpgl jpegpngbmptiff filename
cantcant
filepath ospath joinroot filename
image pltimread filepath
images append image
b leyendo strcant
print b endnr

análisis exploratorio de datos

cantidad de filas y columnas 

nombre
currencyname
languages 

dtypeobject

columnas indexalpha 

name
postal code format

alpha area
eqivalent fips code fips
 neighbours numeric

postal code regex

columnas nulos y tipo de datos

dfinfo

 

rangeindex

alpha 

area

capital

continent
currency code
currency name
eqivalent fips code
fips

geoname id
languages

name

neighbours

numeric

phone

population
postal code format
postal code regex
tld

ies to 

data columns total lumns 
alpha nonnull

 nonnull
 nonnull


 nonnull
 nonnull
 nonnull
 

 nonnull object
 nonnull object
aus

 nonnull g

cintós 

 nonnull object
 nonnull object

dtypes float int object

memory usage

 kb

capital continent
geoname id
phone population

tld

currencycode



en esta salida vemos las columnas el total de filas y la cantidad de filas sin nulos también los tipos

de datos

descripción estadística de los datos numéricos

dfdescribe

eo n 

n

o

l







clasificación de imágenes en python 

if prevroot root
printroot cant
prevrootroot
directories appendroot
dircount append cant
cant
dircount append cant

dircount dircount

dircount dircount 

print directorios leidoslendirectories
printimagenes en cada directorio dircount
printsuma total de imagenes en subdirssumdircount

leyendo imagenes de usersxxxproyecto pythonsportimages directorios leidos 
imagenes en cada directorio tx


suma total de imagenes en subdirs 

 crear etiquetas y clases

crearemos las etiquetas en labels es decir le daremos valores de al a cada deporte esto lo
hacemos para poder usar el algoritmo supervisado e indicar que cuando cargamos una imagen de
 etiqu ión

futbol en la red ya sabemos que corresponde con la etiqueta y con esa información entrada
salida esperada la red al entrenar ajustará los pesos de las neuronas luego convertimos las etiquetas
y las imágenes en numpy array con nparray

labels
indice
for cantidad in dircount
for i in rangecantidad
labelsappend indice
indiceindice
printcantidad etiquetas creadas lenlabels

deportes

indice

for directorio in directories
name directoriosplitossep
printindice namelenname

httpwwwaprendemachinelearningcomaplicaciones delmachinelearning

d



o

clasificación de imágenes en python 

deportes append name lenname 
indiceindice

nparraylabels

 


nparrayimages dtypenpuint convierto de lista a numpy

 find the unique numbers from the train labels
classes npuniquey
nclasses lenclasses

printtotal number of outputs nclasses

print output classes classes

cantidad etiquetas creadas 

 golf basket tenis natacion ciclismo beisball futbol americano f x
boxeo

total number of outputs 

output classes 

creamos sets de entrenamiento y test validación y
preprocesar

nótese la forma shape de los arrays veremos que son de x y por pues el se refiere a
los canales de colores que tiene cada imagen rgb red green blue que tiene valores de a 
preprocesamos el valor de los pixeles y lo normalizamos para que tengan un valor entre y por
eso dividimos en ademas haremos el onehot encoding con tocategorical que se refiere
a convertir las etiquetas nuestras clases por ejemplo de fútbol un a una salida de tipo 
 esto es porque así funcionan mejor las redes neuronales para clasificar y se corresponde
con una capa de salida de la red neuronal de neuronas nota por si no lo entendiste se pone
un en la sexta posición del array y el resto en ceros pero no te olvides que empieza a contar
incluyendo el cero por eso la etiqueta queda realmente en la séptima posición por último
en este bloque subdividimos los datos en para test y entrenamiento con traintestsplit y
nuevamente en el de training para obtener un subconjunto de validación 

httpsmachinelearningmasterycomwhyonehotencodedatainmachinelearning
httpsmediumcomsimpleaihowgoodisyourmodelintrotomachinelearningecbbdca

n

o

o















eo n 

o

clasificación de imágenes en python 

mezclar todo y crear los grupos de entrenamiento y testing

train xtest x train ytesty traintestsplitxytestsize
printtraining data shape trainxshape trainyshape
printtesting data shape testxshape testyshape

train x trainxastypefloat
testx testxastypefloat
trainx train x 

testx testx 

 change the labels from categorical to onehot encoding
train y onehot tocategorical trainy
testyonehot tocategoricaltesty

 display the change for category label using onehot encoding
print original label trainy
print after conversion to onehot trainyonehot 

train xvalidxtrainlabelvalidlabel traintestsplittrainx trainyonev
hot testsize randomstate

printtrainxshapevalidxshape trainlabel shapevalidlabel shape

training data shape 

testing data shape 

original label 

after conversion to onehot 
 

 creamos la red aquí la magia

ahora sí que nos apoyamos en keras para crear la convolutional neural network en un futuro
artículo explicaré mejor lo que se está haciendo por ahora confíen en mi

 declaramos constantes
 el valor inicial del learning rate initlr
 cantidad de epochs y
 tamaño batch de imágenes a procesar batchsize cargan en memoria
 crearemos una primer capa de neuronas convolucional de dimensiones convd donde
entrarán nuestras imágenes de xx

httpwwwaprendemachinelearningcomcomofuncionanlasconvolutionalneuralnetworks visionporordenador

clasificación de imágenes en python 

 aplicaremos filtros kernel de tamaño x no te preocupes si aún no entiendes esto que
detectan ciertas características de la imagen ejemplo lineas verticales

utilizaremos la función leakyrelu como activación de las neuronas

 haremos un maxpooling de x que reduce la imagen que entra de x a la mitadx
manteniendo las características únicas que detectó cada kernel

para evitar el overfitting añadimos una técnica llamada dropout

 aplanamos flatten los filtros y creamos una capa de neuronas tradicionales dense
 y finalizamos la capa de salida con neuronas con activación softmax para que se
corresponda con el hot encoding que hicimos antes

luego compilamos nuestra red sportmodelcompile y le asignamos un optimizador en este
caso de llama adagrad

 initlr e

 epochs 

 batchsize 



 sportmodel sequential

 sportmodel addconvd kernelsize activationlinear paddingsame
 inputshape

 sportmodel addleakyrelualpha

 sportmodel addmaxpoolingd paddingsame 

 sportmodel adddropout



 sportmodel addflatten 

 sportmodel adddense activationlinear

 sportmodel addleakyrelualpha

 sportmodel adddropout

 sportmodel adddensenclasses activationsoftmax



 sportmodel summary 



 sportmodel compile losskeras lossescategoricalcrossentropy optimizerkeras
 optimizersadagradrinitlr decayinitlr metricsaccuracy

 httpsenwikipediaorgwikirectifierneuralnetworks
 httpsenwikipediaorgwikiconvolutionalneuralnetworkdropout

clasificación de imágenes en python 

layer type output shape param 
convd convd nome 
leaky re lu leakyrelu none o

max poolingd maxpooling none 
dropout dropout none o
flatten flatten none o

dense dense none 
leaky re lu leakyrelu none o
dropout dropout none o

dense dense none 

total params 
trainable params 
nontrainable params 

entrenamos la cnn

llegó el momento con esta linea sportmodelfit iniciaremos el entrenamiento y validación de
nuestra máquina pensemos que introduciremos miles de imágenes pixeles arrays colores filtros y
la red se irá regulando sola aprendiendo los mejores pesos para las más de interconexiones
para distinguir los deportes esto tomará tiempo en un ordenador como mi macbook pro del 
unos minutos

nota podemos ejecutar este mismo código pero utilizando gpu en tu ordenador o en la nube
y los mismos cálculos tomaría apenas segundos

por último guardamos la red ya entrenada sportmodelsave en un formato de archivo hpy
que nos permitirá poder utilizarla en el futuro sin necesidad de volver a entrenar y ahorrarnos
los minutos de impaciencia



httpscolabresearchgooglecom

n

o

eo n 

o

o







clasificación de imágenes en python 

sporttraindropout sportmodelfittrainx trainlabel batchsizebatchsizx
eepochsepochs verbose validationdatavalidx validlabel

 guardamos la red para reutilizarla en el futuro sin tener que volver a entre

nar
sportmodel savesportsmnist hpy

train on samples validate on samples

epoch s usstep loss x
 acc valloss valacc 

epoch s usstep loss xn
 acc valloss valacc 

epoch s usstep loss xn
 acc valloss valacc 

epoch s usstep loss xn
 acc valloss valacc 

epoch s usstep loss x
 acc valloss valacc 

epoch s usstep loss xn
 acc valloss valacc 

vemos que tras iteraciones completas al set de entrenamiento logramos un valor de precisión
del y en el set de validación alcanza un será esto suficiente para distinguir las imágenes

deportivas

clasificación de imágenes en python

training and validation accuracy

 

 

 

 

 

 






e training accuracy
e validation accuracy

training and validation loss

e training loss
 validation loss

d

a

ne

a

clasificación de imágenes en python 

resultados de la clasificación

ya con nuestra red entrenada es la hora de la verdad ponerla a prueba con el set de imágenes para
test que separamos al principio y que son muestras que nunca fueron vistas por la máquina

testeval sportmodel evaluatetestx testyonehot verbose

printtest loss testeval 
printtest accuracy testeval

 s usstep
test loss 
test accuracy 

en el conjunto de testing vemos que alcanza una precisión del reconociendo las imágenes de
deportes ahora podríamos hacer un análisis más profundo para mejorar la red revisando los fallos
que tuvimos pero lo dejaremos para otra ocasión bonus en la jupyter notebook verás más
información con esto spoiler alert la clase que peor detecta son las de fórmula 

resumen

creamos una red neuronal distinta una red convolucional que aplica filtros a las imágenes y es
capaz de distinguir distintos deportes con un tamaño de entrada de x pixels a color en tan sólo 
minutos de entrenamiento esta vez fuimos a la inversa que en otras ocasiones y antes de conocer
la teoría de las redes específicas para reconocimiento de imágenes las cnn les he propuesto que
hagamos un ejercicio práctico aunque pueda parecer contraintuitivo muchas veces este método
de aprendizaje en humanos funciona mejor pues vuelve algo más dinámica la teoría espero que
les hayan quedado algunos de los conceptos y los terminaremos de asentar en un próximo capítulo

los recursos
 descarga el conjunto de imágenes para entrenar la red en archivo comprimido ocupa

mb
 descarga el código python completo en jupyter notebook

más enlaces con información sobre las convolutional neural networks

 httpsgithubcomjbagnatomachinelearningrawmastersportimageszip
 httpsgithubcomjbagnatomachinelearningblobmasterejerciciocnnipynb

clasificación de imágenes en python 

 de la universidad de stanford una referencia indudable csn cnn for visual recogni
tion

 introducing convolutional neural networks

intuitively understanding convolutional networks

 convolutional neural networks in python with keras







shttpsngithubioconvolutionalnetworks

 httpneuralnetworksanddeeplearningcomchapóhtmltintroducingconvolutionalnetworks
shttpstowardsdatasciencecomintuitivelyunderstandingconvolutionsfordeeplearning fffaee
s httpswwwdatacampcomcommunitytutorialsconvolutionalneuralnetworkspythoncnn

cómo funcionan las convolutional
neural networks

en este capítulo intentaré explicar la teoría relativa a las redes neuronales convolucionales en
inglés cnn que son el algoritmo utilizado en aprendizaje automático para dar la capacidad
de ver al ordenador gracias a esto desde apenas podemos clasificar imágenes detectar
diversos tipos de tumores automáticamente enseñar a conducir a los coches autónomos y un sinfín
de otras aplicaciones

el tema es bastante complejocomplicado e intentaré explicarlo lo más claro posible en este artículo
doy por sentado que tienes conocimientos básicos de cómo funciona una red neuronal artificial
multicapa feedforward fully connected

la cnn es un tipo de red neuronal artificial con aprendizaje supervisado que procesa sus capas
imitando al cortex visual del ojo humano para identificar distintas características en las entradas
que en definitiva hacen que pueda identificar objetos y ver para ello la cnn contiene varias
capas ocultas especializadas y con una jerarquía esto quiere decir que las primeras capas pueden
detectar lineas curvas y se van especializando hasta llegar a capas más profundas que reconocen
formas complejas como un rostro o la silueta de un animal

muchas imágenes

recodemos que la red neuronal deberá aprender por sí sola a reconocer una diversidad de objetos
dentro de imágenes y para ello necesitaremos una gran cantidad de imágenes lease más de 
imágenes de gatos otras de perros para que la red pueda captar sus características únicas
de cada objeto y a su vez poder generalizarlo esto es que pueda reconocer como gato tanto a un
felino negro uno blanco un gato de frente un gato de perfil gato saltando etc

s httpsenwikipediaorgwikiconvolutionalneuralnetwork

s httpwwwaprendemachinelearningcomqueesmachinelearning

a httpwwwaprendemachinelearningcombrevehistoria delasredesneuronalesartificiales
 httpwwwaprendemachinelearningcomaplicacionesdelmachinelearning

análisis exploratorio de datos

count
mean
std
min




max

pandas filtra las features numéricas y calcula datos estadísticos que pueden ser útiles cantidad

area
e
e
e
e
e
e
e

e

geonameid
e

e
e
e
e
e

e

media desvío estándar valores máximo y mínimo

verifiquemos si hay correlación entre los datos

corr dfsetindex alphacorr

numeric










smgraphicsplotcorrcorr xnameslistcorrcolumns

pltshow

population
e
e
e
e
e
e
e

e

cómo funcionan las convolutional neural networks 

una imagen

es una matriz de pixeles
el valor de los pixeles va de a pero se normaliza para la red
neuronal de a

pixeles y neuronas

para comenzar la red toma como entrada los pixeles de una imagen si tenemos una imagen con
apenas x pixeles de alto y ancho eso equivale a neuronas y eso es si sólo tenemos color
escala de grises si tuviéramos una imagen a color necesitaríamos canales red green blue
y entonces usaríamos xx neuronas de entrada esa es nuestra capa de entrada para
continuar con el ejemplo supondremos que utilizamos la imagen con sólo color

no olvides preprocesamiento

antes de alimentar la red recuerda que como entrada nos conviene normalizar los valores los
colores de los pixeles tienen valores que van de a haremos una transformación de cada pixel
valor y nos quedará siempre un valor entre y

cómo funcionan las convolutional neural networks


si la imagen es a color estará
compuesta de tres canales
rojo verde azul





convoluciones

 istintivo l n convolucio
ahora comienza el procesado distintivo de las cnn es decir haremos las llamadas convolucio
nes estas consisten en tomar grupos de pixeles cercanos de la imagen de entrada e ir operando
matemáticamente producto escalar contra una pequeña matriz que se llama kernel ese kernel
supongamos de tamaño x pixels recorre todas las neuronas de entrada de izquierdaderecha
de arribaabajo y genera una nueva matriz de salida que en definitiva será nuestra nueva capa de

neuronas ocultas
nota si la imagen fuera a color el kernel realmente sería de xx un filtro con kernels de x

luego esos filtros se suman y se le suma una unidad bias y conformarán salida cómo si fuera

 solo canal

cómo funcionan las convolutional neural networks 

kernel

imagen de
entrada

el kernel tomará inicialmente valores aleatorios y se irán ajustando mediante backpropagation
una mejora es hacer que siga una distribución normal siguiendo simetrías pero sus valores son
aleatorios

filtro conjunto de kernels

un detalle en realidad no aplicaremos sólo kernel si no que tendremos muchos kernel su
conjunto se llama filtros por ejemplo en esta primer convolución podríamos tener filtros con lo
cual realmente obtendremos matrices de salida este conjunto se conoce como feature mapping
cada una de xx dando un total del neuronas para nuestra primer capa oculta
de neuronas no les parecen muchas para una imagen cuadrada de apenas pixeles imaginen
cuántas más serían si tomáramos una imagen de entrada de xx que aún es considerado un
tamaño pequeño

cómo funcionan las convolutional neural networks 



aquí vemos al kernel realizando el producto matricial con la imagen de entrada y desplazando de a
 pixel de izquierda a derecha y de arribaabajo y va generando una nueva matriz que compone al
mapa de features

a medida que vamos desplazando el kernel y vamos obteniendo una nueva imagen filtrada por
el kernel en esta primer convolución y siguiendo con el ejemplo anterior es como si obtuviéramos
 imágenes filtradas nuevas estas imágenes nuevas lo que están dibujando son ciertas
características de la imagen original esto ayudará en el futuro a poder distinguir un objeto de otro
por ej gato ó perro

imagen kernel convolucion aplico relu

del kernel
ir

 josjos jo

 ol

 h
 ls loe 

finalmente
obtengo un mapa
de detección de
características



la imagen realiza una convolución con un kernel y aplica la función de activación en este caso relu

la función de activación

la función de activación más utilizada para este tipo de redes neuronales es la llamada relu por
rectifier linear unit y consiste en fxmaxx

 httpwwwaprendemachinelearningcomwpcontentuploadscnnkernelgif
httpwwwaprendemachinelearningcomwpcontentuploadscnnpng
 httpsenwikipediaorgwikirectifierneuralnetworks

cómo funcionan las convolutional neural networks 

subsampling

ahora viene un paso en el que reduciremos la cantidad de neuronas antes de hacer una nueva
convolución por qué como vimos a partir de nuestra imagen blanco y negro de xpx tenemos
una primer capa de entrada de neuronas y luego de la primer convolución obtenemos una capa
oculta de neuronas que realmente son nuestros mapas de características de x si
hiciéramos una nueva convolución a partir de esta capa el número de neuronas de la próxima capa
se iría por las nubes y ello implica mayor procesamiento para reducir el tamaño de la próxima
capa de neuronas haremos un proceso de subsampling en el que reduciremos el tamaño de nuestras
imágenes filtradas pero en donde deberán prevalecer las características más importantes que
detectó cada filtro hay diversos tipos de subsampling yo comentaré el más usado maxpooling

subsampling con maxpooling

e
oee 

subsampling
aplico maxpooling de x
y reduzco mi salida a la mitad



vamos a intentar explicarlo con un ejemplo supongamos que haremos maxpooling de tamaño x
esto quiere decir que recorreremos cada una de nuestras imágenes de características obtenidas
anteriormente de xpx de izquierdaderecha arribaabajo pero en vez de tomar de a pixel
tomaremos de x de alto por de ancho pixeles e iremos preservando el valor más alto
de entre esos pixeles por eso lo de max en este caso usando x la imagen resultante es

 httpwwwaprendemachinelearningcomwpcontentuploadscnnpng

cómo funcionan las convolutional neural networks 

reducida a la mitady quedará de x pixeles luego de este proceso de subsamplig nos quedarán
 imágenes de x pasando de haber tenido neuronas a son bastantes menos y en
teoría siguen almacenando la información más importante para detectar características deseadas

ya terminamos no ahora más convoluciones

primera convolución

u
j

o 
q

imagen de aplico kernels obtengo aplico obtengo 
entrada de feature maxpooling salidas
xx y función de mapping de x xx
activación relu xx



muy bien pues esa ha sido una primer convolución consiste de una entrada un conjunto de filtros
generamos un mapa de características y hacemos un subsampling con lo cual en el ejemplo de
imágenes de sólo color tendremos

entrada imagen xx
aplico kernel filtros de x
obtengo feature mapping xx
aplico maxpooling de x
obtengo salida de la convolución xx

la primer convolución es capaz de detectar características primitivas como lineas ó curvas a medida

s httpwwwaprendemachinelearningcomwpcontentuploadscnnpng

cómo funcionan las convolutional neural networks 

que hagamos más capas con las convoluciones los mapas de características serán capaces de
reconocer formas más complejas y el conjunto total de capas de convoluciones podrá ver pues
ahora deberemos hacer una segunda convolución que será

segunda convolución y sucesivas

t
t

ñ

d

entrada de aplico kernels obtengo aplico obtengo
convolución de xx feature maxpooling salidas
previa y función de mapping de x xx
xx activación relu xx
entrada imagen xx
aplico kernel filtros de x
obtengo feature mapping xx
aplico maxpooling de x
obtengo salida de la convolución xx

la er convolución comenzará en tamaño x pixels y luego del maxpooling quedará en x con
lo cual podríamos hacer sólo convolución más en este ejemplo empezamos con una imagen de
xpx e hicimos convoluciones si la imagen inicial hubiese sido mayor de xpx aún
hubiéramos podido seguir haciendo convoluciones

 httpwwwaprendemachinelearningcomwpcontentuploadscnnpng

cómo funcionan las convolutional neural networks 

entrada imagen xx

aplico kernel filtros de x
obtengo feature mapping xx
aplico maxpooling de x
obtengo salida de la convolución xx

llegamos a la última convolución y nos queda el desenlace

conectar con una red neuronal tradicional

para terminar tomaremos la última capa oculta a la que hicimos subsampling que se dice que
es tridimensional por tomar la forma en nuestro ejemplo xx altoanchomapas y la
aplanamos esto es que deja de ser tridimensional y pasa a ser una capa de neuronas tradicionales
de las que ya conocíamos por ejemplo podríamos aplanar y conectar a una nueva capa oculta de
 neuronas feedforward

arquitectura de una cnn

ísoftmax o gato

clololoioioiolo

imagen de er convolución convolución red neuronal capade salida

entrada filtros y sucesivas multicapa clasificación
e relu fully connected onehot
e subsampling encoded



entonces a esta nueva capa oculta tradicional le aplicamos una función llamada softmax que
conecta contra la capa de salida final que tendrá la cantidad de neuronas correspondientes con
las clases que estamos clasificando si clasificamos perros y gatos serán neuronas si es el dataset
mnist numérico serán neuronas de salida si clasificamos coches aviones ó barcos serán etc las
salidas al momento del entrenamiento tendrán el formato conocido como onehotencoding en
el que para perros y gatos sera y para coches aviones ó barcos será 

 httpwwwaprendemachinelearningcomwpcontentuploadscnnpng
 httpsenwikipediaorgwikisoftmaxfunction
 httpsenwikipediaorgwikionehot

cómo funcionan las convolutional neural networks 

y la función de softmax se encarga de pasar a probabilidad entre y a las neuronas de salida por
ejemplo una salida nos indica probabilidades de que sea perro y de que sea gato

y cómo aprendió la cnn a ver backpropagation

el proceso es similar al de las redes tradicionales en las que tenemos una entrada y una salida
esperada por eso aprendizaje supervisado y mediante el backpropagation mejoramos el valor
de los pesos de las interconexiones entre capas de neuronas y a medida que iteramos esos pesos se
ajustan hasta ser óptimos pero en el caso de la cnn deberemos ajustar el valor de los pesos
de los distintos kernels esto es una gran ventaja al momento del aprendizaje pues como vimos
cada kernel es de un tamaño reducido en nuestro ejemplo en la primer convolución es de tamaño
de x eso son sólo parámetros que debemos ajustar en filtros dan un total de parámetros
en comparación con los pesos entre dos capas de neuronas tradicionales una de y otra de
 en donde están todas interconectarlas con todas y eso equivaldría a tener que entrenar
y ajustar más de millones de pesos repito sólo para capa

comparativa entre una red neuronal tradicional y
una cnn

dejaré un cuadro resumen para intentar aclarar más las diferencias entre las redes fully connected
y las convolutional neural networks

característica

red tradicional
feedforward multicapa

red neuronal
convolucional cnn

datos de entrada en la

capa inicial

capas ocultas

capa de salida

las características que
analizamos por ejemplo
ancho alto grosor etc
elegimos una cantidad de
neuronas para las capas
ocultas

la cantidad de neuronas
que queremos clasificar
para comprar ó
dn t a cará

alquilar serán 
neuronas

pixeles de una imagen si
es color serán capas
para rojoverdeazul
tenemos de tipo 
convolución con un
tamaño de kernel y una
cantidad de filtros 
subsampling

debemos aplanar la
última convolución con
una ó más capas de
neuronas ocultas
tradicionales y hacer
una salida mediante
softmax a la capa de
salida que clasifica perro
y gato serán neuronas

a ohttpwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado

cómo funcionan las convolutional neural networks



característica red tradicional red neuronal
feedforward multicapa convolucional cnn

aprendizaje supervisado supervisado

interconexiones entre capas todas las son muchas menos

significado de la cantidad
de capas ocultas

backpropagation

arquitectura básica

neuronas de una capa con
la siguiente

realmente es algo
desconocido y no
representa algo en sí
mismo

se utiliza para ajustar los
pesos de todas las
interconexiones de las
capas

conexiones necesarias
pues realmente los pesos
que ajustamos serán los
de los filtroskernels que

usamos
las capas ocultas son

mapas de detección de
características de la
imagen y tienen jerarquía
primeras capas detectan
ineas luego curvas y
ormas cada vez más

elaboradas
se utiliza para ajustar los

pesos de los kernels

resumiendo podemos decir que los elementos que usamos para crear cnns son

 entrada serán los pixeles de la imagen serán alto ancho y profundidad será sólo color 

para redgreenblue

 capa de convolución procesará la salida de neuronas que están conectadas en regiones
locales de entrada es decir pixeles cercanos calculando el producto escalar entre sus pesos
valor de pixel y una pequeña región a la que están conectados en el volumen de entrada aquí
usaremos por ejemplo filtros o la cantidad que decidamos y ese será el volumen de salida

 capa relu aplicará la función de activación en los elementos de la matriz

 pool ó subsampling hará una reducción en las dimensiones alto y ancho pero se

mantiene la profundidad

 capa tradicional red de neuronas feedforward que conectará con la última capa de
subsampling y finalizará con la cantidad de neuronas que queremos clasificar

no incluido

quedan muchas cosas más que explicar sobre las cnn temas y definiciones como padding stride
evitar overfitting imageaugmentation dropout o por nombrar algunas redes famosas resnet
alexnet googlenet y densenet al mismísimo yann lecun todo eso se queda fuera de este
texto este artículo pretende ser un punto inicial para seguir investigando y aprendiendo sobre las
cnn al final dejo enlace a varios artículos para ampliar información sobre cnn

i ek n 

análisis exploratorio de datos

geoname id

numenc

population

correlation matrix



en este caso vemos baja correlación entre las variables dependiendo del algoritmo que utilicemos

podría ser una buena decisión eliminar features que tuvieran alta correlación

cargamos un segundo archivo csv para ahondar en el crecimiento de la población en los últimos
años filtramos a españa y visualizamos

url httpsrawgithubusercontentcomdruestaplespopulationgrowthmastercountr y

iescsv

dfpop pdreadcsvur
printdfpophead
dfpopes dfpopdfpopcountry spain 

printdfpopeshead 

dfpopes drop countryaxispopulation plotkindbar

eb wnho

country
afghanistan
afghanistan
afghanistan
afghanistan
afghanistan

year






population

cómo funcionan las convolutional neural networks 

resumen

hemos visto cómo este algoritmo utiliza variantes de una red neuronal tradicional y las combina
con el comportamiento biológico del ojo humano para lograr aprender a ver

understanding convolutional neural networks

csn convolutional neural networks for visual recognition
introducing convolutional networks

intuitively understanding convolutions for deep learning
feature visualizationhow neural networks build up their understanding of images
back propagation in convolutional neural networks
httpsmediumcomtechnologymadeeasy





thebestexplanationofconvolutionalneuralnetworksontheinternetfbbsbaddf

 httpstowardsmlcomdeeplearningseries punderstanding convolutionalneuralnetworks
httpcsngithubio
httpneuralnetworksanddeeplearningcomchaphtmlintroducingconvolutionalnetworks
httpstowardsdatasciencecomintuitively understandingconvolutionsfordeeplearning fffaee
shttpsdistillpubfeaturevisualization

 httpsbecominghumanaibackpropagationinconvolutionalneuralnetworksintuitionandcodeefc

detección de objetos con python

veremos de manera práctica cómo crear tu propio detector de objetos que podrás utilizar con
imagenes estáticas video o cámara avanzaremos paso a paso en una jupyter notebook con el código
completo usando keras sobre tensorflow

agenda

tenemos mucho por delante

 en qué consiste la detección yolo
 algunos parámetros de la red
 el proyecto propuesto

 lo que tienes que instalar y todo el material
 crear un dataset imágenes y anotaciones

 recomendaciones para la imágenes
 anotarlo todo
 el lego dataset
 el código python
 leer el dataset
 train y validación
 data augmentation
 crear la red yolo
 crear la red de detección
 generar las anclas
 entrenar
 revisar los resultados
 probar la red
 conclusiones
 material adicional

detección de objetos con python 

en qué consiste la detección yolo

vamos a hacer un detector de objetos en imágenes utilizando yolo un tipo de técnica muy novedosa
 acrónimo de you only look once y que es la más rápida del momento permitiendo su uso
en video en tiempo real

esta técnica utiliza un tipo de red neuronal convolucional llamada darknet para la clasificacion de
imágenes y le añade la parte de la detección es decir un cuadradito con las posiciones x e y alto
y ancho del objeto encontrado

la dificultad de esta tarea es enorme poder localizar las áreas de las imágenes que para una red
neuronal es tan sólo una matriz de pixeles de colores posicionar múltiples objetos y clasificarlos
yolo lo hace todo de una sola pasada a su red convolucional en resultados sobre el coco
dataset detecta clases de objetos distintos y etiquetar y posicionar objetos en imagen

nota este código se basa en varios trozos de código de diversos repositorios de github y usa una
arquitectura de yolov aunque ya sé que es mejor la versión y de hecho está por salir yolo v
pero con fines didácticos la v nos alzanza

aquí comentaré varios parámetros que manejaremos con esta red y que debemos configurar

algunos parámetros de la red

 tamaño de imagen que procesa la red este será fijo pues encaja con el resto de la red y es de
 pixeles todas las imágenes que le pasemos serán redimensionadas antes de entrar en la
red

 cantidad de cajas por imagen estás serán la cantidad de objetos máximos que queremos
detectar

 etiquetas estas serán las de los objetos que queramos detectar en este ejemplo sólo detectare
mos tipo de objeto pero podrían ser múltiples

 epochs la cantidad de iteraciones sobre todo el dataset que realizará la red neuronal para
entrenar recuerda que a muchas épocas tardará más tiempo y también aumenta el riesgo de
overfitting

 traintimes este valor se refiera a la cantidad de veces de entrenar una misma imagen esto
sirve sobre todo en datasets pequeños pues haremos data augmentation sobre las imágenes
cada vez

 savedweightsname una vez entrenada la red guardaremos sus pesos en este archivo y lo
usaremos para hacer las predicciones

el proyecto propuesto detectar personajes de lego

será porque soy padre ó será porque soy ingeniero al momento de pensar en un objeto para
detectar se me ocurrió esto legos quien no tiene legos en su casa por supuesto que puedes
crear tu propio dataset de imagenes y anotaciones xml para detectar el ó los objetos que tu quieras

eo n 

o

l

detección de objetos con python 

lo que tienes que instalar

primero que nada te recomiendo que crees un nuevo environment de python e instales estas
versiones de librerías que usaremos

en consola escribe

 python m venv detectaenv

y luego lo activas para usarlo en windows con
 detectaenvvscriptsvactivatebat

ó en linux mac con

 source detectaenvbinactivate

y luego instala los paquetes

pip install tensorflow
pip install keras

pip install imgaug

pip install opencvpython

pip install hopy

pip install tqgdm

pip install imutils

aclaraciones usamos una versión antigua de tensorflow si tienes gpu en tu máquina puedes usar
la versión apropiada de tensorflow

si vas crear tu propio dataset como se explica a continuación deberás instalar labellmg que
requiere

pip install pyqt
pip install lxml
pip install labelimg

si no puedes usar el dataset de legos que provee el blog y saltarte la parte de crear el dataset

además otros archivos que deberás descargar

e archivo con esta roto por lo que lo he reemplazado con uno nuevo desde donde podes encon
trar el archivo la nueva urles esta fullyolobackendh

detección de objetos con python 

gracias por avisarme un saludo mb

 código python detección de imágenes
 opcional dataset de lego
 opcional crea y usa tu propio dataset de imágenes y anotaciones

 jupyter notebook

crea un dataset imágenes y anotaciones

es hora de crear un repositorio de miles de imágenes para alimentar tu red de detección

en principio te recomendaría que tengas al menos unas imágenes de cada clase que quieras
detectar y de cada imagen deberás tener un archivo xml con un formato que en breve comentaré
con la clase y la posición de cada objeto pero recuerda que al detectar imágenes podemos tener más
de un objeto entonces puedes tener imágenes que tienen a más de una clase

recomendaciones para las imágenes

algunas recomendaciones para la captura de imágenes si vas a utilizar la camara de tu móvil puede
que convenga que las hagas con pocos megapixeles pues si haces una imagen de de mb luego
la red la reducirá a pixeles de ancho por lo que tendrá un coste ese preprocesado en tiempo
memoria y cpu

intenta tener fotos dellos objetos con distintas condiciones de luz es decir no tengas imagenes de
gatitos siempre al sol mejor imágenes de interior al aire libre con poca luz etc

intenta tener imágenes torcidas rotadas parciales y de distintos tamaños del objeto si sólo tienes
imágenes en donde tu objeto supongamos que mide pixeles malacostumbrarás la red y sólo
detectará en imágenes cuando sea de esas dimensiones

variaciones del mismo objeto si tu objeto es un gato intenta clasificar gatos de distintos colores
razas y en distintas posiciones para que la red pueda generaliza el conocimiento

anotarlo todo

muy bien ya tienes tus imágenes hechas y guardadas en un directorio
ahora deberás crear un archivo xml donde anotarás cada objeto sus posiciones xy su alto y ancho

el xml será de este tipo

 httpsgithubcomjbagnatomachinelearningblobmasterejercicioobjectdetectionipynb
s httpsdrivegooglecomfilednxuskcocykwmozvnqnfcxfviewuspsharing

o ia ek an 

r
oo ioodurñrouonn e y

detección de objetos con python 

 
 lego 
 croped jpg 
 detectaobjetosimageslegocroped jpg 
 
 
 
 
 
 
 lego 
 
 
 
 
 
 
 
 

y lo puedes hacer a mano ó puedes usar un editor como labelimg

 labelimg usersjbagnatopycharmprojectsdetectaobjetosimageslegocropedipg
box labels
 e vabel
open 
image sifficult
z use default lbel
open dir
 lego
change save dir m
 o
next image
prev image
g o file list
 etectaobjetosmageslegodcropedtpg 
verity image a etectaobjetosimageslegocropedipg
 etectaobjetosimageslegocropedipg
e letectaobjetosimageslegocropedipg
save letectaobjetosimageslegocropedipg
 etectaobjetosimageslegocropedipg
 etectaobjetosimageslegocropedipg
ra etectaobjetosimageslegocropedipg
u etectaobjetosimageslegocropedipg
v letectaobjetosimageslegocropedipg
click drag to move shape lego x y 

si lo instalaste con pip puedes ejecutarlo simplemente poniendo en línea de comandos labelimg se
abrirá el editor y podrás

 seleccionar un directorio como fuente de imágenes

detección de objetos con python 

 otro directorio donde guardará los xml

en el editor deberás crear una caja sobre cada objeto que quieras detectar en la imagen y escribir su
nombre cuando terminas le das a guardar y siguiente

el lego dataset

si quieres puedes utilizar un dataset de imágenes que creé para este ejercicio y consta de 
imágenes son fotos tomadas con móvil de diversos personajes lego realmente son fotos y 
variaciones en zoom y recortes y sus correspondientes archivos de anotaciones xml

dicho esto recuerda que siempre es mejor más y más imágenes para entrenar

cropedipg croped ipg croped ipg croped ipg croped ipg cropedipg



cropedipg croped ipg croped ipg croped ipg croped ipg croped ipg

croped ipg croped ipg croped ipg croped ipg croped ipg croped ipg

el código python

usaremos keras sobre tensorflow para crear la red manos a la obra

ae o n 

o

























detección de objetos con python 

en el artículo copiaré los trozos de código más importante pero recuerda descargar la notebook
jupyter con el código completo desde github

leer el dataset

primer paso será el de leer las anotaciones xml que tenemos creadas en un directorio e ir iterando
los objetos para contabilizar las etiquetas

xmldir annotationlego
imgdir imageslego
labels lego

tamanio 

mejorespesos redlegoh

def leerannotationsanndir imgdir labels
allimgs 
seenlabels 

for ann in sortedoslistdiranndir
img object

tree etparseanndir ann

for elem in treeiter
if filename in elemtag
imgfilename imgdir elemtext
if width in elemtag
imgwidth intelemtext
if height in elemtag
imgheight intelemtext

if object in elemtag or part in elemtag
obj 

for attr in listelem

if name in attrtag

objname attrtext

if objname in seenlabels
seenlabelsobjname 
else

httpsgithubcomjbagnatomachinelearningblobmasterejercicioobjectdetectionipynb

n

o

detección de objetos con python 

seenlabelsobjname 

if lenlabels o and objname not in labels
break

else
imgobject obj

if bndbox in attrtag
for dim in listattr

if xmin in dimtag
objxmin introuna floatdimtext
if ymin in dimtag

objymin introundfloatdimtext

if xmax in dimtag
objxmax introuna floatdimtext

if ymax in dimtag
objymax intround floatdimtext

if lenimgobject 
allimgs img

return allimgs seenlabels

trainimgs trainlabels leerannotationsxmdir imgdir labels
printimagenes lentrainimgs labelslentrainlabels

train y validación

separaremos un de las imágenes y anotaciones para testear el modelo en este caso se utilizará
el set de validación al final de cada época para evaluar métricas pero nunca se usará para entrenar

trainvalidsplit intlentrainimgs

np randomshuffletrainimgs

validimgs trainimgstrainvalid split

trainimgs trainimgstrainvalidsplit
printtrainlentrainimgs validatelenvalidimgs

data augmentation

el data augmentation sirve para agregar pequeñas alteraciones ó cambios a las imágenes de entradas
aumentando nuestro dataset de imágenes y mejorando la capacidad de la red para detectar objetos

n

o ad

 s

o ad

detección de objetos con python 

para hacerlo nos apoyamos sobre una librería imgaug que nos brinda muchas funcionalidades como
agregar desenfoque agregar brillo ó ruido aleatoriamente a las imágenes además podemos usar
opencv para voltear la imagen horizontalmente y luego recolocar la bounding box

 fragmento del código

iaaoneof 
iaagaussianblur blur images
iaaaverageblurk blur image using local means with kernel
iaamedianblurk blur image using local medians with kernel
d
iaasharpenalpha lightness sharpen images
iaaadditivegaussiannoise loc scale perchannel add 
gaussian noise to images
iaaoneof 
iaadropout perchannel randomly remove up to of th
e pixels
d
iaaadd perchannel change brightness of images
iaamultiply perchannel change brightness of images
iaacontrastnormalization perchannel improve or worsen the
contrast

crear la red de clasificación

la red es conocida como darknet y está compuesta por capas convolucionales que básicamente
aplican batchnormalizarion maxpooling y activación por leakyrelu para la extracción de caracte
rísticas es decir los patrones que encontrará en las imágenes en sus pixeles para poder diferenciar
entre los objetos que queremos clasificar

va alternando entre aumentar y disminuir la cantidad de filtros y kernel de x y x de la red
convolucional

 fragmento de código solo algunas capas de e jemplo

 layer 

x convd strides paddingsame nameconv usebiasfalsex
inputimage

x batchnormalizationnamenormx

leakyrelualphax

maxpoolingdpoolsize x

x

x

a mis padres angel y graciela que siempre me dieron las herramientas para poder formarme y me
enseñaron que lo importante es el camino

análisis exploratorio de datos 

country year population
 spain 
 spain 
 spain 
 spain 
 spain 

le

















crecimiento de la población de españa el eje x no está establecido y aparece un id de fila

hagamos la comparativa con otro país por ejemplo con el crecimiento poblacional en argentina

n

o

o









detección de objetos con python 

 layer 

x convd strides paddingsame nameconv usebiasfalsex
x

x batchnormalizationnamenormx

x leakyrelualphax

maxpoolingdpoolsize x

le


 layer 

le


convd strides paddingsame nameconv usebiasfalsex


x

batchnormalizationnamenormx
leakyrelualphax

no olvides descargar y copiar en el mismo directorio donde ejecutes la notebook los pesos de la red
darknet pues en este paso se cargaran para incializar la red

crear la red de detección

esta red utilizará la anterior clasificación y utilizará las features obtenidas en sus capas convolu
cionales de salida para hacer la detección de los objetos es decir las posiciones x e y alto y ancho
para ello se valdrá de unas anclas en nuestro caso serán las anclas son unas ventanas o unas
bounding boxes de distintos tamaños pequeños mediano grande rectangulares o cuadrados que
servirán para hacer propuestas de detección

h fragmento de código

inputimage inputshapeselfinputsize selfinputsize 

selftrueboxes inputshape maxboxperimage 

selffeatureextractor fullyolofeatureselfinputsize

printself featureextractor getoutputshape 
selfgrid h selfgridw self featureextractor getoutputshape
features selffeatureextractor extractinputimage

 make the object detection layer

output convdselfnbbox selfnbclass
 strides
paddingsame 

 holaelenlaceoriginaleradelusuariocreadorestaroto porloquelohereemplazadoconunonuevo desdedondepodesencontrarelarchivo
lanuevaurlesesta 
fullyolobackendh graciasporavisarme unsaludo

detección de objetos con python 

namedetectionlayer 
kernelinitializerlecunnormal features
output reshape selfgridh selfgridw selfnbbox selfnbclayn
ss output
output lambda lambda args args output selftrueboxes

selfmodel model inputimage selftrueboxes output

en total la red crea una grilla de x y en cada una realizará predicciones lo que da un total
de posibles detecciones para cada clase que queremos detectar si tenemos clases esto serían
 predicciones cada una con la clase y sus posiciones xy ancho y alto lo más impresionante de
esta red yolo es que lo hace todo de sólo pasada increíble

detección de objetos con python 

para refinar el modelo y que detecte los objetos que hay utilizará dos funciones con las cuales
descartará áreas vacías y se quedará sólo con las mejores propuestas las funciones son

 ou intersection over union que nos da un porcentaje de acierto del área de predicción contra
la cajita real que queremos predecir

 non maximum suppression nos permite quedarnos de entre nuestras anclas con la que
mejor se ajusta al resultado esto es porque podemos tener muchas áreas diferentes propuestas
que se superponen de entre todas nos quedamos con la mejor y eliminamos al resto

detección de objetos con python 

entonces pensemos que si en nuestra red de detección de sólo clase detectamos objeto esto
quiere decir que la red descarto a las restantes

nota por más que haya separado en redes red yolo y red de detección realmente es sóla red
convolucional pues están conectadas y al momento de entrenar los pesos se ajustan como siempre
con el backpropagation

generar las anclas

como antes mencioné la red utiliza anclas para cada una de las celdas de x para realizar las
propuestas de predicción pero qué tamaño tienen que tener esas anclas podríamos pensar en 
tamaños distintos algunos pequeños otros más grandes y que se adapten a las clases que queremos
detectar por ejemplo el ancla para detectar siluetas de personas serán seguramente rectangulares
en vertical

detección de objetos con python 

pues según lo que quieras detectar conviene ajustar esos tamaños ejecutaremos un pequeño script
que utiliza kmeans y determina los mejores clusters de dimensiones que se adapten a tu dataset

entrenar la red

a entrenar la red neuronal como dato informativo en mi ordenador macbook de núcleos y gb
de ram tardó horas en entrenar las imágenes del dataset de lego con épocas y veces cada
imagen

ne

a

d



l








detección de objetos con python 

yolo yoloinputsize tamanio
labels labels
maxboxperimage 
anchors anchors

inalizar verás qu u ivo nuevi redlegoh que i
al finalizar verás que se ha creado un archivo nuevo llamado redlegoh e contiene los pesos
de la red creada

revisar los resultados

los resultados vienen dados por una métrica llamada map y que viene a ser un equivalente a un
fscore pero para imágenes teniendo en cuenta los falsos positivos y negativos ten en cuenta que
si bien la ventaja de yolo es la detección en tiempo real su contra es que es un poco peor en
accuracy que otras redes que son lentas lo podemos notar al ver que las cajitas no se ajustan del
todo con el objeto detectado ó puede llegar a confundir a veces la clase que clasifica con el lego
dataset he logrado al rededor de de map no está mal recordemos que este valor de map se
obtiene al final de la última epoch sobre el dataset de validación que no se usa para entrenar y en
mi caso eran imágenes

probar la red

para finalizar podemos probar la red con imágenes nuevas distintas que no ha visto nunca veamos
cómo se comporta la red

crearemos unas funciones de ayuda para dibujar el rectángulo sobre la imagen original y guardar
la imagen nueva

def draw boxesimage boxes labels
imageh imagew imageshape

for box in boxes
xmin intboxxminimagew
ymin intboxyminimageh

xmax intboxxmaximagew

ymax intboxymaximageh
cvrectangle image xminymin xmaxymax 
cvputtext image
labelsboxgetlabel strboxgetscore 
xmin ymin 
cvfonthersheysimplex

n

o

o










detección de objetos con python 

e imageh
 

return image

recuerda que utilizaremos el archivo de pesos creado al entrenar para recrear la red esto nos permite
poder hacer predicciones sin necesidad de reentrenar cada vez

mejorespesos redlegoh
imagepath imagestestlegogirlpng

miyolo yoloinputsize tamanio
labels labels
maxboxperimage 
anchors anchors

miyololoadweightsmejorespesos

image cvimread imagepath
boxes miyolopredict image
image drawboxes image boxes labels

printdetectados lenboxes

cvimwrite imagepath detected imagepath image

como salida tendremos una nueva imagen llamada legogirl detectedpng con la detección
realizada

detección de objetos con python 





esta imagen me fue prestada por oshundeezofficial
cuenta de instagram que es genial

 muchas gracias les recomiendo ver su

 httpswwwinstagramcomshundeezofficial

detección de objetos con python 



lego

ne



l













detección de objetos con python 

imágenes pero también video y cámara
puedes modificar un poco la manera de realizar predicciones para utilizar un video mp ó tu cámara
web

para aplicarlo a un video
from tqdm import 

videopath lego moviemp
videoout video path detected videopath
videoreader cvvideocapturevideopath

nbframes intvideoreader getcvcappropframecount
frameh intvideoreadergetcvcappropframeheight
framew intvideoreadergetcvcappropframewidth

videowriter cvvideowritervideoout
cvvideowriterfourccmpeg

framew frameh

for i in tqdmrangenbframes
 image videoreader read 

boxes yolopredict image
image drawboxesimage boxes labels

videowriter writenpuintimage

videoreader release
videowriter release

luego de procesar el video nos dejará una versión nueva del archivo mp con la detección que
realizó cuadro a cuadro

y para usar tu cámara presiona q para salir

oo idíawn 

análisis exploratorio de datos 

dfpopar dfpopdfpopcountry argentina

anios dfpopesyearunique
popar dfpoparpopulationvalues

popes dfpopespopulationvalues

dfplot pddataframe argentina popar spain popes
indexanios
dfplot plotkindbar

le

m argentina
m spain

s lrerssesaesa d y s
 aaaaadarr

gráfica comparativa de crecimiento poblacional entre españa y argentina entre los años al


ahora filtremos todos los paises hispanohablantes

dfespanol dfreplacenpnan regextrue

dfespanol dfespanol dfespanol languagesstrcontainses 
dfespanol

o i oañfídao n 











detección de objetos con python

lego detection

x y r g b

winname lego detection
cvnamedwindowwinname

videoreader cvvideocapture

while true
 image videoreader read

boxes yolopredictimage

image drawboxesimage boxes labels
cvimshowwinname image

key cvwaitkey oxff

if key ordq

break

cvdestroyallwindows 
videoreader release

detección de objetos con python 

resumen

esta fue la parte práctica de una de las tareas más interesantes dentro de la visión artificial que es la
de lograr hacer detección de objetos piensen todo el abanico de posibilidades que ofrece poder hacer
esto podríamos con una cámara contabilizar la cantidad de coches y saber si hay una congestión de
tráfico podemos contabilizar cuantas personas entran en un comercio si alguien toma un producto
de una estantería y mil cosas más ni hablar en robótica donde podemos hacer que el robot vea y
pueda coger objetos ó incluso los coches de tesla con autopilot tiene un gran potencial

además en este capítulo te ofrece la posibilidad de entrenar tus propios detectores para los casos de
negocio que a ti te importan

recursos

recuerda todo lo que tienes que descargar

 código python completo en la jupyter notebook

 los esta roto por lo que lo he reemplazado con uno nuevo desde donde podes encontrar el
archivo la nueva url es esta fullyolobackendh 

gracias por avisarme un saludo yolov

 set de imágenes y anotaciones lego

y enlaces a otros artículos de interés

 object detection

 a brief history of cnn in image segmentation

 beginers guide to to implementing yolov in tensorflow
 practical guide to object detection with yolo

 a very shallow overview of yolo and darknet

 yolo v object detection

httpsgithubcomjbagnatomachinelearningblobmasterejercicioobjectdetectionipynb

 httpsdrivegooglecomfilednxuskcocykwmozvnqnfcxfviewuspsharing

 httpswwwsaagiecomblogobjectdetectionpart
httpsblogathelascomabriefhistoryofcnnsinimagesegmentationfromrcnntomaskrcnneade
 httpsmachinelearningspacecomyolovtensorflowpart

 httpswwwanalyticsvidhyacomblogpracticalguideobjectdetectionyoloframeworpython

 httpsmartinapugliesegithubiorecogniseobjectsyolo

 httpswwwgeeksforgeeksorgyolovobjectdetection

anexo webscraping

ejemplo web scraping en python ibex la bolsa de
madrid

en este artículo aprenderemos a utilizar la librería beatifulsoap de python para obtener contenidos
de páginas webs de manera automática

en internet encontramos de todo artículos noticias estadísticas e información útil e inútil pero
cómo la extraemos no siempre se encuentra en forma de descarga ó puede haber información
repartida en multiples dominios ó puede que necesitemos información histórica de webs que
cambian con el tiempo

para poder generar nuestros propios archivos con los datos que nos interesan y de manera automática
es que utilizaremos la técnica de webscraping

contenidos

 requerimientos para webscraping

 lo básico de html y css que debes saber

 inspeccionar manualmente una página web

 al código obtener el valor actual del ibex de la bolsa de madrid
 exportar a archivo csv y poder abrir en excel

 otros casos frecuentes de rascar la web

puedes ver y descargar el código python completo de este artículo desde github haciendo

click aquí

requerimientos

para poder usar esta técnica hay diversas librerías pero utilizaremos una muy popular llamada
beautiful soap como siempre te recomiendo tener instalado el ambiente de desarrollo con
anaconda que ya trae incluida la librería si no lo puedes instalar desde línea de comandos con

ohttpsgithubcomjbagnatomachinelearningblobmasterejemplowebscrapingbolsayfutbolipynb
 httpwwwaprendemachinelearningcominstalarambientededesarrollopythonanacondaparaaprendizajeautomatico

eo n 

o

o




n

o

o

oo



anexo i webscraping 

pip install beautifulsoup
pip install requests

si bien utilizaremos una jupyter notebook para el código python podríamos ejecutar un archivo
de texto plano py desde nuestra terminal

conocimientos básicos de html y css

daré por sentados conocimientos de html y css por qué las páginas webs están hechas con
html y deberemos indicarle a nuestro botspider de qué etiquetas ó campos deseamos extraer
el contenido

repaso mínimo de html es

 
 titulo de pagina 
 
 
 soy un parrafo 
 soy un texto en un div 
 soy una celda dentro de una tabla 
 
 
 

aqui vemos las etiquetas básicas de html es decir las de inicio y cierre y dentro de body el
contenido de la página como ejemplo vemos un párrafo p un div y una tabla

y porqué css en realidad no necesitamos estrictamente saber css pero sí sus selectores puesto
que nos pueden ser de mucha ayuda lo básico para comprender selectores usando este bloque de
ejemplo es

 
 
 
 
 
bienvenido a mi web

anexo i webscraping 
para poder seleccionar el texto bienvenido a mi wep tenemos diversas formas

 la más directa será si la etiqueta tiene un atributo id que es único en el ejemplo 

 podríamos buscar los nodos de tipo div pero podría haber muchos y deberemos filtrarlos

 podemos filtrar un div con el atributo name bloquebienvenida

 podemos buscar por clase css en el ejemplo verde

 muchas veces se combinan selectores por ejemplo dentro de la clase contenedor la clase
verde o decir traer un div con la clase verde

la librería de beautiful soap nos permite buscar dentro de los nodos del árbol de la página web
también conocido como dom al final del artículo veremos como obtener el texto bienvenido a
mi web con diversos selectores y en la jupyter notebook de github

inspección manual de la web

vy

bolsa v madrid pacromeets

resumen de índices

esta es la web de la bolsa de madrid en donde nos interesa obtener el indice del ibex

para el ejemplo inspeccionaremos la web de la bolsa de madrid qué es eso de inspeccionar
bueno los navegadores web modernos safari firefox chrome cuentan con una opción que nos
permite ver el código html completo de la página que estamos viendo

además existe una opción de inspección del código que nos permite ver el html javascript css
y la web al mismo tiempo con ello buscaremos la manera de extraer el texto que nos interesa si
buscamos por id por algún atributo clase ó nodos

por lo general podemos inspeccionar haciendo click con el botón derecho del mouse sobre el área
que nos interesa veamos cómo hacerlo con un gif animado 

httpsgithubcomjbagnatomachinelearningblobmasterejemplowebscrapingbolsayfutbolipynb
 httpwwwbolsamadridesespaspxindicesresumenaspx
sohttpwwwbolsamadridesespaspxindicesresumenaspx

n

anexo i webscraping 

e
na
wlllle
bolsa r madi

j la cortiiención weia c inatiuto

 m

resumen de índices

cotlaciones

 mo persu
e ree pe unzzis a
ea eac unabandza asunecis ca

 ea velasa 
 eza svól lao
 za uaza
ndees leiica e trergle 
 tondrzde 
 aa ae e
 m an ms e ii
estadibticas y m te e
 pc a e ee
 na e m e
 me m m

wdeor bex

 mbmos anto arviietoa


adiidir
antatir

d 
ae 
m 
 o antadcir

índces ectues

 at jatajtir

e
an
eo

ce
ce
ntaitir

anza insersores

 pe er
para cupresat e ma re aquda nl e 
para memeros y a
profesonales y ee j e auda url nca 
pama medico de eza apaar c 
coyuncación ee ia e apdor coco ve 
y ez aularod m a

al hacer clic derecho aparece la opción de inspeccionar elemento así podemos ver las entrañas
de la web en la que estamos navegando y pensar la mejor opción para extraer contenidos

en nuestro caso nos interesa obtener el valor de la fila con nombre ibex y el valor de la columna



último

código webscraping python

veamos en código cómo haremos para acceder a esa porción de texto
primero importamos las librerías python que utilizaremos

import requests

from bs import beautifulsoup

import csv

from datetime import datetime

indicamos la ruta de la web que deseamos acceder

 indicar la ruta
urlpage httpwwwbolsamadridesespaspxindicesresumenaspx

y ahora haremos el request a esa ruta y procesaremos el html mediante un objeto de tipo
beautifulsoap

shttpsiwpcomwwwaprendemachinelearningcomwpcontentuploadsinspeccionarwebsafarigif

o o iaaddpra n 



anexo i webscraping 

 tarda milisegundos

page requestsgeturlpagetext

soup beautifulsouppage lxm

bien ahora toca pensar la estrategia para acceder al valor en nuestro caso nos interesa primero
acceder a la tabla y de allí a sus celdas por suerte la tabla tiene un id único

 obtenemos la tabla por un id específico
tabla soup find table attrsid ct contenido tblííndices
tabla

resumen de índices

aqui vemos el id de la tabla marcado en amarillo
en rojo se muestra la tercera celda de la primer fila a la que queremos acceder

bien ahora dentro de la tabla y siendo que en este caso no tenemos un acceso directo a las celdas por
ids únicos ni por clases sólo nos queda iterar entonces accederemos a la primer fila y obtendremos
de las celdas el nombre del índice y su valor

nota realmente es la segunda fila pues hay un encabezado por eso usamos el índice 
y no el cero

name
price
nrofila
for fila in tablafind alltr
if nrofila
nrocelda
for celda in filafind alltd
if nrocelda
nameceldatext
printindice name

o shttpsiwpcomwwwaprendemachinelearningcomwpcontentuploadsinspeccionabolsamadpng

s

anexo i webscraping 

if nrocelda
priceceldatext
printvalor price
nroceldanrocelda
nrofilanrofila

veremos cómo salida

indice ibex 
valor 

ya sólo nos queda guardar los datos para usar en el futuro

guardar csv y ver en excel

vamos a suponer que ejecutaremos este script una vez al día entonces lo que haremos es ir
escribiendo una nueva línea al final del archivo cada vez

 abrimos el csv con append para que pueda agregar contenidos al final del archivo
with open bolsaibexcsv a as csvfile

writer csvwritercsvfile

writer writerow name price datetimenow

finalmente obtenemos el archivo llamado bolsaibexcsv listo para ser usado en nuestro
proyecto 

 libreoffice file edit view insert format styles sheet date


ebusrxos 
liberation sans bu lm
al f bex

 help us make libreoffice even better

a b c d
t 








podemos abrir el archivo csv en excel libreoffice spreadsheets ó como archivo de texto plano

d



l

d



l

anexo i webscraping 

otros ejemplos útiles de webscaping

veamos otros ejemplos de uso de beatifulsoap para extraer contenidos con python

usemos el bloque de ejemplo que usé antes e intentemos extraer el texto bienvenido a mi web de
diversas maneras

obtener por id

eltexto soupfind div attrsid gettext

printeltexto

obtener por clase css

eltexto soupfinddiv attrsclass verdegettext
printeltexto

obtener dentro de otra etiqueta anidado

eltexto nextsoupdivchildrengettext con next obtiene primer hijo
printeltexto

obtener los enlaces de una página web

otro caso práctico que nos suele ocurrir es querer colectar los enlaces de una página web para ello
obtenemos las etiquetas a e iteramos obteniendo el atributo href que es donde se encuentran las
nuevas rutas con posibilidad de hacer un nuevo request a cada una y extraer sus contenidos

urlpage httpswwwlifedercomcientificosfamosos
page requestsgeturlpagetext
soup beautifulsouppage lxm
contenido soupfinddiv attrsclass tdpostcontent
items contenidofindalla
for item in items
printitemhref

en el archivo jupyter notebook de mi cuenta de github se ven estos ejemplos y alguno más

resumen

ahora sabemos cómo afrontar el proceso de obtener información de cualquier página web resu
miendo el procedimiento básico que seguimos es

 cargar la página en el navegador

httpsgithubcomjbagnatomachinelearningblobmasterejemplowebscrapingbolsayfutbolipynb

anexo i webscraping 

 inspeccionar e investigar el html

 en python importar las librerías

 obtener la página parsear el contenido con beautifulsoap
 obtner el trozo de contenido que buscamos

 mediante id

 mediante etiqueta

 mediante clases css

 otros atributos

 guardamos los datos en csv

repasando cuando ya tenemos el contenido en un objeto soap solemos utilizar los métodos find
ó para múltiples etiquetas el findall

si combinamos un script para webscraping como en el ejemplo para capturar valores de la bolsa con
el cron del sistema ó con algún tipo de repetidor de tareas del sistema que nos permita ejecutar
nuestro código cada x tiempo podremos generar un valioso archivo de información muy a medida
de lo que necesitamos

otro ejemplo clásico es el de la obtención automática de los resultados de partidos de fútbol y en
el código de este ejemplo en github encontrarás cómo hacerlo

obtener el jupyter notebook con código python con este y más ejemplos de websca
píng

httpsgithubcomjbagnatomachinelearningblobmasterejemplowebscrapingbolsayfutbolipynb
o httpsgithubcomjbagnatomachinelearningblobmasterejemplowebscrapingbolsayfutbolipynb

análisis exploratorio de datos 
alpha alpha area capital continent currency code currency name eqivalent fips code fips geonameid languages name
buenos es
 ar arg sa ars peso ar ar entdemon agentina
 aw abw oranjestad awg guilder aa nawesen aruba
 bol sucre sa boliviano bl esboquay bolvia
 ba bra brasila sa bal real br ptbresenfr brazil sa
 b biz belmopan dollar bh enbzes belize
 cl chl santiago sa clp peso cl escl chile
 co col bogota sa cop peso co sesermo esco colombia
 car cai sanjose crc colon cs escren costa rica
 cu cub havana cup peso cu escu cuba
santo dominican
 do dom oooo dop peso dr es e
 ec ecu quito sa usd dollar esec ecuador
 es 
 es esp madrid eu eur euro sp es a euco spain
visualizamos

dfespanol setindex alpha population area plotkindbar rot figsiz y

e
e
m populaton
m ra





s
 llllll ll ll i
 ds eslfíeeeleae es x
ph



vamos a hacer detección de outliers en este caso definimos como límite superior e inferior la
media más menos veces la desviación estándar que muchas veces es tomada como máximos
de tolerancia

httpswwwaprendemachinelearningcomdeteccionde outliersenpythonanomalia

anexo ii machine learning en la
nube

machine learning en la nube google colaboratory
con gpu

por increíble que parezca tenemos disponible una cuenta gratuita para programar nuestros modelos
de machine learning en la nube con python jupyter notebooks de manera remota y hasta con gpu
para poder aumentar nuestro poder de procesamiento gratis sí sí esto no es un cuento del tío
ni tiene trampa

machine learning desde el navegador

primero lo primero porqué voy a querer tener mi código en la nube lo normal ideal es que
tengamos un entorno de desarrollo local en nuestro propio ordenador un entorno de pruebas en
algún servidor staging y producción pero qué pasa si aún no tenemos instalado el ambiente o
tenemos conflictos con algún archivolibrería versión de python ó por lo que sea no tenemos
espacio en disco ó hasta si nos va muy lento y no disponemos en el corto plazo de mayor
procesadorram o hasta por comodidad está siempre bien tener a mano una web online siempre
lista en donde ya esté todo el software que necesitamos instalado y ese servicio lo da google
entre otros proveedores lo interesante es que google colab ofrece varias ventajas frente a sus
competidores

la gpu en casa o en la nube

una gpu para que quiero eso si ya tengo como núcleos la realidad es que para el
procesamiento de algoritmos de aprendizaje automático y para videojuegos ejem la gpu
resulta mucho más potente en realizar cálculos y en paralelo sobre todo las multiplicaciones
matriciales esas que hacemos toodo el tiempo al entrenar nuestros modelos
para hacer el descenso por gradiente ó toooodo el rato con el backpropagation de nuestras redes
neuronales esto supone una aceleración de hasta x en velocidad de procesado algoritmos
que antes tomaban días y ahora se resuelven en horas un avance enorme

 ohttpwwwaprendemachinelearningcominstalarambiente dedesarrollopythonanacondaparaaprendizajeautomatico
httpscolabresearchgooglecom
httpwwwaprendemachinelearningcompasosmachinelearningconstruirmaquina

 httpwwwaprendemachinelearningcomaprendizajeprofundounaguiarapida

anexo ii machine learning en la nube 

si tienes una tarjeta nvidia con gpu ya instalada felicidades ya tienes el poder si no la tienes y no
tienes previsto invertir unos cuántos dólares en comprarla puedes tener toda su potencia desde
la nube

nota google se reserva el poder limitar el uso de gpu si considera que estás abusando ó
utilizando en demasía ese recurso ó para fines indebidos como la minería de bitcoins

recientes

primera vez que abierto por
se abrió última vez

co hola colaboratory ct hace minutos

 telco chum prediction ipynb hacedías hace días

 orendemachineleming ipynb hacedías hace días

co snippets importing lbraries hace días hace días

co extenal data drive sheets and cloud storage hace días hace días

nuevo cuaderno depython cancelar

qué es google colab

google colab es un servicio en la nube que nos provee de una jupyter notebook a la que podemos
acceder con un navegador web sin importar si en casa usamos windows linux o mac tiene como
grandes ventajas

posibilidad de activar una gpu

 podemos compartir el código fácilmente

está basado en jupyter notebook y nos resultará un entorno familiar

 podemos crear libros en python ó en 

tiene preinstaladas las librerías comunes usadas en datascience deep learning y la posibilidad
de instalar otras que necesitemos

al enlazar con nuestra cuenta de google drive podemos leer desde ahí archivos csv de entrada
ó guardar imágenes de salida etc

anexo ii machine learning en la nube 

como se usa googlecolab

primero que nada entramos y logueamos con nuestra cuenta de google en colaboratory ahora
ya podemos

 crear una nueva notebook
 vamos a archivo crear nuevo cuaderno en python 
 y habilitar gpu

 vamos a entorno de ejecución cambiar tipo de entorno de ejecución y elegimos
acelerador por hardware gpu

configuración del cuaderno

ipo de entom

python 

acelerador por hardware

gpu

 omitir resultado de las celdas de código al guardar este cuaderno

cancelar guardar

seleccionamos el uso de gpu en google colaboratory

enlazar con google drive

una ventaja de enlazar nuestra cuenta con drive es que nos facilita poder subir o descargar archivos
ubir u iv ivos izaui ón subir
para subir un archivo seleccionamos archivos del panel izquierdo y damos al botón subir como

se muestra en la imagen

 httpscolabresearchgooglecom

anexo ii machine learning en la nube 

 copia de ejerciciocnnipynb
archivo edtar ver insertar entormo de ejecución herramientas ayuda

e códico q teo ceda ceda

índice c

jalizar convolutional neural networks

 m sompleaqta realizaremos un ejercicio descrito en el blog www aprendemachinale

 importar librerías

import mumpy as mp
import s

import re

import matplotlibpyplot as pit

amatplotib inline

fron sklearnmodelselection import traín testsp
fron sklearnmetrics import classification report

j usersjbagnatoanacondalenvspythonb
 jbasi y py
this module will be removed ín depn

import
fron kerasutils import to categorícal
ron kerasmodele ímport sequential inputmodel

pero si quieres poder usar cualquier archivo por ej csv que tengas en tu unidad de drive deberas
ejecutar en una celda

from googlecolab import drive
drivemount contentdrive

te pedirá que hagas click a un enlace y escribas un código que te dará cuando autorices la app
cuando vuelvas y hagas actualizar en el tab de archivos veras tu unidad montada y lista para usar 

ejecutar una jupyter notebook de github

vamos a abrir una jupyter notebook que contiene el ejercicio explicado en el artículo de convolu
tional neural networks clasificar imágenes deportivas para ello en el cuadro de abrir
cuaderno

 shttpwwwaprendemachinelearningcomclasificacion deimagenesenpython

anexo ii machine learning en la nube 

cmhus

introduce una url de ghub busca por organización o usuario ncluirrepositonios prvados

httpsgithubcomjbagnatomachinelearning 

repostoro e bifurcación 

 bagnatomachinelearning master 

 ao

c eercco x mensiono ac

 o

 gerccio regresion linestigyro 
nuevo cuaderno de python cancelar

 seleccionamos github

 copiamos la dirección del repositorio en nuestro caso httpsgithubcomjbagnatomachine
learning

 y le damos a la lupa de buscar

 nos aparecerá el listado con los archivos del repo

 y de allí seleccionamos el cuaderno llamado ejerciciocnnipynb

veremos que tenemos el mismo notebook pero en google colab 

descargar un recurso al cuaderno

casi todo listo pero aún nos queda algo antes de poder ejecutar en este ejercicio
necesitamos tener las imágenes en sus directorios respectivos

para ello primero descargaremos el zip creamos una celda nueva y ejecutamos
e iwget httpsgithubcomjbagnatomachinelearningrawmastersportimageszip

y veremos que aparece nuestro archivo zip en el listado dale a actualizar si hace falta

 copia de ejercicio cnnipynb

 httpsgithubcomjbagnatomachinelearning

anexo ii machine learning en la nube 

descomprimir un archivo en el cuaderno

y ahora deberemos descomprimirlo creamos una celda y ejecutamos

 lunzip uq sportimageszip d 

e copia de ejercicio cnnipynb

acchvo eduar ver ineiar entomo de ejcución herramienas ayuda
 xto ceda y ceda
indce fragns

código archivos x tunzip uq s jeszip d 
 e ee

a
 m macosx realizaremos un ejercicio descrito en el blog www aprendemachinelear
 a sampledata
 sporimages

 basket

 belsball

 import numpy as p
 boxeo 
 cclismo import re
 import matplotibpyplot as plt



 a futbol

 agl

 natacion

 importar librerías

 selection import traintestaplit
las import elassificationreport

 to categorical
 sequential input model
dense dropout flstten

 convd maxpoolingd 

n soorimages zip

normalisation imort batehormalís

recuerda habilitar el entorno de ejecución con gpu como vimos antes ahora ya podemos
ejecutar todas las celdas y veremos qué rápido ejecuta la cnn con gpu en comparación con cpu
pasa de tardar minutos a sólo segundos

instalar otras librerías python con pip

deberemos ejecutar por ejemplo pip install gensim 

resumen

hemos visto que tenemos la opción de tener nuestro ambiente de desarrollo local pero también

esta alternativa de poder programar experimentar y trabajar en la nube gracias a este servicio
podemos tener listo el ambiente en pocos minutos y aprovechar las ventajas que nos ofrece sobre
todo el uso de gpu que es un recurso del que no todos disponemos

otros recursos

otros artículos que explican el uso de google colab en inglés

 httpwwwaprendemachinelearningcomwpcontentuploadsdescomprimirimagenespng
 httpwwwaprendemachinelearningcominstalarambientededesarrollopythonanacondaparaaprendizajeautomatico

anexo ii machine learning en la nube

 google colab free gpu tutorial
 google colab for beginners
 picking a gpu for deep learning



 httpsmediumcomdeeplearningturkeygooglecolabfreegpututorialebfd
httpsmediumcomleaninwomenintechindiagooglecolabthebeginners guideadbdfa
 httpsblogslavvcompickingagpufordeeplearningdcb

anexo iii principal component
analysis

en este capítulo veremos una herramienta muy importante para nuestro kit de machine learning

y data science pca para reducción de dimensiones como bonustrack veremos un ejemplo
rápidosencillo en python usando scikitlearn

introducción a pca

imaginemos que queremos predecir los precios de alquiler de vivienda del mercado al recopilar
información de diversas fuentes tendremos en cuenta variables como tipo de vivienda tamaño
de vivienda antigiedad servicios habitaciones consin jardín consin piscina consin muebles
pero también podemos tener en cuenta la distancia al centro si hay colegio en las cercanías o
supermercados si es un entorno ruidoso si tiene autopistas en las cercanías la seguridad del
barrio si se aceptan mascotas tiene wifi tiene garaje trastero y seguir y seguir sumando variables
es posible que cuanta más y mejor información obtengamos una predicción más acertada pero
también empezaremos a notar que la ejecución de nuestro algoritmo seleccionado regresión lineal
redes neuronales etc empezará a tomar más y más tiempo y recursos es posible que algunas de
las variables sean menos importantes y no aporten demasiado valor a la predicción también
podríamos acercarnos peligrosamente a causar overfitting al modelo

no sería mejor tomar menos variables pero más valiosas

al quitar variables estaríamos haciendo reducción de dimensiones al hacer reducción de
dimensiones las características tendremos menos relaciones entre variables a considerar para
reducir las dimensiones podemos hacer dos cosas

 eliminar por completo dimensiones
 extracción de características

eliminar por completo algunas dimensiones no estaría mal pero deberemos tener certeza en
que estamos quitando dimensiones poco importantes por ejemplo para nuestro ejemplo podemos
suponer que el precio de alquiler no cambiará mucho si el dueño acepta mascotas en la vivienda

httpwwwaprendemachinelearningcomqueesmachinelearning

 httpwwwaprendemachinelearningcompasosmachinelearningconstruirmaquina
httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning
httpwwwaprendemachinelearningcomqueesoverfittingyunderfittingycomosolucionarlo

anexo iii principal component analysis 

podría ser un acierto o podríamos estar perdiendo información importante en la extracción
de características si tenemos características crearemos otras características nuevas inde
pendientes en donde cada una de esas nuevas características es una combinación de las
 características viejas al crear estas nuevas variables independientes lo haremos de una
manera específica y las pondremos en un orden de mejor a peor sean para predecir a la variable
dependiente y la reducción de dimensiónes te preguntarás bueno intentaremos mantener todas
las variables posibles pero prescindiremos de las menos importantes como tenemos las variables
ordenadas de mejor a peores predictoras ya sabemos cuales serán las más y menos valiosas
a diferencia de la eliminación directa de una característica vieja nuestras nuevas variables
son combinaciones de todas las variables originales aunque eliminemos algunas estaremos
manteniendo la información útil de todas las variables iniciales

qué es principal component analysis

entonces principal component analysis es una técnica de extracción de características donde
combinamos las entradas de una manera específica y podemos eliminar algunas de las variables
menos importantes manteniendo la parte más importante todas las variables como valor añadido
luego de aplicar pca conseguiremos que todas las nuevas variables sean independientes una de otra

cómo funciona pca

en resumen lo que hace el algoritmo es

 estandarizar los datos de entrada ó normalización de las variables

 obtener los autovectores y autovalores de la matriz de covarianza

 ordenar los autovalores de mayor a menor y elegir los k autovectores que se correspondan
con los autovectores k más grandes donde k es el número de dimensiones del nuevo
subespacio de características

 construir la matriz de proyección w con los kk autovectores seleccionados

 transformamos el dataset original x estandarizado vía w para obtener las nuevas caracterís
ticas kdimensionales

tranquilos que todo esto ya lo hace solito scikitlearn u otros paquetes python ahora que
tenemos las nuevas dimensiones deberemos seleccionar con cuales nos quedamos

selección de los componentes principales

típicamente utilizamos pca para reducir dimensiones del espacio de características original
aunque pca tiene más aplicaciones hemos rankeado las nuevas dimensiones de mejor a

shttpseswikipediaorgwikivectorpropioyvalorpropio

anexo iii principal component analysis 

peor reteniendo información pero cuantas elegir para obtener buenas predicciones sin perder
información valiosa podemos seguir métodos

método elegimos arbitrariamente las primeras n dimensiones las más importantes por
ejemplo si lo que queremos es poder graficar en dimensiones podríamos tomar las características
nuevas y usarlas como los ejes x e y

método calcular la proporción de variación explicada de cada característica e ir tomando
dimensiones hasta alcanzar un mínimo que nos propongamos por ejemplo hasta alcanzar a explicar
el de la variabilidad total

método crear una gráfica especial llamada scree plot a partir del método y seleccionar
cuántas dimensiones usaremos por el método del codo en donde identificamos visualmente el
punto en donde se produce una caída significativa en la variación explicada relativa a la característica
anterior

pero porqué funciona pca

suponiendo nuestras características de entrada estandarizadas como la matriz z y zt su transpuesta
cuando creamos la matriz de covarianza ztz es una matriz que contiene estimados de cómo cada
variable de z se relaciona con cada otra variable de z comprender como una variable es asociada con
otra esimportante los autovectores representan dirección los autovalores representan magnitud
a mayores autovalores se correlacionan direcciones más importantes por último asumimos que a
más variabilidad en una dirección particular se correlaciona con explicar mejor el comportamiento
de una variable dependiente mucha variabilidad usualmente indica información mientras que
poca variabilidad indica ruido

ejemplo mínimo en python

utilizaré un archivo csv de entrada de un ejercicio anterior en el cual decidíamos si convenía

alquilar o comprar casa dadas dimensiones en este ejemplo

 normalizamos los datos de entrada
 aplicamos pca
 y veremos que con de las nuevas dimensiones y descartando obtendremos

 hasta un de variación explicada y
 buenas predicciones
 realizaremos gráficas

 httpsenwikipediaorgwikiexplainedvariation

httpsmediumcom ebioturinghowtoreadpcabiplotsandscreeplots aae

 httpwwwaprendemachinelearningcomwpcontentuploadscompraralquilarcsv
ohttpwwwaprendemachinelearningcomcomprarcasaoalquilarnaivebayesusandopython
 httpwwwaprendemachinelearningcomcomprarcasaoalquilarnaivebayesusandopython

o ia ek an 

r
id ge wnhhaooyy







 s

análisis exploratorio de datos 

anomalies 

 funcion ejemplo para detección de outliers

def findanomaliesdata
 set upper and lower limit to standard deviation
datastd datasta
datamean datamean
anomalycutoff datastd 
lowerlimit data mean anomalycutoff
upperlimit datamean anomalycutoff
printlowerlimitiloc
printupperlimitiloc

 generate outliers
for index row in dataiterrows
outlier row obtener primer columna
if outlieriloc upperlimitiloc or outlieriloc lowerlimity
iloc
anomalies append index
return anomalies

findanomaliesdfespanol setindex alphapopulation

detectamos como outliers a brasil y a usa los eliminamos y graficamos ordenado por población
de menor a mayor

 quitemos bra y usa por ser outliers y volvamos a graficar
dfespanol drop inplacetrue

dfespanol setindex alphapopulationarea sortvalues populationpx
lotkindbar rot figsize

o i añfmsyos e



























anexo iii principal component analysis 

 una con el acumulado de variabilidad explicada y

 una gráfica d en donde el eje x e y serán los primero componentes principales
obtenidos por pca
y veremos cómo los resultados comprar ó alquilar tienen icon nameangledoubleleft class
unprefixedclassbastante buenaicon nameangledoubleright class unprefixedclass
separación en dimensiones

importamos librerías

import pandas as pd

import numpy as np

import matplotlibpyplot as plt

zmatplotlib inline

pltrcparamsfigure figsize 
pltstyleuse ggplot

from sklearndecomposition import pca

from sklearnpreprocessing import standardscaler

cargamos los datos de entrada
dataframe pdreadcsvrcompraralquilarcsv
printdataframetail

normalizamos los datos

scalerstandardscaler

df dataframedrop comprar axis quito la variable dependiente y
scalerfitdf calculo la media para poder hacer la transformacion
xscaledscaler transformdf ahora si escalo los datos y los normalizo

instanciamos objeto pca y aplicamos

pcapcancomponents otra opción es instanciar pca sólo con dimensiones nuevas 
hasta obtener un mínimo explicado ej pcapca 

pcafitxscaled obtener los componentes principales

xpcapcatransformxscaled convertimos nuestros datos con las nuevas dimensione 
s de pca

printshape of x pca x pcashape

expl pcaexplained varianceratio

printexp

printsuma sumexp 

vemos que con componentes tenemos algo mas del de varianza explicada

graficamos el acumulado de varianza explicada en las nuevas dimensiones

pltplotnpcumsumpcaexplainedvarianceratio

anexo iii principal component analysis 

pltxlabel number of components
pltylabel cumulative explained variance
pltshow

graficamos en dimensiones tomando los primeros componentes principales
xaxxpca
yaxxpca
labelsdataframe comprar values
cedict red green
labl alquilarcomprar
marker
alpha 
figaxplt subplots figsize
figpatchsetfacecolor white
for in npuniquelabels
ixnpwhere labels
axscatter xaxixyaxix ccdict labelab s markermarker alphaxn
alpha

pltxlabelfirst principal component fontsize
pltylabelsecond principal component fontsize
pltlegena

pltshow

anexo iii principal component analysis 

 

 

 

 

 

 

cumulative explained variance

 

 
number of components

en esta gráfica de variabilidad explicada acumulada vemos que tomando los primeros componen
tes llegamos al

anexo iii principal component analysis 

 alquilar
é s comprar
 
 e 
v 
ea 
o 
 lo 
é a u 
o l x oo o ta ón

 a m a
 
 e td
c 
 
a júa a r a
 
 la
e e a
 
o o s p 
v e y
 
un c
 
 
 o 

first principal component

aquí vemos que al reducir las dimensiones iniciales a tan sólo logramos darnos una idea de dónde
visualizar nuestras predicciones para comprar o alquilar casa

resumen

con pca obtenemos
 una medida de como cada variable se asocia con las otras matriz de covarianza
 la dirección en las que nuestros datos están dispersos autovectores

 la relativa importancia de esas distintas direcciones autovalores

pca combina nuestros predictores y nos permite deshacernos de los autovectores de menor
importancia relativa

contras de pca y variantes

no todo es perfecto en la vida ni en pca como contras debemos decir que el algoritmo de
pca es muy influenciado por los outliers en los datos por esta razón surgieron variantes de

httpsmediumcomosocialcopshowtofindanddealwithoutliersinadatasetaeaaffa

anexo iii principal component analysis 

pca para minimizar esta debilidad entre otros se encuentran randomizedpca sparcepca y
kernelpca por último decir que pca fue creado en y ha surgido una buena alternativa en
 llamada tsne con un enfoque distinto

resultados de pca en el mundo real

para concluir les comentaré un ejemplo muy interesante que vi para demostrar la eficacia de

aplicar pca si conocen el ejercicio clásico mnist algunos le llaman el hello word del machine
learning donde tenemos un conjunto de imágenes con números a mano del al y
debemos reconocerlos utilizando alguno de los algoritmos de clasificación pues en el caso de mnist
nuestras características de entrada son las imágenes de x pixeles lo que nos da un total de 
dimensiones de entrada ejecutar regresión logística en con una macbook tarda unos segundos
en entrenar el set de datos y lograr una precisión del 

aplicando pca al mnist con una varianza retenida del logramos reducir las
dimensiones de a ejecutar regresión logística ahora toma segundos y la
precisión obtenida sigue siendo del 

más recursos

 el ejercicio python en mi cuenta de github
 archivo csv de entrada para el ejercicio

mas información en los siguientes enlaces en inglés

 principal component analysis in python
 dive into pca

 pca with python

 pca using python scikitlearn
 in depth pca

 interpreting pca

 video dimensionality reduction andrew ng



 httpswwwquoracomwhatisrandomizedpca
httpstowardsdatasciencecomkernelpcavspcavsicaintensorflowsklearneeba

 httpswwwwkdnuggetscomintroductiontsnepythonhtml

 httpstowardsdatasciencecompcausingpythonscikitlearnefe
httpsenwikipediaorgwikimnistdatabase
httpsdeeplearninglaptopcomblogmnist

 httpwwwaprendemachinelearningcomregresionlogisticaconpythonpasoapaso
ohttpsgithubcomjbagnatomachinelearningblobmasterejerciciopcaipynb
httpsrawgithubusercontentcomjbagnatomachinelearningmastercompraralquilarcsv
httpsplotlyipythonnotebooksprincipalcomponentanalysis

 httpstowardsdatasciencecomdiveintopcaprincipalcomponentanalysiswithpythondedead
httpsmediumcomdistrictdatalabsprincipalcomponentanalysiswithpythoncd
shttpstowardsdatasciencecompcausingpythonscikitlearnefe
shttpsjakevdpgithubiopythondatasciencehandbookprincipalcomponentanalysishtml
httpbenalexkeencomprinciplecomponentanalysisinpython

 httpswwwwyoutubecomwatchvrngvxut

análisis exploratorio de datos 

e

m population
m area





 j

c j
 j


 j



y u
j



ay j


f

 
 e
ca

y
e
h
y
z
c
esp
oy
mey

 a

z

e
n
dy

así queda nuestra gráfica sin outliers 

en pocos minutos hemos podido responder cuántos datos tenemos si hay nulos los tipos de datos
entero float string su correlación hicimos visualizaciones comparativas manipulación de datos
detección de outliers y volver a graficar no está nada mal no

más cosas que se suelen hacer

otras pruebas y gráficas que se suelen hacer son

si hay datos categóricos agruparlos contabilizarlos y ver su relación con las clases de salida
gráficas de distribución en el tiempo por ejemplo si tuviéramos ventas para tener una primera
impresión sobre su estacionalidad

rankings del tipo productos más vendidos ó ítems con más referencias por usuario
 calcular importancia de features y descartar las menos útiles

resumen

vimos un repaso sobre qué es y cómo lograr hacer un análisis exploratorio de datos en pocos
minutos su importancia es sobre todo la de darnos un vistazo sobre la calidad de datos que tenemos
y hasta puede determinar la continuidad o no de un proyecto

análisis exploratorio de datos 

siempre dependerá de los datos que tengamos en cantidad y calidad y por supuesto nunca
deberemos dejar de tener en vista el objetivo el propósito que buscamos lograr siempre
debemos apuntar a lograr eso con nuestras acciones

como resultado del eda si determinamos continuar pasaremos a una etapa en la que ya preproce
saremos los datos pensando en la entrada a un modelo ó modelos de machine learning

recursos
puedes descargar la notebook relacionada con este artículo desde aquí

 descargar notebook ejemplo eda para machine learning github

bonus track notebook sobre manipulación de datos con pandas

como adicional te dejo una notebook con los casos más comunes de uso de manipulación de datos
con pandas

 descargar notebook educativa sobre uso de pandas

más recursos

estos son otros artículos relacionados que pueden ser de tu interés

 eda house prices data python
 ml project in python

 eda example in python

 eda tutorial

httpsgithubcomjbagnatomachinelearningblobmasterejercicioedaipynb

httpsgithubcomjbagnatomachinelearningblobmastermanipulaciondatospandasipynb

shttpswwwhackerearthcompracticemachinelearningmachinelearningprojectspythonprojecttutorial

 httpsmachinelearningmasterycommachinelearninginpythonstepbystep

httpswwwactivestatecomblogexploratorydataanalysisusingpythonutmcampaignexploratory dataanalysisblogutm
mediumreferraléutm sourcekdnuggetsérutmcontentkdnuggetsarticle

httpswwwdatacampcomcommunitytutorialsexploratory dataanalysis python

regresión lineal con python

qué es la regresión lineal

la regresión lineal es un algoritmo de aprendizaje supervisado que se utiliza en machine
learning y en estadística en su versión más sencilla lo que haremos es dibujar una recta que nos
indicará la tendencia de un conjunto de datos continuos si fueran discretos utilizaríamos regresión
logística en estadísticas regresión lineal es una aproximación para modelar la relación entre una
variable escalar dependiente y y una o mas variables explicativas nombradas con x

recordemos rápidamente la fórmula de la recta
y mxb

donde y es el resultado x es la variable m la pendiente o coeficiente de la recta y b la constante
o también conocida como el punto de corte con el eje y en la gráfica cuando x

the development in pizza prices in denmark from to 

aqui vemos un ejemplo donde vemos datos recabados sobre los precios de las pizzas en dinamarca
los puntos en rojo y la linea negra es la tendencia esa es la línea de regresión que buscamos que
el algoritmo aprenda y calcule sólo

 httpseswikipediaorgwikiregresicbnlineal

 httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning
httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado
httpwwwaprendemachinelearningcomregresionlogisticaconpythonpasoapaso

regresión lineal con python 

cómo funciona el algoritmo de regresión lineal en
machine learning

recordemos que los algoritmos de machine learning supervisados aprenden por sí mismos y en
este caso a obtener automáticamente esa recta que buscamos con la tendencia de predicción para
hacerlo se mide el error con respecto a los puntos de entrada y el valor y de salida real el algoritmo
deberá minimizar el coste de una función de error cuadrático y esos coeficientes corresponderán
con la recta óptima hay diversos métodos para conseguir minimizar el coste lo más común es
utilizar una versión vectorial y la llamada ecuación normal que nos dará un resultado directo

nota cuando hablo de recta es en el caso particular de regresión lineal simple si hubiera más
variables hay que generalizar el término

un ejercicio práctico

en este ejemplo cargaremos un archivo csv de entrada obtenido por webscraping que contiene
diversas urls a artículos sobre machine learning de algunos sitios muy importantes como
techcrunch o kdnuggets y como características de entrada las columnas tendremos

title titulo del artículo

 url ruta al artículo

 word count la cantidad de palabras del artículo

 of links los enlaces externos que contiene

e of comments cantidad de comentarios

 images video suma de imágenes o videos

 elapsed days la cantidad de días transcurridos al momento de crear el archivo

 shares nuestra columna de salida que será la cantidad de veces que se compartió el artículo

a partir de las características de un artículo de machine learning intentaremos predecir cuantas
veces será compartido en redes sociales haremos una primer predicción de regresión lineal simple 
con una sola variable predictora para poder graficar en dimensiones ejes x e y y luego un ejemplo
de regresión lineal múltiple en la que utilizaremos dimensiones xyz y predicciones

nota el archivo csv contiene mitad de datos reales y otra mitad los generados de manera aleatoria
por lo que las predicciones que obtendremos no serán reales

httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado
httpseswikipediaorgwikierrorcuadrcaticomedio
httpsenwikipediaorgwikilinearleastsquaresmathematicsderivationofthenormalequations
 httpwwwaprendemachinelearningcomarticulosml
httpwwwaprendemachinelearningcomejemplowebscrapingpythonibexbolsavalores

 httpstechcrunchcomtagmachinelearning

httpswwwwkdnuggetscom

httpsenwikipediaorgwikisimplelinearregression

s ss

o 





d

a

regresión lineal con python 

requerimientos para hacer el ejercicio

para realizar este ejercicio crearemos una jupyter notebook con código python y la librería scikit
learn muy utilizada en data science recomendamos utilizar la suite de anaconda
podrás descargar los archivos de entrada csv o visualizar la notebook online

predecir cuántas veces será compartido un artículo de
machine learning

regresión lineal simple en python con variable

aqui vamos con nuestra notebook comencemos por importar las librerías que utilizaremos

 imports necesarios

import numpy as np

import pandas as pd

import seaborn as sb

import matplotlibpyplot as plt
zmatplotlib inline

from mpltoolkitsmplotd import axesd
from matplotlib import cm
pltrcparamsfigure figsize 
pltstyleuse ggplot

from sklearn import linear model

from sklearnmetrics import meansquarederror rscore

leemos el archivo csv y lo cargamos como un dataset de pandas y vemos su tamaño

cargamos los datos de entrada

data pdreadcsvarticulosmlcsv

veamos cuantas dimensiones y registros contiene
data shape

nos devuelve veamos esas primeras filas

httpdataspeakslucadcompythonparatodosjupyternotebookhtml
httpswwwanacondacomdownload
httpwwwaprendemachinelearningcomarticulosml
httpsgithubcomjbagnatomachinelearningblobmasterejercicioregresionlinealipynb

regresión lineal con python 

son registros con columnas veamos los primeros registros
datahead 

ti word hof hof images elapsed e
le u count links comments video days shares

 whatis machine leaming and how do we use i httpsblogsignals networkwhatis 
machine

 companies using machine leamning in s nan nan 

 howartíficial inteligence is revolutionizing nan 

 dbrainandthe blockchain of artíficial intell nan nan 

 nasafinds entire solar system filled with eig nan 

se ven algunos campos con valores nan nulos por ejemplo algunas urls o en comentarios veamos
algunas estadísticas básicas de nuestros datos de entrada

 ahora veamos algunas estadísticas de nuestros datos
datadescribe

wordcount lfof links of comments images video elapsed days shares

count 

mean 
std 
min 
 
 
 
max 



aqui vemos que la media de palabras en los artículos es de el artículo más corto tiene 
palabras y el más extenso intentaremos ver con nuestra relación lineal si hay una correlación
entre la cantidad de palabras del texto y la cantidad de shares obtenidos hacemos una visualización
en general de los datos de entrada

 visualizamos rápidamente las caraterísticas de entrada
datadrop titleurl elapsed dayshist
pltshow

httpwwwaprendemachinelearningcomwpcontentuploadsreglinealfilasinicialespng
httpwwwaprendemachinelearningcomwpcontentuploadsreglinealstatsbasepng

índice general

nota inicial eeec 
repositorio 
tuopinión 

qué es el machine learning 
definiendo machine learning s 
una definición técnica en 
diagrama de venn ereere re nede ann 
aproximación para programadores ormerrr 
resumen ea aa eeee oeec 

instalar el ambiente de desarrollo python 
por qué instalar python y anaconda en mi ordenador o 
 descargar anaconda e 
 instalar anaconda ee eee 
 iniciar y actualizar anaconda a 
 actualizar libreria scikitlearn a 
 instalar librerías para deep learning eereerre 
resumen ea aa eeee oeec 

análisis exploratorio de datos a 
qué esel eda 
eda deconstruido ae 
qué sacamos del eda e 
técnicas para eda ee 
un eda de pocos minutos con pandas 
más cosas que se suelen hacer e 
resumen ea aa eeee oeec 

regresión lineal con python s 
qué es la regresión lineal 
cómo funciona el algoritmo de regresión lineal en machine learning 
un ejercicio práctico enoce 

predecir cuántas veces será compartido un artículo de machine learning

o ia ek an 

r
id ge wnhhaooyy



regresión lineal con python 

 images video shares

 
 of links of comments



 

ecl 
word count

 

en estas gráficas vemos entre qué valores se concentran la mayoría de registros vamos a filtrar los
datos de cantidad de palabras para quedarnos con los registros con menos de palabras y también
con los que tengan cantidad de compartidos menor a lo gratificaremos pintando en azul los
puntos con menos de palabras la media y en naranja los que tengan más

 vamos a recortar los datos en la zona donde se concentran más los puntos

 esto es en el eje x entre o y 

 y en el eje y entre y 

filtereddata datadataword count data shares 

colores orange blue
tamanios

p


filtereddata word countvalues

filtereddata shares values

 vamos a pintar en colores los puntos por debajo y por encima de la media de cantid
ad de palabras
asignar
for index row in filtered dataiterrows
ifrowword count
asignar appendcolores
else

httpwwwaprendemachinelearningcomwpcontentuploadsreglinvisualizaentradaspng

d



l




regresión lineal con python 

asignar appendcolores

pltscatterf f casignar stamanios
pltshow

 

regresión lineal con python y sklearn

vamos a crear nuestros datos de entrada por el momento sólo word count y como etiquetas los 
shares creamos el objeto linearregression y lo hacemos encajar entrenar con el método fit
finalmente imprimimos los coeficientes y puntajes obtenidos

 asignamos nuestra variable de entrada x para entrenamiento y las etiquetas y
datax filtered data word count

xtrain nparraydatax

ytrain filtereddata sharesvalues

 creamos el objeto de regresión linear
regr linearmodel linearregression

 entrenamos nuestro modelo
regrfitx train ytrain

 httpwwwaprendemachinelearningcomwpcontentuploadsreglinealgraficapalvssharespng

s

regresión lineal con python 

 hacemos las predicciones que en definitiva una línea en este caso al ser d
y pred regrpredictxtrain

 veamos los coeficienetes obtenidos en nuestro caso serán la tangente
printcoefficients w regrcoef

 este es el valor donde corta el eje y en x

printindependent term w regrintercept

 error cuadrado medio

printmean squared error f meansquarederror ytrain y pred
 puntaje de varianza el mejor puntaje es un 

print variance score rscoreytrain y pred

coefficients 
independent term 
mean squared error 
variance score 

de la ecuación de la recta y mx b nuestra pendiente m es el coeficiente y el término
independiente b es tenemos un error cuadrático medio enorme por lo que en realidad
este modelo no será muy bueno prediciendo pero estamos aprendiendo a usarlo que es lo que nos
importa ahora esto también se ve reflejado en el puntaje de varianza que debería ser cercano a

d



o

regresión lineal con python 

visualicemos la recta

veamos la recta que obtuvimos

regresión lineal



cantidad



predicción en regresión lineal simple

vamos a intentar probar nuestro algoritmo suponiendo que quisiéramos predecir cuántos compar
tir obtendrá un articulo sobre ml de palabras

vamos a comprobar

 quiero predecir cuántos shares voy a obtener por un artículo con palabras
 según nuestro modelo hacemos

y dosmil regr predict 

printintydosmi

nos devuelve una predicción de shares para un artículo de palabras

httpwwwaprendemachinelearningcomwpcontentuploadsreglinealrectavariablepng

d



l





regresión lineal con python 

regresión lineal múltiple en python

 regresión con múltiples variables

vamos a extender el ejercicio utilizando más de una variable de entrada para el modelo esto le da
mayor poder al algoritmo de machine learning pues de esta manera podremos obtener predicciones
más complejas nuestra ecuación de la recta ahora pasa a ser

y b mi x m x mn xn

y deja de ser una recta en nuestro caso utilizaremos variables predictivas para poder
graficar en d pero recordar que para mejores predicciones podemos utilizar más de entradas y
prescindir del gráfico nuestra primer variable seguirá siendo la cantidad de palabras y la segunda
variable la crearemos artificialmente y será la suma de columnas de entrada la cantidad de
enlaces comentarios y cantidad de imágenes vamos a programar

vamos a intentar mejorar el modelo con una dimensión más

 para poder graficar en d haremos una variable nueva que será la suma de los enla
ces comentarios e imágenes

suma filtereddata of links filtereddata of commentsfillna filx
tereddata images video

datax pddataframe

dataxword count filtereddataword count
dataxsuma suma

xy train nparraydatax

ztrain filtereddata sharesvalues

ya tenemos nuestras variables de entrada en xytrain y nuestra variable de salida pasa de ser
y a ser el eje z creamos un nuevo objeto de regresión lineal con sklearn pero esta vez tendrá
las dos dimensiones que entrenar las que contiene xytrain al igual que antes imprimimos los
coeficientes y puntajes obtenidos

n

o

o










regresión lineal con python 

 creamos un nuevo objeto de regresión lineal
regr linearmodel linearregression 

 entrenamos el modelo esta vez con dimensiones
 obtendremos coeficientes para graficar un plano
regrfitxytrain ztrain

 hacemos la predicción con la que tendremos puntos sobre el plano hallado
zpred regrpredictxytrain

 los coeficientes

printcoefficients ww regrcoef

 error cuadrático medio

printmean squared error f meansquarederrorztrain zpred
 evaluamos el puntaje de varianza siendo el mejor posible

print variance score rscoreztrain zpred

coefficients 
mean squared error 
variance score 

como vemos obtenemos coeficientes cada uno correspondiente a nuestras variables predictivas
pues ahora lo que graficamos no será una linea si no un plano en dimensiones el error obtenido
sigue siendo grande aunque algo mejor que el anterior y el puntaje de varianza mejora casi el doble
del anterior aunque sigue siendo muy malo muy lejos del 

visualizar un plano en dimensiones en python

graficaremos nuestros puntos de las características de entrada en color azul y los puntos proyectados
en el plano en rojo recordemos que en esta gráfica el eje z corresponde a la altura y representa
la cantidad de shares que obtendremos

o ia ek an 

v n lreleenerennhessehhaarrr
d o oiapean ogogpoiaod e w noo

regresión lineal con python 

fig plt figure
ax axesd fig

 creamos una malla sobre la cual graficaremos el plano
xx yy npmeshgridnplinspace num nplinspace num

 calculamos los valores del plano para los puntos x e y
nuevox regrcoef xx
nuevoy regrcoef yy

 calculamos los correspondientes valores para z debemos sumar el punto de intercep 
ción

z nuevox nuevoy regrintercept

 graficamos el plano
axplotsurfacexx yy z alpha cmaphot

 cgraficamos en azul los puntos en d
axscatter xytrain xy train ztrain cblue s

 craficamos en rojo los puntos que
axscatter xytrain xy train zpred cred s

 con esto situamos la camara con la que visualizamos
axviewinitelev azim

axsetxlabel cantidad de palabras

axsetylabel cantidad de enlacescomentarios e imagenes
axsetzlabel compartido en redes

axsettitle regresión lineal con múltiples variables

d



o

regresión lineal con python 

regresión lineal con múltiples variables







 o
 u
 zoo mildad de palabra



podemos rotar el gráfico para apreciar el plano desde diversos ángulos modificando el valor del
parámetro azim en viewinit con números de a 

predicción con el modelo de mútiples variables

veamos ahora que predicción tendremos para un artículo de palabras con enlaces 
comentarios y imágenes

 si quiero predecir cuántos shares voy a obtener por un artículo con
 palabras y con enlaces comentarios imagenes 

zdosmil regrpredict 
printintzdosmi

esta predicción nos da y probablemente sea un poco mejor que nuestra predicción anterior
con variables

resumen

hemos visto cómo utilizar sklearn en python para crear modelos de regresión lineal con 
múltiples variables en nuestro ejercicio no tuvimos una gran confianza en las predicciónes por

httpwwwaprendemachinelearningcomwpcontentuploadsregresionlinealdplanopng

regresión lineal con python 

ejemplo en nuestro primer modelo con palabras nos predice que podemos tener pero el
margen de error haciendo raíz del error cuartico medio es más menos es decir que escribiendo
un artículo de palabras lo mismo tenemos shares que en este caso usamos este
modelo para aprender a usarlo y habrá que ver en otros casos en los que sí nos brinde predicciones
acertadas para mejorar nuestro modelo deberíamos utilizar más dimensiones y encontrar datos de
entrada mejores

atención también es posible que no exista ninguna relación fuerte entre nuestras variables de
entrada y el éxito en shares del artículo

recursos y enlaces

 descarga la jupyter notebook y el archivo de entrada csv
 ó puedes visualizar online
 over y descargar desde mi cuenta github

otros enlaces con artículos sobre regresión lineal en inglés

 introduction to linear regression using python

 linear regression using python sklearn

e linear regression detailed view

 how do you solve a linear regression problem in python

 python tutorial on linearregression with batch gradient descent

 httpwwwaprendemachinelearningcomejercicioregresionlineal
httpwwwaprendemachinelearningcomarticulosml
httpsgithubcomjbagnatomachinelearningblobmasterejercicioregresionlinealipynb
httpsgithubcomjbagnatomachinelearning

 httpsaktechthoughtswordpresscomintroductiontolinearregressionusingpython
httpsdzonecomarticleslinearregressionusing pythonscikitlearn
httpstowardsdatasciencecomlinearregression detailedvieweafe
httplineardatanethowdoyousolvealinearregression machinelearningprobleminpython
httpozzieliucomgradientdescenttutorial

regresión logística

introducción

utilizaremos algoritmos de machine learning en python para resolver un problema de regresión
logística a partir de un conjunto de datos de entrada características nuestra salida será discreta
y no continua por eso utilizamos regresión logística y no regresión lineal la regresión
logística es un algoritmo supervisado y se utiliza para clasificación

vamos a clasificar problemas con dos posibles estados sino binario o un número finito de
etiquetas o clases múltiple

algunos ejemplos de regresión logística son

clasificar si el correo que llega es spam o no es spam

 dados unos resultados clínicos de un tumor clasificar en benigno o maligno

 el texto de un artículo a analizar es entretenimiento deportes política ó ciencia
 a partir de historial bancario conceder un crédito o no

confiaremos en la implementación del paquete scikitlearn de python para ponerlo en práctica

ejercicio de regresión logística en python

para el ejercicio he creado un archivo csv con datos de entrada a modo de ejemplo para clasificar si
el usuario que visita un sitio web usa como sistema operativo windows macintosh o linux nuestra
información de entrada son características que tomé de una web que utiliza google analytics y
son

 duración de la visita en segundos

 cantidad de páginas vistas durante la sesión

e cantidad de acciones del usuario click scroll uso de checkbox etc

 suma del valor de las acciones cada acción lleva asociada una valoración de importancia

httpsenwikipediaorgwikilogisticregression
httpwwwaprendemachinelearningcomregresionlinealenespanolconpython
shttpwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado
httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning
shttpwwwaprendemachinelearningcomwpcontentuploadsusuarioswinmaclincsv

índice general

regresión lineal con python y sklearn r s 
visualicemos la recta eeseemeeedi e eere eee 
predicción en regresión lineal simple 
regresión lineal múltiple en python 
visualizar un plano en dimensiones en python 
predicción con el modelo de mútiples variables 
resumen emedrererderverdederverve verdevedederererer 
regresión logística 
introducción e i a eee eee e 
ejercicio de regresión logística en python 
regresión logística con sklearn ea 
visualización de datos eeemreerie ee eee 
creamos el modelo de regresión logística 
validación de nuestro modelo rere re e 
reporte de resultados del modelo e 
clasificación de nuevos valores oemrier 
resumen emedrererderverdederverve verdevedederererer 
arbol de decisión eri e aeee 
qué es un árbol de decisión a 
cómo funciona un árbol de decisión 
arbol de decisión con scikitlearn paso a paso 
predicción del billboard qué artista llegará al número uno del ranking 
obtención de los datos de entrada rri e 
análisis exploratorio inicial s 
balanceo de datos pocos artistas llegan al número uno 
preparamos los datos 
mapeo de datos ad 
buscamos la profundidad para el árbol de decisión 
visualización del árbol de decisión xi i 
análisis del árbol eee 
predicción de canciones al billboard ei 
resumen emedrererderverdederverve verdevedederererer 
qué es overfitting y cómo solucionarlo 
generalización del conocimiento riee 
el problema de la máquina al generalizar 
overfitting en machine learning a 
el equilibrio del aprendizaje 
prevenir el sobreajuste de datos 
resumen emedrererderverdederverve verdevedederererer 

datos desbalanceados

d



l




regresión logística 

como la salida es discreta asignaremos los siguientes valores a las etiquetas

 windows

 macintosh

 linux

la muestra es pequeña son registros para poder comprender el ejercicio pero recordemos que
para conseguir buenos resultados siempre es mejor contar con un número abundante de datos que
darán mayor exactitud a las predicciones

regresión logística con sklearn

identificar sistema operativo de los usuarios

para comenzar hacemos los import necesarios con los paquetes que utilizaremos en el ejercicio

import pandas as pd

import numpy as np

from sklearn import linear model

from sklearn import modelselection

from sklearnmetrics import classificationreport
from sklearnmetrics import confusionmatrix

from sklearnmetrics import accuracyscore

import matplotlibpyplot as plt

import seaborn as sb

zmatplotlib inline

leemos el archivo csv por sencillez se considera que estará en el mismo directorio que el archivo
de notebook ipynb y lo asignamos mediante pandas a la variable dataframe mediante el método
dataframehead vemos en pantalla los primeros registros

dataframe pdreadcsvrusuarioswinmaclincsv
dataframe head

regresión logística 

a continuación llamamos al método dataframedescribe que nos dará algo de información
estadística básica de nuestro set de datos la media el desvío estándar valores mínimo y máximo
de cada característica

 dataframedescribe

 jaración egas acsones r jaae 
u aa c ao sa
eanri esszorea jos 
ua n oo rr acoa aa josre

i oosooasoas ooo asoao an

max 

luego analizaremos cuantos resultados tenemos de cada tipo usando la función groupby y vemos
que tenemos usuarios clase es decir windows usuarios mac y de linux

regresión logística 

 printdataframegroupbyclasesize

clase

 
 
 

dtype int

visualización de datos

antes de empezar a procesar el conjunto de datos vamos a hacer unas visualizaciones que muchas
veces nos pueden ayudar a comprender mejor las características de la información con la que
trabajamos y su correlación primero visualizamos en formato de historial los cuatro features de
entrada con nombres duración páginasacciones y valor podemos ver gráficamente entre qué
valores se comprenden sus mínimos y máximos y en qué intervalos concentran la mayor densidad
de registros

 dataframedrop clasehist
 pltshow

acciones duracion


d 
 
o a o 
paginas valor





 s 

 o

regresión logística 

y también podemos interrelacionar las entradas de a pares para ver como se concentran linealmente

las salidas de usuarios por colores sistema operativo windows en azul macintosh en verde y linux
en rojo

sbpairplotdataframedropna hueclasesizevarsduracion paginasacciwn
ones valor kindreg

e

duracion

 beneei

d

a

regresión logística 

creamos el modelo de regresión logística

ahora cargamos las variables de las columnas de entrada en x excluyendo la columna clase con
el método drop en cambio agregamos la columna clase en la variable y ejecutamos xshape para
comprobar la dimensión de nuestra matriz con datos de entrada de registros por columnas

nparraydataframe drop clase

nparraydataframeclase
xshape

 y creamos nuestro modelo y hacemos que se ajuste fit a nuestro conjunto de entradas x y
salidas y
model linearmodel logisticregression 

model fitxy

una vez compilado nuestro modelo le hacemos clasificar todo nuestro conjunto de entradas x
utilizando el método predictx y revisamos algunas de sus salidas y vemos que coincide con
las salidas reales de nuestro archivo csv

predictions modelpredictx
printpredictions 



y confirmamos cuan bueno fue nuestro modelo utilizando modelscore que nos devuelve la
precisión media de las predicciones en nuestro caso del 

model scorexy


validación de nuestro modelo

una buena práctica en machine learning es la de subdividir nuestro conjunto de datos de entrada
en un set de entrenamiento y otro para validar el modelo que no se utiliza durante el entrenamiento
y por lo tanto la máquina desconoce esto evitará problemas en los que nuestro algoritmo pueda
fallar por sobregeneralizar el conocimiento para ello dividimos nuestros datos de entrada en
forma aleatoria mezclados utilizando de registros para entrenamiento y para testear

httpwwwaprendemachinelearningcomqueesoverfittingyunderfittingycomosolucionarlo

ne

a

d



o

regresión logística 

validationsize 

seed 

xtrain x validation y train y validation modelselectiontraintestsplitx yxn
 testsizevalidationsize randomstateseed

volvemos a compilar nuestro modelo de regresión logística pero esta vez sólo con de los datos
de entrada y calculamos el nuevo scoring que ahora nos da 

namelogistic regression
kfold modelselectionkfoldnsplits randomstateseed

cvresults modelselectioncrossvalscoremodel xtrain ytrain cvkfold scor y
ingaccuracy

msg s z zf name cvresultsmean cvresultsstd

printmsg

logistic regression 

y ahora hacemos las predicciones en realidad clasificación utilizando nuestro test set es decir
del subconjunto que habíamos apartado en este caso vemos que los aciertos fueron del pero
hay que tener en cuenta que el tamaño de datos era pequeño

predictions modelpredictx validation
printaccuracyscore yvalidation predictions



finalmente vemos en pantalla la matriz de confusión donde muestra cuantos resultados equivo
cados tuvo de cada clase los que no están en la diagonal por ejemplo predijo usuarios que eran
mac como usuarios de windows y predijo a usuarios linux que realmente eran de windows

reporte de resultados del modelo

printconfusion matrixy validation predictions

regresión logística 

también podemos ver el reporte de clasificación con nuestro conjunto de test en nuestro caso vemos
que se utilizaron como soporte registros windows de mac y de linux total de registros
podemos ver la precisión con que se acertaron cada una de las clases y vemos que por ejemplo de
macintosh tuvo aciertos y fallos recall la valoración que de aqui nos conviene tener en
cuenta es la de fscore que tiene en cuenta la precisión y recall el promedio de f es de lo
cual no está nada mal

 printclassification reporty validation predictions

precision recall flscore support

 

 

 

avg total 

clasificación de nuevos valores

como último ejercicio vamos a inventar los datos de entrada de navegación de un usuario ficticio
que tiene estos valores

 tiempo duración 
 paginas visitadas 

 acciones al navegar 
 valoración 

lo probamos en nuestro modelo y vemos que lo clasifica como un usuario tipo es decir de linux

 x new pddataframe duracion paginas acciones valor n
 
 modelpredictx new

httpsenwikipediaorgwikifscore

regresión logística 

array

los invito a jugar y variar estos valores para obtener usuarios de tipo windows o macintosh

resumen

durante este artículo vimos cómo crear un modelo de regresión logística en python para poder
clasificar el sistema operativo de usuarios a partir de sus características de navegación en un sitio
web este ejemplo se podrá extender a otro tipos de tareas que pueden surgir durante nuestro trabajo
en el que deberemos clasificar resultados en valores discretos si tuviéramos que predecir valores
continuos deberemos aplicar regresión lineal

recuerda descargar los archivos para realizar el ejercicio

 archivo de entrada csv su nombre es usuarioswinmaclincsv

notebook jupyter python clic derecho y descargar archivo como

 opción se puede ver online en jupyter notebook viewer

 opción se puede visualizar y descargar el notebook y el csv desde mi cuenta de github

httpwwwaprendemachinelearningcomregresionlinealen espanolconpython
httpwwwaprendemachinelearningcomwpcontentuploadsusuarioswinmaclincsv
ohttpwwwaprendemachinelearningcomwpcontentuploadsregresionlogisticaipynb
httpnbviewerjupyterorggithubjbagnatomachinelearningblobmasterregresionlogisticaipynb
httpsgithubcomjbagnatomachinelearningblobmasterregresionlogisticaipynb

 httpsgithubcomjbagnatomachinelearningblobmasterusuarioswinmaclincsv
httpsgithubcomjbagnatomachinelearning

arbol de decisión

en este capítulo describiremos en qué consisten y cómo funcionan los árboles de decisión utilizados
en aprendizaje automático y nos centraremos en un divertido ejemplo en python en el que
analizaremos a los cantantes y bandas que lograron un puesto número uno en las listas de billboard
hot e intentaremos predecir quién será el próximo ed sheeran a fuerza de inteligencia
artificial

realizaremos gráficas que nos ayudarán a visualizar los datos de entrada y un grafo para interpretar
el árbol que crearemos con el paquete scikitlearn comencemos

 z

qué es un árbol de decisión

los arboles de decisión son representaciones gráficas de posibles soluciones a una decisión
basadas en ciertas condiciones es uno de los algoritmos deaprendizaje supervisado más utilizados
en machine learning y pueden realizar tareas de clasificación o regresión acrónimo del inglés
cart

la comprensión de su funcionamiento suele ser simple y a la vez muy potente utilizamos
mentalmente estructuras de árbol de decisión constantemente en nuestra vida diaria sin darnos
cuenta

llueve lleva paraguas

soleado lleva gafas de sol

estoy cansado toma café

son decisiones del tipo if this then that

los árboles de decisión tienen un primer nodo llamado raíz root y luego se descomponen el resto de
atributos de entrada en dos ramas podrían ser más pero no nos meteremos en eso ahora planteando
una condición que puede ser cierta o falsa se bifurca cada nodo en y vuelven a subdividirse
hasta llegar a las hojas que son los nodos finales y que equivalen a respuestas a la solución sino
comprarvender o lo que sea que estemos clasificando otro ejemplo son los populares juegos de
adivinanza

 animal ó vegetal animal
 tiene cuatro patas si

 hace guau si

 es un perro

httpwwwaprendemachinelearningcomqueesmachinelearning

 httpswwwbillboardcomchartshot

httpswwwbillboardcommusicedsheeran
httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado
httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning
httpsenwikipediaorgwikiclassificationandregressiontree

arbol de decisión 

qué necesidad hay de usar el algoritmo de arbol

supongamos que tenemos atributos como género con valores hombre ó mujer y edad en rangos
menor de ó mayor de para tomar una decisión podríamos crear un árbol en el que dividamos
primero por género y luego subdividir por edad ó podría ser al revés primero por edad y luego por
género el algoritmo es quien analizando los datos y las salidas por eso es supervisado decidirá
la mejor forma de hacer las divisiones splits entre nodos tendrá en cuenta de qué manera lograr
una predicción clasificación ó regresión con mayor probabilidad de acierto parece sencillo no
pensemos que si tenemos atributos de entrada cada uno con o más valores posibles las
combinaciones para decidir el mejor árbol serían cientos ó miles esto ya no es un trabajo
para hacer artesanalmente y ahí es donde este algoritmo cobra importancia pues él nos devolverá
el árbol óptimo para la toma de decisión más acertada desde un punto de vista probabilístico

cómo funciona un árbol de decisión

para obtener el árbol óptimo y valorar cada subdivisión entre todos los árboles posibles y conseguir
el nodo raiz y los subsiguientes el algoritmo deberá medir de alguna manera las predicciones
logradas y valorarlas para comparar de entre todas y obtener la mejor para medir y valorar
utiliza diversas funciones siendo las más conocidas y usadas los indice gini y ganancia de
información que utiliza la denominada entropía la división de nodos continuará hasta que
lleguemos a la profundidad máxima posible del árbol ó se limiten los nodos a una cantidad
mínima de muestras en cada hoja a continuación describiremos muy brevemente cada una de las

estrategias nombradas

indice gini

se utiliza para atributos con valores continuos precio de una casa esta función de coste mide el
grado de impureza de los nodos es decir cuán desordenados o mezclados quedan los nodos
una vez divididos deberemos minimizar ese gini index

ganancia de información

se utiliza para atributos categóricos como en hombremujer este criterio intenta estimar la
información que aporta cada atributo basado en la teoría de la información para medir la
aleatoriedad de incertidumbre de un valor aleatorio de una variable x se define la entropia
al obtener la medida de entropía de cada atributo podemos calcular la ganancia de información del
árbol deberemos maximizar esa ganancia

 httpsenwikipediaorgwikidecisiontreelearningéginiimpurity
httpsenwikipediaorgwikiinformationgainindecisiontrees
shttpseswikipediaorgwikientropcadainformacicbn
httpseswikipediaorgwikiteorcadadelainformacicbn
httpseswikipediaorgwikientropcadainformacicbn

índice general

problemas de clasificación con clases desequilibradas e
cómo nos afectan los datos desbalanceados 
métricas y confusion matrix 
vamos al ejercicio con python 
análisis exploratorio 

probando el modelo sin estrategias e
estrategia penalización para compensar r s
estrategia subsampling en la clase mayoritaria 
estrategia oversampling de la clase minoritaria 
estrategia combinamos resampling con smotetomek qe
estrategia ensamble de modelos con balanceo 
resultados de las estrategias 
resumen emedrererderverdederverve verdevedederererer

random forest el poder del ensamble 
cómo surge random forest 
cómo funciona random forest a
por qué es aleatorio eee
ventajas y desventajas del uso de random forest 
vamos al código python 
creamos el modelo y lo entrenamos a
los hiperparámetros más importantes ss
evaluamos resultados eesedie e
comparamos con el baseline a
resumen emedrererderverdederverve verdevedederererer

conjunto de entrenamiento test y validación 
un nuevo mundo saeie eee en eeeene a
hágase el conjunto de test s
al séptimo día dios creo el crossvalidation 
técnicas de validación cruzada sedede 
ejemplo kfolds en python eee
más técnicas para validación del modelo 
series temporales atención al validar 
pero entonces cuando uso crossvalidation ee

kmeans rr ederee eee eerererereeererccec
cómo funciona kmeans edemmm a eooaa oeo cmz
casos de uso de kmeans dreream
datos de entrada para kmeans a
el algoritmo kmeans ene nn

ne



o

arbol de decisión 

arbol de decisión con scikitlearn paso a paso

para este ejercicio me propuse crear un set de datos original e intentar que sea divertido a la vez que
explique de forma clara el funcionamiento del árbol

predicción del billboard qué artista llegará al
número uno del ranking

a partir de atributos de cantantes y de un histórico de canciones que alcanzaron entrar al billboard
 us en y crearemos un árbol que nos permita intentar predecir si un nuevo cantante
podrá llegar a número uno

obtención de los datos de entrada

utilicé un código python para hacer webscraping de una web pública ultimate music database
con información histórica del billboard que obtuve de este artículo analyzing billboard 
luego completé atributos utilizando la api de deezer duración de las canciones la api de
gracenote género y ritmo de las canciones finalmente agregué varias fechas de nacimiento de
artistas utilizando la wikipedia que no había conseguido con la ultimate music database algunos
artistas quedaron sin completar su fecha de nacimiento y con valor veremos como superar este
obstáculo tratando los datos para empezar importemos las librerías que utilizaremos y revisemos
sus atributos de entrada

 imports

import numpy as np

import pandas as pd

import seaborn as sb

import matplotlibpyplot as plt

zmatplotlib inline

pltrcparamsfigure figsize 
pltstyleuse ggplot

from sklearn import tree

from sklearnmetrics import accuracyscore
from sklearnmodelselection import kfold
from sklearnmodelselection import crossvalscore
from ipythondisplay import image as pimage

httpwwwaprendemachinelearningcomejemplowebscrapingpythonibexbolsavalores
 httpmikeklingcomanalyzingthebillboardhot

httpsdevelopersdeezercomapi

httpsdevelopergracenotecomwebapi

arbol de decisión 

from subprocess import checkcall
from pil import image imagedraw imagefont

si te falta alguna de ellas recuerda que puedes instalarla con el entorno anaconda o con la
herramienta pip para este ejercicio recuerda instalar pillow para graficar con pip install pillow
en tu terminal

análisis exploratorio inicial

ahora veamos cuantas columnas y registros tenemos
artistsbillboardshape

esto nos devuelve es decir que tenemos columnas features y filas de datos vamos
a echar un ojo a los primeros registros para tener una mejor idea del contenido

artistsbillboardhead
id title artist mood tempo genre artisttype chart date durationseg top anionacimiento
smail town brantley gilbert featuring medium 
o throwdown justin moore thom brooding tempo traditional male 
jessie j ariana grande medium
 bang bang nicki minaj energizing tempo pop female 
 medium 
 timber pitbull featuring kesha excited po uban mixed 
sweater medium alternative
 nnihor the neighbourhood brooding iro a punk male 
medium
 automatic miranda lambert yearning tempo traditional female 

vemos que tenemos titulo de la canción artista mood ó estado de ánimo de esa canción tempo
género tipo de artista fecha en que apareció en el billboard por ejemplo equivale al 

de junio de la columna top será nuestra etiqueta en la que aparece si llegó al número
uno de billboard ó si no lo alcanzó y el anio de nacimiento del artista vemos que muchas de
las columnas contienen información categórica la columna durationseg contiene la duración en
segundos de la canción siendo un valor continuo pero que nos convendrá pasar a categórico más
adelante vamos a realizar algunas visualizaciones para comprender mejor nuestros datos primero
agrupemos registros para ver cuántos alcanzaron el número uno y cuantos no

artistsbillboardgroupby topsize

 httpwwwaprendemachinelearningcomwpcontentuploadsbillboarddatasetheadpng

arbol de decisión 

nos devuelve top es decir que tenemos canciones que no alcanzaron la cima y a 

que alcanzaron el número uno esto quiere decir que tenemos una cantidad desbalanceada
de etiquetas con y lo tendremos en cuenta al momento de crear el árbol
visualizamos esta diferencia





count





top

nuestras etiquetas que indican no llego al top y llego al número uno billboard están
desbalanceadas

deberemos resolver este inconveniente

veamos cuántos registros hay de tipo de artista mood tempo y género de las canciones

sbfactorplot artisttype dataartistsbillboardkindcount

 httpwwwaprendemachinelearningcomclasificacioncondatos desbalanceados

arbol de decisión

munt

female
artisttype

aqui vemos que tenemos más del doble de artistas masculinos que femeninos y unos registros
de canciones mixtas

 sbfactorplotmood dataartistsbillboardkindcount aspect

erglzmkcuedvearmngwnea cool ugenpggresumwsucalansensnbpowelngrlfy romanticrowdy other fierysentimerfaisygoinirinylelanchobeaceful lively

munt

brood

vemos que de tipos de mood destacan con picos altos además notamos que algunos estados
de ánimo son similares

 sbfactorplot tempodataartistsbillboardhuetopkind

count

arbol de decisión 

count

top
m 
 



 

medium tempo slow tempo fast tempo
tempo

en esta gráfica vemos que hay tipos de tempo medium slow y fast evidentemente predominan

los tiempos medium y también es donde encontramos más canciones que hayan alcanzado el top 
en azul

 sbfactorplotgenre dataartistsbillboardkindcount aspect







sd 

 
urban

traditional pop

count

alemative punk electronica other soundtrack rock
genre

entre los géneros musicales destacan urban y pop seguidos de tradicional
veamos ahora qué pasa al visualizar los años de nacimiento de los artistas

arbol de decisión 

 sbfactorplot anionacimientodataartists billboardkindcount aspect





e

oount

d
n iiiiiiiiiiii 

 
anionacimiento

aqui notamos algo raro en el año cero tenemos cerca de registros

como se ve en la gráfica tenemos cerca de canciones de las cuales desconocemos el año de
nacimiento del artista el resto de años parecen concentrarse entre y a ojo más adelante
trataremos estos registros

balanceo de datos pocos artistas llegan al número
uno

como dijimos antes no tenemos equilibrio en la cantidad de etiquetas top y notop de las
canciones esto se debe a que en el transcurso de un año apenas unas o canciones logran el
primer puesto y se mantienen durante varias semanas en ese puesto cuando inicialmente extraje
las canciones utilicé y y tenía apenas a canciones en el top de billboard y que no
llegaron para intentar equilibrar los casos positivos agregué solamente los top de los años 
al con eso conseguí los valores que tenemos en el archivo csv son notop y top a
pesar de esto sigue estando desbalanceado y podríamos seguir agregando sólo canciones top de
años previos pero utilizaremos un parámetro classweight del algoritmo de árbol de decisión para
compensar esta diferencia

en el capítulo clasificación con datos desbalanceados te cuento todas las estrategias
para equilibrar las clases

visualicemos los top y no top de acuerdo a sus fechas en los charts

httpwwwaprendemachinelearningcomclasificacioncondatos desbalanceados

n

o

l









arbol de decisión 

p


artistsbillboardchartdate values

artistsbillboarddurationseg values

colores orange blue si no estaban declarados previamente
tamanios si no estaban declarados previamente

asignar

asignar

for index row in artistsbillboarditerrows
asignar appendcolores rowtop
asignarappend tamanios row top

pltscatterf f casignar sasignar
pltaxis 

pltshow





 


 h t
 o 
 y e i
 t 
o l s l
e artoos 
 
 
 o 
 e e c 
 
 
 e o r
 

e

en nuestro conjunto de datos se agregaron canciones que llegaron al top en azul de años al
 para sumar a los apenas que lo habían logrado en

ne



l

d



l



arbol de decisión 

preparamos los datos

vamos a arreglar el problema de los años de nacimiento que están en cero realmente el feature
o característica que queremos obtener es sabiendo el año de nacimiento del cantante calcular
qué edad tenía al momento de aparecer en el billboard por ejemplo un artista que nació en
 y apareció en los charts en tenía años vamos a sustituir los ceros de la columna
anionacimiento por el valor none que es es nulo en python

def edadfixanio
if anio
return none
return anio

artistsbillboardanionacimientoartistsbillboardapplylambda x edadfixxanx
ionacimiento axis

luego vamos a calcular las edades en una nueva columna edadenbillboard restando el año de
aparición los primeros caracteres de chartdate al año de nacimiento en las filas que estaba el
año en none tendremos como resultado edad none

def calculaedadaniocuando
cad strcuando
momento cad
if anio
return none
return intmomento anio

artistsbillboardedadenbillboardartistsbillboardapply lambda x calculaedayx
dxanionacimientoxchartdate axis

y finalmente asignaremos edades aleatorias a los registros faltantes para ello obtenemos el promedio
de edad de nuestro conjunto avg y su desvío estándar std por eso necesitábamos las edades en
none y pedimos valores random a la función que van desde avg std hasta avg std en nuestro
caso son edades de entre a años

rn 

o










s s

o









arbol de decisión 

ageavg artistsbillboardedadenbillboard mean
agestd artistsbillboardedadenbillboardsta
agenullcount artistsbillboardedadenbillboardisnullsum

agenullrandomlist nprandomrandintageavg agestd ageavg agestd sizew
agenulcount

convaloresnulos npisnanartistsbillboardedadenbillboard

artistsbillboardlocnpisnanartistsbillboard edadenbillboard edadenbilxn
lboard agenullrandomlist

artistsbillboardedadenbillboard artistsbillboardedadenbillboard asty n
peint

printedad promedio strageavg

printdesvió std edad stragestd

printintervalo para asignar edad aleatoria strintageavg agestd a
 strintageavg agestd

si bien lo ideal es contar con la información real y de hecho la podemos obtener buscando
en wikipedia o en otras webs de música quise mostrar otra vía para poder completar datos
faltantes manteniendo los promedios de edades que teníamos en nuestro conjunto de datos podemos
visualizar los valores que agregamos en color verde en el siguiente gráfico

p


artistsbillboardedadenbillboard values

artistsbillboardindex

colores orangebluegreen

asignar
for index row in artistsbillboarditerrows
if convaloresnulosindex
asignar appendcolores verde
else
asignar appendcolores rowtop

pltscatterf f casignar s
pltaxis 
pltshow

ae o n 

o








arbol de decisión 

 
 
 e e ra g 
 
 
 i 
o
 eorote d
n 
 a 
d d 
c e e
 o 
a ej 
a 
 

 r 
 o teras
o j s e te
 l n 
 
 o 
 
 u

 
 h 
 l 
o m o 

b a s 
 
d k o
 olo 
 lt 
 l 
 e e 
 
 o 
 la 
o o 
 
 h 
 h
o e 

mapeo de datos

vamos a transformar varios de los datos de entrada en valores categóricos las edades las separamos
en menor de años entre y etc las duraciones de canciones también por ej entre y 
segundos etc para los estados de ánimo mood agrupé los que eran similares el tempo que puede
ser lento medio o rápido queda mapeado rapido lento medio por cantidad de canciones
en cada tempo el medio es el que más tiene

 mood mapping
artistsbillboardmoodencoded artistsbillboard moodmap fenergizing 
empowering 
cool 
yearning anhelo deseo ansia
excited emocionado
defiant 
sensual 
gritty coraje
sophisticated 
aggressive provocativo
fiery caracter fuerte
urgent 
rowdy ruidoso alboroto

índice general

elegir el valor dek 
ejemplo kmeans con scikitlearn 
agrupar usuarios twitter de acuerdo a su personalidad con kmeans 
visualización de datos eeemreerie ee eee 
definimos la entradaeeeemdi eee 
obtener el valor k re eee 
ejecutamos kmeans eeeo oec 
clasificar nuevas muestras eeeceseeededi re re ee ee 
resumen emedrererderverdederverve verdevedederererer 
knearestneighbor e 
qué es el algoritmo knearest neighbor 
dónde se aplica knearest neighbor 
ac 
cómo funciona knn 
un ejemplo knearest neighbor en python 
el ejercicio app reviews e 
un poco de visualización 
preparamos las entradas e 
usemos knearest neighbor con scikit learn 
precisión del modelo i eee 
y ahora la gráfica que queríamos ver aa 
elegir el mejor valor de k a 
clasificar ó predecir nuevas muestras oceereeeerierire ee ee 
resumen emedrererderverdederverve verdevedederererer 
naive bayes comprar casa o alquilar 
los datos de entrada 
el teorema de bayes an 
clasificador gaussian naive bayes 
visualización de datos eeemreerie ee eee 
preparar los datos de entrada 
feature selection ó selección de características eeeea 
crear el modelo gaussian naive bayes con sklearn eeme 
probemos el modelo comprar o alquilar 
resumen emedrererderverdederverve verdevedederererer 
sistemas de recomendación eseieri 
qué son los sistemas ó motores de recomendación e 
tipos de motores an re nn 
cómo funciona collaborative filtering 
predecir gustos userbased e 
ejercicio en python sistema de recomendación de repositorios github 

dividimos en train y testset

arbol de decisión

sentimental 
easygoing 
melancholy 
romantic 
peaceful 



sencillo

brooding melancolico

upbeat optimista alegre

stirring emocionante

lively animado
other astypeint

 tempo mapping
artistsbillboardtempoencoded
 medium tempo slow tempo
 genre mapping
artistsbillboardgenreencoded
pop 
traditional 

 artistsbillboardtempo map
 astypeint

fast tempo on

 artistsbillboardgenremap furban 

alternative punk 

electronica 

rock 
soundtrack 
jazz 

other
 astypeint
 artisttype mapping

artistsbillboardartisttypeencoded 

male male mixed 

 mapping edad en la que llegaron
artistsbi bi

artistsbi
oard edadenbillboard
artistsbi
oard edadenbillboard
artistsbi
oard edadenbillboard
artistsbi



boardloc artists

boardlocartists

artists

artistsbi


artistsbi


boara loc

boara loc



boara loc

 mapping song duration

artistsbillboardloc
o

artistsbillboard

artistsbillboardartisttypemap fex
 astypeint

al billboard

boardedadenbillboard edadencoded xn

boardedadenbillboard artistsbillbx

edadencoded 

boardedadenbillboard artistsbillbx

edadencoded 

boardedadenbillboard artistsbillbx

edadencoded 

boardedadenbillboard edadencoded x
durationseg durationencoded y

d

a

arbol de decisión 

artistsbillboardlocartistsbillboarddurationseg artistsbillboard x

durationseg durationencoded 
artistsbillboardlocartistsbillboarddurationseg artistsbillboard x

durationseg durationencoded 
artistsbillboardlocartistsbillboarddurationseg artistsbillboard x

durationseg durationencoded 
artistsbillboardlocartistsbillboarddurationseg artistsbillboard x

durationseg durationencoded 

artistsbillboardlocartistsbillboarddurationseg artistsbillboard x

durationseg durationencoded 

artistsbillboardloc artistsbillboarddurationseg durationencoded 

finalmente obtenemos un nuevo conjunto de datos llamado artistsencoded con el que tenemos
los atributos definitivos para crear nuestro árbol para ello quitamos todas las columnas que no
necesitamos con drop

dropelements idtitleartistmoodtempo genre artisttype chartdw
ate anionacimiento durationseg edadenbillboard
artistsencoded artistsbillboarddropdropelements axis 

como quedan los top en relación a los datos mapeados

revisemos en tablas cómo se reparten los top en los diversos atributos mapeados sobre la columna
sum estarán los top pues al ser valor o sólo se sumarán los que sí llegaron al número 

artistsencoded moodencoded topgroupby moodencoded asindexfalseagg x
mean count sum

top

mean count sum

moodencoded
o o
 o

 

 

 

 

la lin

 

la mayoría de top los vemos en los estados de ánimo y con y canciones

arbol de decisión 

 artistsencodedartist typeencoded topgroupby artisttypeencoded asinyn

 dexfalseagg mean count sum
top
mean count sum

artisttypeencoded

 
 
 

aqui están bastante repartidos pero hay mayoría en tipo artistas masculinos

 artistsencodedgenreencoded topgroupby genreencoded asindexfalseax
 gg mean count sum
top
mean count sum
genreencoded
o 
 
 
 
 

los géneros con mayoría son evidentemente los géneros y que corresponden con urbano y pop

 artistsencoded tempoencoded topgroupby tempoencoded asindexfalse ax
 gg mean count sum

arbol de decisión



count sum

tempoencoded







el tempo con más canciones exitosas en el número es el tempo medio

artistsencoded durationencoded topgroupby durationencoded asindexfayn
lseagg mean count sum
top
mean count sum
durationencoded
 
 
 
 
 
 
 

están bastante repartidos en relación a la duración de las canciones

artistsencoded edadencoded

mean

count

sum

topgroupby edadencoded asindexfalseaggx

arbol de decisión 

edadencoded

 
 
 
 
 

edad con mayoría es la tipo que comprende de a años

buscamos la profundidad para el árbol de decisión

ya casi tenemos nuestro árbol antes de crearlo vamos a buscar cuántos niveles de profundidad
le asignaremos para ello aprovecharemos la función de kfold que nos ayudará a crear varios
subgrupos con nuestros datos de entrada para validar y valorar los árboles con diversos niveles
de profundidad de entre ellos escogeremos el de mejor resultado

creamos el árbol y lo tuneamos

para crear el árbol utilizamos de la librería de sklearn treedecision treeclasifier pues buscamos
un árbol de clasificación no de regresión lo configuramos con los parámetros

 criterionentropy ó podría ser gini pero utilizamos entradas categóricas
minsamplessplit se refiere a la cantidad mínima de muestras que debe tener un nodo
para poder subdividir

minsamplesleaf cantidad mínima que puede tener una hoja final si tuviera menos no
se formaría esa hoja y subiría un nivel su antecesor

classweight importantísimo con esto compensamos los desbalances que hubiera en
nuestro caso como venía diciendo anteriormente tenemos menos etiquetas de tipo top los
artistas que llegaron al número del ranking por lo tanto le asignamos de peso a la etiqueta
 para compensar el valor sale de dividir la cantidad de top son con los top son 

nota estos valores asignados a los parámetros fueron puestos luego de prueba y error muchas
veces visualizando el árbol en el siguiente paso y retrocediendo a este

 httpscikitlearnorgstablemodulesgeneratedsklearntree decisiontreeclassifierhtml
httpwwwaprendemachinelearningcomclasificacioncondatos desbalanceados

n

o ad























arbol de decisión 

cv kfoldnsplits numero deseado de folds que haremos
accuracies list

maxattributes lenlistartistsencoded

depthrange range maxattributes 

 testearemos la profundidad de a cantidad de atributos 
for depth in depthrange
foldaccuracy 
treemodel treedecisiontreeclassifiercriterionentropy
minsamplessplit
minsamplesleaf
maxdepth depth
classweight
for trainfold validfold in cvsplitartistsencoded
ftrain artistsencoded loctrainfold
fvalid artistsencoded locvalidfold

model treemodelfitx ftraindrop top axis
y ftraintop
validacc modelscorex f validdrop top axis
y fvalidtop calculamos la precision con el
segmento de validacion
foldaccuracy appendvalidacc

avg sum foldaccuracylen foldaccuracy
accuracies appendavg

 mostramos los resultados obtenidos
df
df
printdftostring indexfalse

pddataframe max depth depthrange average accuracy accuracies

dfmax depth average accuracy

max depth average accuracy

 







iaurwn

podemos ver que en niveles de splits tenemos el score más alto con casi ahora ya sólo nos
queda crear y visualizar nuestro árbol de niveles de profundidad

s

o oa















arbol de decisión 

 z

visualización del árbol de decisión

asignamos los datos de entrada y los parámetros que configuramos anteriormente con niveles de
profundidad utilizaremos la función de exportgraphviz para crear un archivo de extensión dot
que luego convertiremos en un gráfico png para visualizar el árbol

 crear arrays de entrenamiento y las etiquetas que indican si llegó a top o no

ytrain artistsencodedtop
xtrain artistsencodeddrop top axisvalues

 crear arbol de decision con profundidad 

decisiontree treedecisiontreeclassifiercriterionentropy
minsamplessplit
minsamplesleaf
maxdepth 
classweight

decisiontreefitxtrain ytrain

 exportar el modelo a archivo dot
with openrtreedot w as f
f treeexportgraphvizdecisiontree
outfilef
maxdepth 
impurity true
featurenames listartistsencodeddrop top ax
xis
classnames no m billboard
rounded true
filled true 

 convertir el archivo dot a png para poder visualizarlo
checkcal dottpng r treedotrtreepng
pimage treepng

arbol de decisión 

artistas al nro de billboard genreencod
entrof

guralionencoded artitnpoencoded 
ntrop 

entropy 
samples 
valve 

elass n biliboard

al fin nuestro preciado árbol aparece en pantalla ahora podríamos ver si lo podemos mejorar por
ejemplo tuneando los parámetros de entrada

as 
 

moodencoded 
w ntropy w

entropy 

samples 

value 
class n billboard

análisis del árbol

en la gráfica vemos un nodo raíz que hace una primer subdivisión por género y las salidas van
a izquierda por true que sea menor a es decir los géneros y eran los que menos top
tenían y a derecha en false van los géneros y que eran pop y urban con gran cantidad de usuarios
top billboard en el segundo nivel vemos que la cantidad de muestras samples queda repartida en
 y respectivamente a medida que bajamos de nivel veremos que los valores de entropía se
aproximan más a cuando el nodo tiene más muestras top azul y se acercan a cuando hay
mayoría de muestras top naranja en los diversos niveles veremos divisiones por tipo de artista
 edad duración y mood también vemos algunas hojas naranjas que finalizan antes de llegar al
último nivel esto es porque alcanzan un nivel de entropía cero o porque quedan con una cantidad
de muestras menor a nuestro mínimo permitido para hacer split veamos cuál fue la precisión
alcanzada por nuestro árbol

accdecisiontree round decisiontreescorextrain ytrain 
printaccdecisiontree

nos da un valor de notamos que casi todas las hojas finales del árbol tienen samples
mezclados sobre todo en los de salida para clasificar los top esto hace que se reduzca el score
pongamos a prueba nuestro algoritmo

s

 oo aa

 s

 oo o

arbol de decisión 

predicción de canciones al billboard 

vamos a testear nuestro árbol con artistas que entraron al billboard en camila cabello
que llegó al numero con la canción havana y imagine dragons con su canción believer que
alcanzó un puesto pero no llegó a la cima

predecir artista camila cabello featuring young thug
 con su canción havana llego a numero billboard us en 

xtest pddataframecolumns top moodencoded tempoencoded genreencoded x
artisttypeencoded edadencoded durationencoded 

xtestloc 

y pred decisiontreepredictxtestdrop top axis 

printprediccion strypred

y proba decisiontreepredictprobaxtestdrop top axis 
printprobabilidad de acierto strroundyproba ypred 

nos da que havana llegará al top con una probabilidad del nada mal

predecir artista imagine dragons
 con su canción believer llego al puesto billboard us en 

xtest pddataframecolumns top moodencoded tempoencoded genreencoded x
artisttypeencoded edadencoded durationencoded 

xtestloc 

y pred decisiontreepredictxtestdrop top axis 

printprediccion strypred

y proba decisiontreepredictprobaxtestdrop top axis 
printprobabilidad de acierto strroundyproba ypred 

nos da que la canción de imagine dragons no llegará con una certeza del otro acierto veamos
los caminos tomados por cada una de las canciones

httpswwwbillboardcommusiccamilacabello
httpswwwyoutubecomwatchvbqomxqxmlsk
httpswwwbillboardcommusicimaginedragons
httpswwwyoutubecomwatchvwtfhzwyrcc

arbol de decisión

arttypoencodad 
u 

value 
class n biliboard



m

bibosrd

aqui vemos los caminos tomados por havana en rojo que alcanzó el número y el camino por

believer en rosa que no llegó

resumen

anduvimos un largo camino para poder crear y generar nuestro árbol hemos revisado los datos
de entrada los hemos procesado los pasamos a valores categóricos y generamos el árbol lo hemos
puesto a prueba para validarlo obtener un score de menos de en el árbol no es un valor muy
alto pero tengamos en cuenta que nos pusimos una tarea bastante difícil de lograr poder predecir
al número del billboard y con un tamaño de muestras pequeño registros y desbalanceado

ya quisieran las discográficas poder hacerlo 

recursos y enlaces

 descarga la jupyter notebook y el archivo de entrada csv
 ó puedes visualizar online
 over y descargar desde github

otros enlaces con artículos sobre decisión tree en inglés

introductiontodecisiontreestitanicdataset
 decision trees in python

decision trees with scikit learn

building decision tree algorithm

httpwwwaprendemachinelearningcomclasificacioncondatos desbalanceados
ohttpwwwaprendemachinelearningcomwpcontentuploadsejercicioarboldedecisionipynb
httpwwwaprendemachinelearningcomwpcontentuploadsartistsbillboardfixcsv
httpsgithubcomjbagnatomachinelearningblobmasterejercicioarboldedecisionipynb
httpsgithubcomjbagnatomachinelearning

httpswww kagglecomdmillaintroductiontodecisiontreestitanic datasetnotebook
httpstackabusecomdecisiontreesinpythonwithscikitlearn
httpsnapitupulujonappspotcompostsdecisiontreeudhtml
httpdataaspirantcomdecisiontreealgorithmpythonwithscikitlearn

índice general

resumen emedrererderverdederverve verdevedederererer 
breve historia de las redes neuronales artificiales ee 
arquitecturas y aplicaciones de las redes neuronales qep 
evolución de las redes neuronales en ciencias de la computación 
el inicio de todo la neurona artificial 
los s aprendizaje automático a 
se alcanza el deep learning a 
resumen emedrererderverdederverve verdevedederererer 
aprendizaje profundo una guía rápida 
deep learning y redes neuronales sin código 
cómo funciona el deep learning mejor un ejemplo 
creamos una red neuronal esrierier eee 
cómo se calcula la predicción e 
entrenando nuestra red neuronal eeemerere re e 
cómo reducimos la función coste y mejoramos las predicciones 
resumen emedrererderverdederverve verdevedederererer 
crear una red neuronal en python desde cero 
el proyecto eae nn nn 
funciones sigmoide r nn 
forward propagation ó red feedforward s 
backpropagation cómputo del gradiente 
el código de la red neuronal e 
resumen emedrererderverdederverve verdevedederererer 
programa un coche robot arduino que conduce con ia 
la nueva red neuronal eeieriee re eeeeee 
el coche arduino i 
circuito del coche 
montar el cocheesi ei eie eee 
copiar la red neuronal e 
el código arduino ae 
el coche en acción eee eenee 
resumen emedrererderverdederverve verdevedederererer 
una sencilla red neuronal con keras y tensorflow eee 
requerimientos para el ejercicio aa 
las compuertas xor 
una red neuronal artificial sencilla con python y keras 
analicemos la red neuronal que hicimos 
visualización de la red neuronal eerere eee 

a entrenar la red aaa oo ooerreo

qué es overfitting y cómo
solucionario

las principales causas al obtener malos resultados en machine learning son el overfitting o el

underfitting de los datos cuando entrenamos nuestro modelo intentamos hacer encajar fit
en inglés los datos de entrada entre ellos y con la salida tal vez se pueda traducir overfitting
como sobreajuste y underfitting como subajuste y hacen referencia al fallo de nuestro modelo al
generalizar el conocimiento que pretendemos que adquieran lo explicaré a continuación con un
ejemplo

generalización del conocimiento

como si se tratase de un ser humano las máquinas de aprendizaje deberán ser capaces de generalizar
conceptos supongamos que vemos un perro labrador por primera vez en la vida y nos dicen eso
es un perro luego nos enseñan un caniche y nos preguntan eso es un perro diremos no pues
no se parece en nada a lo que aprendimos anteriormente ahora imaginemos que nuestro tutor nos
muestra un libro con fotos de razas de perros distintas cuando veamos una raza de perro que
desconocíamos seguramente seremos capaces de reconocer al cuadrúpedo canino al tiempo de poder
discernir en que un gato no es un perro aunque sea peludo y tenga patas cuando entrenamos
nuestros modelos computacionales con un conjunto de datos de entrada estamos haciendo que el
algoritmo sea capaz de generalizar un concepto para que al consultarle por un nuevo conjunto
de datos desconocido éste sea capaz de sintetizarlo comprenderlo y devolvernos un resultado fiable
dada su capacidad de generalización

el problema de la máquina al generalizar

si nuestros datos de entrenamiento son muy pocos nuestra máquina no será capaz de generalizar
el conocimiento y estará incurriendo en underfitting este es el caso en el que le enseñamos sólo
una raza de perros y pretendemos que pueda reconocer a otras razas de perros distintas el
algoritmo no será capaz de darnos un resultado bueno por falta de materia prima para hacer sólido
su conocimiento también es ejemplo de subajuste cuando la máquina reconoce todo lo que ve
como un perro tanto una foto de un gato o un coche por el contrario si entrenamos a nuestra
máquina con razas de perros sólo de color marrón de manera rigurosa y luego enseñamos
una foto de un perro blanco nuestro modelo no podrá reconocerlo cómo perro por no cumplir

 ttpwwwaprendemachinelearningcomqueesmachinelearning
ttpwwwaprendemachinelearningcompasosmachinelearningconstruirmaquina

qué es overfitting y cómo solucionarlo 

exactamente con las características que aprendió el color forzosamente debía ser marrón aquí se
trata de un problema de overfitting tanto el problema del ajuste por debajo como por encima
de los datos son malos porque no permiten que nuestra máquina generalice el conocimiento y no
nos dará buenas predicciones

overfitting en machine learning

es muy común que al comenzar a aprender machine learning caigamos en el problema del overfit
ting lo que ocurrirá es que nuestra máquina sólo se ajustará a aprender los casos particulares que
le enseñamos y será incapaz de reconocer nuevos datos de entrada en nuestro conjunto de datos
de entrada muchas veces introducimos muestras atípicas anomalas o con ruidodistorción en
alguna de sus dimensiones o muestras que pueden no ser del todo representativas cuando sobre
entrenamos nuestro modelo y caemos en el overfitting nuestro algoritmo estará considerando como
válidos sólo los datos idénticos a los de nuestro conjunto de entrenamiento incluidos sus defectos
y siendo incapaz de distinguir entradas buenas como fiables si se salen un poco de los rangos ya
prestablecidos

el equilibrio del aprendizaje

underfitting correcto overfitting

deberemos encontrar un punto medio en el aprendizaje de nuestro modelo en el que no estemos
incurriendo en underfitting y tampoco en overfitting a veces esto puede resultar una tarea muy
difícil para reconocer este problema deberemos subvididir nuestro conjunto de datos de entrada
para entrenamiento en dos uno para entrenamiento y otro para test que el modelo no conocerá
de antemano esta división se suele hacer del para entrenar y el conjunto de test deberá
tener muestras diversas en lo posible y una cantidad de muestras suficiente para poder comprobar
los resultados una vez entrenado el modelo

qué es overfitting y cómo solucionarlo 

número total de muestras

datos de entrenamiento validación

cuando entrenamos nuestro modelo solemos parametrizar y limitar el algoritmo por ejemplo la
cantidad de iteraciones que tendrá o un valor de tasa de aprendizaje learningrate por iteración y
muchos otros para lograr que nuestro modelo dé buenos resultados iremos revisando y contrastando
nuestro entrenamiento con el conjunto de test y su tasa de errores utilizando más o menos
iteraciones etc hasta dar con buenas predicciones y sin tener los problemas de overunderfitting

prevenir el sobreajuste de datos

para intentar que estos problemas nos afecten lo menos posible podemos llevar a cabo diversas
acciones

 cantidad mínima de muestras tanto para entrenar el modelo como para validarlo

 clases variadas y equilibradas en cantidad en caso de aprendizaje supervisado y suponien
do que tenemos que clasificar diversas clases o categorías es importante que los datos de
entrenamiento estén balanceados supongamos que tenemos que diferenciar entre manzanas
peras y bananas debemos tener muchas fotos de las frutas y en cantidades similares si
tenemos muy pocas fotos de peras esto afectará en el aprendizaje de nuestro algoritmo para
identificar esa fruta

 conjunto de test de datos siempre subdividir nuestro conjunto de datos y mantener una
porción del mismo oculto a nuestra máquina entrenada esto nos permitirá obtener una
valoración de aciertosfallos real del modelo y también nos permitirá detectar fácilmente
efectos del overfitting underfitting

 parameter tunning o ajuste de parámetros deberemos experimentar sobre todo dando
másmenos tiempoiteraciones al entrenamiento y su aprendizaje hasta encontrar el equili
brio

 httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado
httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning
httpwwwaprendemachinelearningcomregresionlogisticaconpythonpasoapaso

s httpwwwaprendemachinelearningcomclasificacioncondatos desbalanceados

 httpwwwaprendemachinelearningcompasosmachinelearning construirmaquinaparametertuning

qué es overfitting y cómo solucionarlo 

 cantidad excesiva de dimensiones features con muchas variantes distintas sin suficientes
muestras a veces conviene eliminar o reducir la cantidad de características que utilizaremos
para entrenar el modelo una herramienta útil para hacerlo es pca

 quiero notar que si nuestro modelo es una red neuronal artificial deep learning podemos
caer en overfitting si usamos capas ocultas en exceso ya que haríamos que el modelo
memorice las posibles salidas en vez de ser flexible y adecuar las activaciones a las entradas
nuevas

si el modelo entrenado con el conjunto de train tiene un de aciertos y con el conjunto de test
tiene un porcentaje muy bajo esto señala claramente un problema de overfitting si en el conjunto
de test sólo se acierta un tipo de clase por ejemplo peras o el único resultado que se obtiene es
siempre el mismo valor será que se produjo un problema de underfitting

resumen

siempre que creamos una máquina de aprendizaje deberemos tener en cuenta que pueden caer en
uno de estos problemas por no poder generalizar correctamente el conocimiento underfitting
indicará la imposibilidad de identificar o de obtener resultados correctos por carecer de suficientes
muestras de entrenamiento o un entrenamiento muy pobre overfitting indicará un aprendizaje
excesivo del conjunto de datos de entrenamiento haciendo que nuestro modelo únicamente pueda
producir unos resultados singulares y con la imposibilidad de comprender nuevos datos de entrada

shttpwwwaprendemachinelearningcomcomprendeprincipalcomponentanalysis
 httpwwwaprendemachinelearningcombrevehistoriadelasredesneuronalesartificiales
httpwwwaprendemachinelearningcomaprendizajeprofundounaguiarapida

datos desbalanceados

veremos qué son y cómo contrarrestar problemas con clases desbalanceadas

estrategias para resolver desequilibrio de datos en python con la librería imbalancedlearn

agenda

qué son las clases desequilibradas en un dataset
métricas y confusión matrix

ejercicio con python

estrategias

modelo sin modificar

penalización para compensar métricas
resampling y muestras sintéticas

omapasna

 subsampling

 oversamplig

 combinación

 balanced ensemble

 e

empecemos

problemas de clasificación con clases desequilibradas

en los problemas de clasificación en donde tenemos que etiquetar por ejemplo entre spam o
not spam ó entre múltiples categorías coche barco avión solemos encontrar que en nuestro
conjunto de datos de entrenamiento contamos con que alguna de las clases de muestra es una clase
minoritaria es decir de la cual tenemos muy poquitas muestras esto provoca un desbalanceo en
los datos que utilizaremos para el entrenamiento de nuestra máquina

un caso evidente es en el área de salud en donde solemos encontrar conjuntos de datos con miles de
registros con pacientes negativos y unos pocos casos positivos es decir que padecen la enfermedad
que queremos clasificar

otros ejemplos suelen ser los de detección de fraude donde tenemos muchas muestras de clientes
honestos y pocos casos etiquetados como fraudulentos ó en un funnel de marketing en donde
por lo general tenemos un de los datos de clientes que compran ó ejecutan algún tipo de acción
cta que queremos predecir

 httpsimbalancedlearnreadthedocsioenstable
httpswwwaprendemachinelearningcomaplicacionesdelmachinelearning

datos desbalanceados 

cómo nos afectan los datos desbalanceados

por lo general afecta a los algoritmos en su proceso de generalización de la información y
perjudicando a las clases minoritarias esto suena bastante razonable si a una red neuronal le damos
 de fotos de gatitos y sólo de perros no podemos pretender que logre diferenciar una clase de
otra lo más probable que la red se limite a responder siempre tu foto es un gato puesto que así
tuvo un acierto del en su fase de entrenamiento

métricas y confusion matrix

como decía si medimos la efectividad de nuestro modelo por la cantidad de aciertos que tuvo sólo
teniendo en cuenta a la clase mayoritaria podemos estar teniendo una falsa sensación de que el
modelo funciona bien

para poder entender esto un poco mejor utilizaremos la llamada confusión matrix que nos
ayudará a comprender las salidas de nuestra máquina

datos desbalanceados 

predicción predicción
clase clase 
valor real
clase 
valor real
clase 

clase 

precisión


clase 
e

precisión 



y de aqui salen nuevas métricas precisión y recall

veamos la confusion matrix con el ejemplo de las predicciones de perro y gato

httpwwwaprendemachinelearningcomwpcontentuploadsconfusionmatixexamplepng

datos desbalanceados 

predicción predicción
gato perro
valor real
gato
valor real
perro
precisión recall 
clase clase y
 
accuracy
 precisión recall 
clase clase 
 a 

breve explicación de estás métricas

la accuracy del modelo es básicamente el numero total de predicciones correctas dividido por el
número total de predicciones en este caso da cuando no hemos logrado identificar ningún
perro

la precisión de una clase define cuan confiable es un modelo en responder si un punto pertenece a
esa clase para la clase gato será del sin embargo para la de perro será 

el recall de una clase expresa cuan bien puede el modelo detectar a esa clase para gatos será de y
para perros 

el f score de una clase es dada por la media harmoníca de precisión y recall x precision x recall
 precisionrecall digamos que combina precisión y recall en una sola métrica en nuestro caso
daría cero para perros

tenemos cuatro casos posibles para cada clase

datos desbalanceados 

 alta precision y alto recall el modelo maneja perfectamente esa clase

 alta precision y bajo recall el modelo no detecta la clase muy bien pero cuando lo hace es
altamente confiable

 baja precisión y alto recall la clase detecta bien la clase pero también incluye muestras de
otras clases

 baja precisión y bajo recall el modelo no logra clasificar la clase correctamente

cuando tenemos un dataset con desequilibrio suele ocurrir que obtenemos un alto valor de
precisión en la clase mayoritaria y un bajo recall en la clase minoritaria

vamos al ejercicio con python

usaremos el set de datos credit card fraut detection de la web de kaggle son mb que al
descomprimir ocuparán mb usaremos el archivo creditcardcsv este dataset consta de 
filas con columnas features como la información es privada no sabemos realmente que
significan los features y están nombradas como v v v etc excepto por las columnas time y
amount el importe de la transacción y nuestras clases son y correspondiendo con transacción

normal ó hubo fraude como podrán imaginar el set de datos está muy desequilibrado y
tendremos muy pocas muestras etiquetadas como fraude

también debo decir que no nos centraremos tanto en la elección del modelo ni en su configura

 y tuneo si no que nos centraremos en aplicar las diversas estrategias para mejorar los
resultados a pesar del desequilibrio de clases

ción

instala la librería de imbalanced learn desde linea de comando con toda la documentación en la
web oficial imblearn

pip install u imbalancedlearn

veamos el dataset

análisis exploratorio

haremos eda para comprobar el desequilibrio entre las clases

httpswwwkagglecommlgulbcreditcardfrauddata
httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning
httpwwwaprendemachinelearningcompasosmachinelearningconstruirmaquina

 httpwwwaprendemachinelearningcomconsejosutilesparaaplicarmachinelearning
httpsimbalancedlearnreadthedocsioenstable

o ia ek an 

d aa ha ha a aa
 oo iduupraon o o

datos desbalanceados 

import pandas as pd

import numpy as np

import matplotlibpyplot as plt
import seaborn as sns

from sklearnmetrics import confusionmatrix

from sklearnmetrics import classificationreport
from sklearnmodelselection import traintestsplit
from sklearnlinearmodel import logisticregression
from sklearndecomposition import pca

from sklearntree import decisiontreeclassifier

from pylab import rcparams

from imblearnundersampling import nearmiss

from imblearnoversampling import randomoversampler
from imblearncombine import smotetomek

from imblearnensemble import balancedbaggingclassifier

from collections import counter

luego de importar las librerías que usaremos cargamos con pandas el dataframe y vemos las
primeras filas

df pdreadcsvcreditcardcsv read in data downloaded to the local directory
dfheadn

tm vv ve n w w ve vi eo
 n doza zna inz qmzion angn comdno di ooo lnon aoi
 d nigies ozgion dicuabo damuigo oconis omig qutaec domic dza dz mmet anooo tna
 sonion nodiro oo dao isna oiatdo ojtigto opoiz iginen
 osona isza iuemo meo dcicios anooo iem imniz ioo ocoita na 
 oatti stio oacko dantios qdbsmlt osd dzisii daviio ogowmi dtto inzo
 rows x columns 

veamos de cuantas filas tenemos y cuantas hay de cada clase

printdfshape
printpdvaluecountsdfclass sort true

 shttpwwwaprendemachinelearningcomwpcontentuploadsimbalancedataframepng

índice general

resultados del entrenamiento a 
evaluamos y predecimos aa 
afinando parámetros de la red neuronal s 
guardar la red y usarla de verdad e 
vale la pena una red neuronal ee 
resumen ea aa eeee oeec 
pronóstico de series temporales con redes neuronales 
qué es una serie temporal y qué tiene de especial 
cargar el ejemplo con pandas a 
visualización de datos 
cómo hacer pronóstico de series temporales qq 
pronóstico de ventas diarias con redes neuronal 
creamos la red neuronal artificial 
entrenamiento y resultados 
pronóstico de ventas futuras 
resumen ea aa eeee oeec 
pronóstico de ventas con redes neuronales parte 
mejora del modelo de series temporales con múltiples variables y embeddings 
mejoras al modelo de series temporales a 
primer mejora serie temporal de múltilples variables 
fecha como variable de entrada 
segunda mejora embeddings en variables categóricas 
qué son los embeddings 
quiero python aec 
comparemos los resultados de los modelos 
resumen ea aa eeee oeec 
crea tu propio servicio de machine learning con flask 
implementar modelos de machine leamning rre 
servir mediante una api aa 
instalar flask 
crear el modelo de ml 
guardar el modelo serialización de objetos en python 
crear una api con flask 
actualizar el modelo según sea necesario 
resumen ea aa eeee oeec 
clasificación de imágenes en python 
ejercicio clasificar imágenes de deportes 
vamos al código python 
 importar librerías 

cargar lasimágenes seieeeeneneoeve

a o n 

a a r qv n 

datos desbalanceados 

 
o 
 

name class dtype int

vemos que son filas y solamente son la clase minoritaria con los casos de fraude
representan el de las muestras

countclasses pdvaluecountsdfclass sort true
countclasses plotkind bar rot
pltxticksrange labels

plttitlefrequency by observation number
pltxlabelclass

pltylabel number of observations

frequency by observation number








number of observations

o
normal fraud

class



llegas a ver la mínima linea roja que representa los casos de fraude son muy pocas muestras

httpwwwaprendemachinelearningcomwpcontentuploadsimbalancecardvisualizationpng

datos desbalanceados 

estrategias para el manejo de datos desbalanceados

tenemos diversas estrategias para tratar de mejorar la situación las comentaremos brevemente y
pasaremos a la acción al código a continuación

 ajuste de parámetros del modelo consiste en ajustar parametros ó metricas del propio
algoritmo para intentar equilibrar a la clase minoritaria penalizando a la clase mayoritaria
durante el entrenamiento ejemplos on ajuste de peso en árboles también en logisticregression
tenemos el parámetro classweight balanced que utilizaremos en este ejemplo no todos los
algoritmos tienen estas posibilidades en redes neuronales por ejemplo podríamos ajustar la
métrica de loss para que penalice a las clases mayoritarias

 modificar el dataset podemos eliminar muestras de la clase mayoritaria para reducirlo e
intentar equilibrar la situación tiene como peligroso que podemos prescindir de muestras
importantes que brindan información y por lo tanto empeorar el modelo entonces para
seleccionar qué muestras eliminar deberíamos seguir algún criterio también podríamos
agregar nuevas filas con los mismos valores de las clases minoritarias por ejemplo cuadriplicar
nuestras filas pero esto no sirve demasiado y podemos llevar al modelo a caer en overfitting

 muestras artificiales podemos intentar crear muestras sintéticas no idénticas utilizando
diversos algoritmos que intentan seguir la tendencia del grupo minoritario según el método
podemos mejorar los resultados lo peligroso de crear muestras sintéticas es que podemos
alterar la distribución natural de esa clase y confundir al modelo en su clasificación

 balanced ensemble methods utiliza las ventajas de hacer ensamble de métodos es decir
entrenar diversos modelos y entre todos obtener el resultado final por ejemplo votando
pero se asegura de tomar muestras de entrenamiento equilibradas

apliquemos estas técnicas de a una a nuestro código y veamos los resultados

pero antes de empezar ejecutaremos el modelo de regresión logística desequilibrado para

tener un baseline es decir unas métricas contra las cuales podremos comparar y ver si mejoramos

probando el modelo sin estrategias

httpwwwaprendemachinelearningcomregresionlogisticaconpythonpasoapaso

o ia ek an 

v n lreleenerennhessehhaarrr
d o oiapean ogogpoiaod e w noo

datos desbalanceados

definimos nuestras etiquetas y features
y dfclass

x dfdrop class axis

dividimos en sets de entrenamiento y test

xtrain x test ytrain y test traintestsplitx y trainsize

creamos una función que crea el modelo que usaremos cada vez

def runmodelxtrain x test ytrain y test



clfbase logisticregressionc penalty randomstate solvernewtoncx


clfbasefitx train ytrain
return clfbase

ejecutamos el modelo tal cual
model runmodelxtrain x test ytrain ytest

definimos funciona para mostrar los resultados

def mostrarresultadosytest pred y
confmatrix confusion matrixytest pred y
plt figure figsize 

snsheatmapconfmatrix xticklabelslabels yticklabelslabels annottrue fmty

d
plttitleconfusion matrix
pltylabel true class
pltxlabelpredicted class
pltshow
print classification reportytest predy

predy model predictxtest
mostrarresultadosytest predy

datos desbalanceados 

confusion matrix

yrmai

true class


 
norma fraud
predicted class
precision recall flscore support
o 
 
accuracy 
macro avg 
weighted avg 



aqui vemos la confusion matrix y en la clase es lo que nos interesa detectar vemos fallos y 
aciertos dando un recall de y es el valor que queremos mejorar también es interesante
notar que en la columna de fscore obtenemos muy buenos resultados pero que realmente no nos
deben engañar pues están reflejando una realidad parcial lo cierto es que nuestro modelo no es
capaz de detectar correctamente los casos de fraude

estrategia penalización para compensar

utilizaremos un parámetro adicional en el modelo de regresión logística en donde indicamos class
weight balanced y con esto el algoritmo se encargará de equilibrar a la clase minoritaria durante
el entrenamiento veamos

 httpwwwaprendemachinelearningcomwpcontentuploadsconfusnormalpng

n

a



datos desbalanceados

def runmodelbalancedxtrain yx test y train ytest

clf logisticregressioncpenalty randomstatesolvernewtoncgclxn

assweightbalanced
clffitxtrain ytrain
return clf

model runmodelbalancedxtrain x test y train ytest

predy model predictxtest
mostrarresultadosytest predy

confusion matrix

norma

rue class

fraud

norma fraud

predicted class
precision recall flscore
o 
 
accuracy 
macro avg 
weighted avg 

support




















ahora vemos una notable mejora en la clase que indica si hubo fraude se han acertado
 muestras y fallado en dando un recall de y sólo con agregar un parámetro al
modelo también notemos que en la columna de fscore parecería que hubieran empeorado
los resultados cuando realmente estamos mejorando la detección de casos fraudulentos es cierto
que aumentan los falsos positivos y se han etiquetado muestras como fraudulentas cuando no
lo eran pero ustedes piensen qué prefiere la compañía bancaria tener que revisar esos casos
manualmente ó fallar en detectar los verdaderos casos de fraude

sohttpwwwaprendemachinelearningcomwpcontentuploadsconfusbalancedpng

d



l

datos desbalanceados 

sigamos con más métodos

estrategia subsampling en la clase mayoritaria

lo que haremos es utilizar un algoritmo para reducir la clase mayoritaria lo haremos usando un
algoritmo que hace similar al knearest neighbor para ir seleccionando cuales eliminar fijemonos
que reducimos bestialmente de muestras de clase cero la mayoría y pasan a ser y con
esas muestras entrenamos el modelo

us nearmissratio nneighbors version randomstate
xtrainres ytrainres us fitsamplextrain ytrain

print distribution before resampling formatcounterytrain
print distribution after resampling formatcounterytrainres

model runmodelxtrainres xtest ytrainres ytest
predy model predictxtest
mostrarresultadosytest predy

distribution before resampling counter 
distribution after resampling counter

datos desbalanceados

confusion matrix

normal

rue class

fraud

normal fraud
predicted class

precision recall flscore

o 

 

accuracy 
macro avg 
weighted avg 

 i 

support




















también vemos que obtenemos muy buen resultado con recall de aunque a costa de que

aumentaran los falsos positivos

estrategia oversampling de la clase minoritaria

en este caso crearemos muestras nuevas sintéticas de la clase minoritaria usando randomover
sampler y vemos que pasamos de muestras de fraudes a 

 httpwwwaprendemachinelearningcomwpcontentuploadsconfussubsamplingpng

n

a

datos desbalanceados 

os randomoversamplerratio

xtrainres ytrainres osfitsamplextrain ytrain

print distribution before resampling formatcounterytrain
print distribution labels after resampling formatcounterytrainres

model runmodelxtrainres xtest ytrainres ytest
predy model predictxtest
mostrarresultadosytest predy

distribution before resampling counter 
distribution after resampling counter 

confusion matrix

 

normal


ea l 
y
el

 
normai fraud
predicted class
precision recall flscore support
o 
 
accuracy 
macro avg 
weighted avg 



tenemos un de recall para la clase y los falsos positivos son nada mal

httpwwwaprendemachinelearningcomwpcontentuploadsconfusoversamplingpng

s s

o 

datos desbalanceados 

estrategia combinamos resampling con smotetomek

ahora probaremos una técnica muy usada que consiste en aplicar en simultáneo un algoritmo
de subsampling y otro de oversampling a la vez al dataset en este caso usaremos smote para
oversampling busca puntos vecinos cercanos y agrega puntos en linea recta entre ellos y usaremos
tomek para undersampling que quita los de distinta clase que sean nearest neighbor y deja ver mejor
el decisión boundary la zona limítrofe de nuestras clases

osus smotetomekratio
xtrainres ytrainres osus fitsamplextrain ytrain

print distribution before resampling formatcounterytrain
print distribution after resampling formatcounterytrainres

model runmodelxtrainres xtest ytrainres ytest
predy model predictxtest

mostrarresultadosytest predy

distribution labels before resampling counter 
distribution after resampling counter

datos desbalanceados 

confusion matrix



normai

true

fraud

normal fraud
predicted class

precision recall flscore support
o 
 
accuracy 
macro avg 
weighted avg 



en este caso seguimos teniendo bastante buen recall de la clase y vemos que los falsos positivos
de la clase son bastante pocos de muestras

estrategia ensamble de modelos con balanceo

para esta estrategia usaremos un clasificador de ensamble que utiliza bagging y el modelo estimador
será un decisiontree veamos como se comporta

 httpwwwaprendemachinelearningcomwpcontentuploadssmotetomekpng

