aprende
machine
learning

escrito por
juan ignacio bagnato

basado en
el contenido del blog

aprende machine learning en español
teoría práctica python

juan ignacio bagnato
este libro está a la venta en httpleanpubcomaprendeml

esta versión se publicó en 

la

leanpub

éste es un libro de leanpub leanpub anima a los autores y publicadoras con el proceso de
publicación lean publishing es el acto de publicar un libro en progreso usando herramientas
sencillas y muchas iteraciones para obtener retroalimentación del lector hasta conseguir el libro
adecuado

o juan ignacio bagnato

índice general

 crear etiquetas y clases ea 
creamos sets de entrenamiento y test validación y preprocesar 
 creamos la red aquí la magia 
entrenamos la cnn dedier eee eee 
resultados de la clasificación 
resumen emedrererderverdederverve verdevedederererer 
cómo funcionan las convolutional neural networks qoqer 
muchas imágenes r a anene nn 
pixeles y neuronas e aa ede nnc 
convoluciones oedeaerede eee ereerecveeveee 
filtro conjunto de kernels 
la función de activación eeiiee eee 
subsampling eeec 
ya terminamos no ahora más convoluciones 
conectar con una red neuronal tradicional 
y cómo aprendió la cnn a ver backpropagation e 
comparativa entre una red neuronal tradicional y una cnn e 
arquitectura básica eñeere ene nn 
resumen emedrererderverdederverve verdevedederererer 
detección de objetos con python s 
en qué consiste la detección yolo a 
el proyecto propuesto detectar personajes de lego 
crea un dataset imágenes y anotaciones e a 
el lego dataset 
el código python as 
leer el dataset i eee eee 
train y validación 
data augmentation r eee 
crear la red de clasificación 
crear la red de detección i re e e 
generar las anclas ei i eee eee 
entrenar la red eee 
revisar los resultados emrriee eee 
probar la red i eee eee 
resumen emedrererderverdederverve verdevedederererer 
anexo i webscraping ere eee oeeeere 
ejemplo web scraping en python ibex la bolsa de madrid 
requerimientos 
conocimientos básicos de html y css 
inspección manual de la web 

código webscraping python

n

a



datos desbalanceados

bbc balancedbaggingclassifierbaseestimatordecisiontreeclassifier

samplingstrategyauto
replacementfalse

randomstate

train the classifier
bbcfitxtrain ytrain

predy bbc predictxtest
mostrarresultadosytest predy

confusion matrix

normal

fraud

normal fraud
predicted class

precision recall flscore

o 

 

accuracy 
macro avg 
weighted avg 

tampoco está mal vemos siempre mejora con respecto al modelo inicial con un recall de para

los casos de fraude

support












resultados de las estrategias

veamos en una tabla ordenada de mejor a peor los resultados obtenidos

d httpwwwaprendemachinelearningcomwpcontentuploadsbalanceensemblepng

datos desbalanceados 

algorithm precision recall overall

 penalizacion 
 nearmiss subsampling 
 random oversampling 
 ensemble 
 smote tomek 
o regresion logística 



en nuestro caso las estrategias de penalización y subsampling nos dan el mejor resultado cada
una con un recall de 

pero quedémonos con esto con cualquiera de las técnicas que aplicamos mejoramos el
modelo inicial de regresión logística que lograba un de recall para la clase de fraude y
no olvidemos que hay un tremendo desbalance de clases en el dataset

importante esto no quiere decir que siempre hay que aplicar penalización ó nearmiss subsam
pling dependerá del caso del desbalanceo y del modelo en este caso usamos regresión logística
pero podría ser otro

resumen

es muy frecuente encontrarnos con datasets con clases desbalanceadas de hecho lo más raro sería
encontrar datasets bien equilibrados

siempre que puedas sal a la calle y consigue más muestras de la clase minoritaria pero la

realidad es que a veces no es posible conseguir más datos de esas clases como por ejemplo en casos
de salud

vimos diversas estrategias a seguir para combatir esta problemática eliminar muestras del set
mayoritario crear muestras sintéticas con algún criterio ensamble y penalización

además revisamos la matriz de confusión y comprendimos que las métricas pueden ser
engañosas si miramos a nuestros aciertos únicamente puede que pensemos que tenemos un buen
clasificador cuando realmente está fallando

httpwwwaprendemachinelearningcomwpcontentuploadsimbalanceresultpng

datos desbalanceados

recursos

 notebook con todo el ejercicio y más cositas

 descarga el dataset desde kaggle
 librería de imbalancedlearn

enlaces de interés

 tactics to combat imbalanced classes

 how to fix unbalanced dataset

httpsgithubcomjbagnatomachinelearningblobmasterejercicioimbalanceddataipynb

httpswww kagglecommlgulbcreditcardfrauddata

httpsimbalancedlearnreadthedocsioenstable

 httpsmachinelearningmasterycomtacticstocombatimbalancedclassesinyourmachinelearning dataset
httpswwwwkdnuggetscomfixunbalanced datasethtml

random forest el poder del
ensamble

luego del algoritmo de árbol de decisión tu próximo paso es el de estudiar random forest
comprende qué és y cómo funciona con un ejemplo práctico en python

random forest es un tipo de ensamble en machine learning en donde combinaremos diversos
árboles ya veremos cómo y con qué características y la salida de cada uno se contará como un
voto y la opción más votada será la respuesta del bosque aleatorio

random forest al igual que el árbol e decisión es un modelo de aprendizaje supervisado

clasificación aunque también puede usarse para problemas de regresión

para

cómo surge random forest

uno de los problemas que aparecía con la creación de un árbol de decisión es que si le damos
la profundidad suficiente el árbol tiende a memorizar las soluciones en vez de generalizar el
aprendizaje es decir a padecer de overfitting la solución para evitar esto es la de crear muchos
árboles y que trabajen en conjunto veamos cómo

cómo funciona random forest

random forest funciona así

 seleccionamos k features columnas de las m totales siendo k menor a m y creamos un árbol
de decisión con esas k características

 creamos n árboles variando siempre la cantidad de k features y también podríamos variar la
cantidad de muestras que pasamos a esos árboles esto es conocido como bootstrap sample

 tomamos cada uno de los n árboles y le pedimos que hagan una misma clasificación
guardamos el resultado de cada árbol obteniendo n salidas

 calculamos los votos obtenidos para cada clase seleccionada y consideraremos a la más
votada como la clasificación final de nuestro bosque

 httpwwwaprendemachinelearningcomarbolde decisionenpythonclasificacionyprediccion
httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning

s httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado

 httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning
shttpwwwaprendemachinelearningcomqueesoverfittingyunderfittingycomosolucionarlo

random forest el poder del ensamble 

por qué es aleatorio

contamos con una doble aleatoriedad tanto en la selección del valor k de características para cada
árbol como en la cantidad de muestras que usaremos para entrenar cada árbol creado

es curioso que para este algoritmo la aleatoriedad sea tan importante y de hecho es lo que lo hace
bueno pues le brinda flexibilidad suficiente como para poder obtener gran variedad de árboles y de
muestras que en su conjunto aparentemente caótico producen una salida concreta darwin estaría
orgulloso 

ventajas y desventajas del uso de random forest

vemos algunas de sus ventajas son

 funciona bien aún sin ajuste de hiperparámetros

 sirve para problemas de clasificación y también de regresión

 al utilizar múltiples árboles se reduce considerablemente el riesgo de overfiting

 se mantiene estable con nuevas muestras puesto que al utilizar cientos de árboles sigue
prevaleciendo el promedio de sus votaciones



y sus desventajas

 en algunos datos de entrada particulares random forest también puede caer en overfitting

 es mucho más costoso de crear y ejecutar que un sólo árbol de decisión

 puede requerir muchísimo tiempo de entrenamiento

 random forest no funciona bien con datasets pequeños

 es muy difícil poder interpretar los cientos de árboles creados en el bosque si quisiéramos
comprender y explicar a un cliente su comportamiento

vamos al código python

continuaremos con el ejercicio propuesto en el artículo desbalanceo de datos en donde
utilizamos el dataset de kaggle con información de fraude en tarjetas de crédito cuenta con 
filas y columnas de características nuestra salida será si es un cliente normal o si hizo uso
fraudulento

httpwwwaprendemachinelearningcompasosmachinelearningconstruirmaquina

 httpwwwaprendemachinelearningcomqueesoverfittingyunderfittingycomosolucionarlo
 httpwwwaprendemachinelearningcominterpretacionde modelosdemachinelearning

 httpwwwaprendemachinelearningcomclasificacioncondatos desbalanceados

random forest el poder del ensamble 

frequency by observation number








number of observations

o
normal fraud

class

llegas a ver la mínima linea roja que representa los casos de fraude son apenas frente a más
de casos de uso normal

retomaremos el mejor caso que obtuvimos en el ejercicio anterior utilizando regresión logística

y logrando un de aciertos pero recuerda también las métricas de f precisión y recall que eran
las que realmente nos ayudaban a validar el modelo

creamos el modelo y lo entrenamos

utilizaremos el modelo randomforrestclassifier de scikitlearn

 httpsenwikipediaorgwikilogisticregression

rn 

o 

random forest el poder del ensamble 

from sklearnensemble import randomforestclassifier

 crear el modelo con arboles

model randomforestclassifiernestimators
bootstrap true verbose
maxfeatures sgrt

 a entrenar

model fitxtrain ytrain

luego de unos minutos obtendremos el modelo entrenado en mi caso minuto segundos

los hiperparámetros más importantes

al momento de ajustar el modelo debemos tener en cuenta los siguientes hiperparámetros estos
nos ayudarán a que el bosque de mejores resultados para cada ejercicio recuerda que esto no se
trata de copiar y pegar

 nestimators será la cantidad de árboles que generaremos

 maxfeatures la manera de seleccionar la cantidad máxima de features para cada árbol

 minsampleleaf número mínimo de elementos en las hojas para permitir un nuevo split
división del nodo

 oobscore es un método que emula el crossvalidation en árboles y permite mejorar la
precisión y evitar overfitting

 boostrap para utilizar diversos tamaños de muestras para entrenar si se pone en falso utilizará
siempre el dataset completo

 njobs si tienes multiples cores en tu cpu puedes indicar cuantos puede usar el modelo al
entrenar para acelerar el entrenamiento

evaluamos resultados

veamos la matriz de confusión y las métricas sobre el conjunto de test no confundir con el de
training

random forest el poder del ensamble

normal

true class

fraud

u
normal

vemos muy buenos resultados clasificando con error apenas muestras

accuracy
macro avg
weighted avg

confusion matrix

predicted class

precision

recall

fraud

flscore








support

random forest el poder del ensamble 

aquí podemos destacar que para la clase minoritaria es decir la que detecta los casos de fraude
tenemos un buen valor de recall de lo cual es un buen indicador y el fscore macro avg es de
 logramos construir un modelo de bosque aleatorio que a pesar de tener un conjunto de datos
de entrada muy desigual logra buenos resultados

comparamos con el baseline

si comparamos estos resultados con los del algoritmo de regresión logística

random forest nos dio mejores clasificaciones menos falsos positivos
general

 vemos que el
y mejores métricas en

resumen

avanzando en nuestro aprendizaje sobre diversos modelos que podemos aplicar a las problemáticas
que nos enfrentamos hoy sumamos a nuestro kit de herramientas el random forest vemos que
es un modelo sencillo bastante rápido y si bien perdemos la interpretabilidad maravillosa que nos
brindaba sólo árbol de decisión es el precio a pagar para evitar el overfitting y para ganar un
clasificador más robusto

los algoritmos treebased en inglés son muchos todos parten de la idea principal de árbol
de decisión y la mejoran con diferentes tipos de ensambles y técnicas tenemos que destacar a
 modelos que según el caso logran superar a las mismísimas redes neuronales son xgboost y
lightgbm

recursos y adicionales

puedes descargar la notebook para este ejercicio desde mi cuenta de github

 código en jupyter notebook en github
 dataset de kaggle

otros artículos sobre random forest en inglés

 random forest simple explanation
 an implementation of random forest in python

 httpwwwaprendemachinelearningcomregresionlogisticaconpythonpasoapaso
httpwwwaprendemachinelearningcomclasificacioncondatos desbalanceados
httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning

 httpwwwaprendemachinelearningcominterpretacion de modelosde machinelearning
httpwwwaprendemachinelearningcomqueesoverfittingyunderfittingycomosolucionarlo
dhttpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning
httpwwwaprendemachinelearningcomarbolde decisionenpythonclasificacionyprediccion

 httpwwwaprendemachinelearningcomunasencillaredneuronalenpythonconkerasytensorflow
 httpsgithubcomjbagnatomachinelearningblobmasterejerciciorandomforestipynb
ohttpswwwkagglecommlgulbcreditcardfrauddata

 httpsmediumcomqwilliamkoehrsenrandomforestsimpleexplanationadd
httpstowardsdatasciencecomanimplementationandexplanationoftherandomforestinpythonbfab

conjunto de entrenamiento test y
validación

vamos a comentar las diferencias entre los conjuntos de entrenamiento validación y test utilizados
en machine learning ya que suele haber bastante confusión en para qué es cada uno y cómo
utilizarlos adecuadamente

veremos que tenemos distintas técnicas de hacer la validación del modelo y aplicarlas con scikit
learn en python

un nuevo mundo

al principio de los tiempos sólo tenemos un conjunto pangea que contiene todo nuestro dato
disponible digamos que tenemos un archivo csv con registros

para entrenar nuestro modelo de machine learning y poder saber si está funcionando bien alguien
dijo separemos el conjunto de datos inicial en conjunto de entrenamiento train y conjunto de
pruebas test por lo general se divide haciendo y se toman muestras aleatorias no en
secuencia si no mezclado

para hacer el ejemplo sencillo supongamos que queremos hacer clasificación usando un algoritmo
supervisado con lo cual tendremos

 x train con registros para entrenar

 ytrain con las etiquetas de los resultados esperados de xtrain
 x test con registros para test

 ytest con las etiquetas de los resultados de xtest

nttpswwwaprendemachinelearningcomqueesmachinelearning

 httpswwwaprendemachinelearningcompasosmachinelearningconstruirmaquina
shttpswwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning
 httpswwwaprendemachinelearningcomaplicacionesdelmachinelearning

índice general

guardar csv y ver en excel 
otros ejemplos útiles de webscaping 
resumen emedrererderverdederverve verdevedederererer 
anexo ii machine learning en la nube aa 
machine learning en la nube google colaboratory con gpu 
machine learning desde el navegador 
la gpu en casa o en la nube a 
qué es google colab eee 
enlazar con google drive ae 
ejecutar una jupyter notebook de github 
instalar otras librerías python con pip 
resumen emedrererderverdederverve verdevedederererer 
anexo iii principal component analysis 
introducción a pca ee eee 
qué es principal component analysis 
cómo funciona pca eee ee 
selección de los componentes principales ñ 
pero porqué funciona pca 
ejemplo mínimo en python 
resumen emedrererderverdederverve verdevedederererer 

resultados de pca en el mundo real

conjunto de entrenamiento test y validación 

hágase el conjunto de test

lo interesante de esto es que una vez los separamos en registros para entrenar y para
probar usaremos sólo esos registros para alimentar el modelo al entrenar haciendo

modelofitxtrain y train

luego de entrenar nuestro modelo y habiendo decidido como métrica de negocio el accuracy el 
de aciertos obtenemos un sobre el set de entrenamiento y asumimos que ese porcentaje nos
sirve para nuestro objetivo de negocio

los registros que separamos en xtest aún nunca han pasado por el modelo de ml se
entiende esto porque eso es muy importante cuando usemos el set de test haremos

modelopredictxtest

como verás no estamos usando fit sólo pasaremos los datos sin la columna de ytest que
contiene las etiquetas además remarco que estamos haciendo predicción me refiero a que el modelo
no se está entrenando ni incorporando conocimiento el modelo se limita a ver la entrada y
escupir una salida

cuando hacemos el predict sobre el conjunto de test y obtenemos las predicciones las podemos
comprobar y contrastar con los valores reales almacenados en ytest y hallar así la métrica que
usamos los resultados que nos puede dar serán

conjunto de entrenamiento test y validación 

 si el accuracy en test es cercano al de entrenamiento dijimos por ejemplo en este caso
si estuviera entre ú quiere decir que nuestro modelo entrenado está generalizando bien
y lo podemos dar por bueno siempre y cuando estemos conformes con las métricas obtenidas

 si el accuracy en test es muy distinto al de entrenamiento tanto por encima como por debajo
nos da un ó un lejano al entonces es un indicador de que nuestro modelo no ha
entrenado bien y no nos sirve de hecho este podría ser un indicador de overfitting

para evaluar mejor el segundo caso es donde aparece el conjunto de validación

al séptimo día dios creo el crossvalidation

si el conjunto de train y test nos está dando métricas muy distintas esto es que el modelo no nos
sirve

para mejorar el modelo podemos pensar en tunear sus parámetros y volver a entrenar y probar
podemos intentar obtener más registros cambiar el preprocesado de datos limpieza balanceo de
clases selección de features generación de features de hecho podemos pensar que seleccio

 httpswwwaprendemachinelearningcomqueesoverfittingy underfittingycomosolucionarlo
shttpswwwaprendemachinelearningcomclasificacioncon datos desbalanceados

conjunto de entrenamiento test y validación 

namos un mal modelo y podemos intentar con distintos modelos de árbol de decisión redes
neuronales ensambles

la técnica de validación cruzada nos ayudará a medir el comportamiento dellos modelos
que creamos y nos ayudará a encontrar un mejor modelo rápidamente

repasemos antes de empezar hasta ahora contamos con conjuntos el de train y test el set de
validación no es realmente un tercer set si no que vive dentro del conjunto de train reitero el set
de validación no es un conjunto que apartemos de nuestro archivo esv original el set de validación
se utilizará durante iteraciones que haremos con el conjunto de entrenamiento

técnicas de validación cruzada

entonces volvamos a tener las cosas claras solo tenemos conjunto de train y test ok el de test
seguirá tratándose como antes lo apartamos y lo usaremos al final una vez entrenemos el modelo

o httpswwwaprendemachinelearningcomarbol dedecision enpythonclasificaciony prediccion
httpswwwaprendemachinelearningcomunasencillaredneuronalenpythonconkerasytensorflow
 httpswwwaprendemachinelearningcomrandomforestelpoderdelensamble

conjunto de entrenamiento test y validación 

dentro del conjunto de train y siguiendo nuestro ejemplo inicial tenemos registros la
validación más común utilizada y que nos sirve para entender el concepto es kfolds vamos a
comentarla

crossvalidation kfold con splits

lo que hacemos normalmente al entrenar el modelo es pasarle los registros y que haga el
fit con kfolds en este ejemplo de splits para entrenar en vez de pasarle todos los registros
directamente al modelo haremos así

e iterar veces
 apartaremos de muestras es decir 
 entrenamos al modelo con el restante de muestras 
 mediremos el accuracy obtenido sobre las que habíamos apartado

 esto quiere decir que hacemos entrenamientos independientes
 el accuracy final será el promedio de las accuracies anteriores

kfolds

iteracion 

en amarillo las muestras para entrenar y en verde el conjunto de validación

entonces fijémonos que estamos ocultando una quinta parte del conjunto de train durante cada
iteración esto es similar a lo que explique antes pero esta vez aplicado al momento de entrenamiento
al cabo de esas iteraciones obtenemos accuracies que deberían ser similares entre sí esto sería
un indicador de que el modelo está funcionando bien

ejemplo kfolds en python

veamos en código python usando la librería de data science scikitlearn como podemos hacer el
crossvalidation con kfolds

httpsscikitlearnorgstablemodulescrossvalidationhtmlikfold

conjunto de entrenamiento test y validación 

 from sklearn import datasets metrics
 from sklearnmodelselection import traintestsplit
 from sklearnmodelselection import crossvalscore
 from sklearnmodelselection import kfold
 from sklearnlinearmodel import logisticregression


iris datasetsloadiris


 x train x test ytrain ytest traintestsplitirisdata iristarget testsizy
 e randomstate



 kf kfoldnsplits



 clf logisticregression



 clffitx train ytrain



 score clfscorextrainytrain

 printmetrica del modelo score


 scores crossvalscoreclf x train ytrain cvkf scoringaccuracy

 printmetricas crossvalidation scores

 printmedia de crossvalidation scoresmean 

 preds clfpredictxtest

 scorepred metricsaccuracyscoreytest preds

 printmetrica en test scorepred
en el ejemplo vemos los pasos descritos anteriormente

e cargar el dataset

 dividir en train y test en 

 creamos un modelo de regresión logística podría ser otro y lo entrenamos con los datos de
train

 hacemos crossvalidation usando kfolds con splits

 comparamos los resultados obtenidos en el modelo inicial en el cross validation y vemos que
son similares

 finalmente hacemos predict sobre el conjunto de test y veremos que también obtenemos buen
accuracy

conjunto de entrenamiento test y validación 

más técnicas para validación del modelo
otras técnicas usadas y que nos provee sklearn para python son

stratified kfold

statified kfold es una variante mejorada de kfold que cuando hace los splits las divisiones del
conjunto de train tiene en cuenta mantener equilibradas las clases esto es muy útil porque
imaginen que tenemos que clasificar en sino y si una de las iteraciones del kfold normal
tuviera muestras con etiquetas sólo si el modelo no podría aprender a generalizar y aprenderá
para cualquier input a responder st esto lo soluciona el stratified kfold

leave p out

leave p out selecciona una cantidad p por ejemplo entonces se separarán de a muestras
contra las cuales validar y se iterará como se explico anteriormente si el valor p es pequeño
esto resultará en muchísimas iteraciones de entrenamiento con un alto coste computacional y
seguramente en tiempo si el valor p es muy grande podría contener más muestras que las
usadas para entrenamiento lo cual sería absurdo usar esta técnica con algo de sentido común y
manteniendo un equilibrio entre los scores y el tiempo de entreno

shufflesplit

shufflesplit primero mezcla los datos y nos deja indicar la cantidad de splits divisiones es decir
las iteraciones independientes que haremos y también indicar el tamaño del set de validación

 httpswwwaprendemachinelearningcomclasificacioncon datos desbalanceados
httpsscikitlearnorgstablemodulescrossvalidationhtmlistratifiedkfold
httpsscikitlearnorgstablemodulescrossvalidationhtmléleavepoutipo
shttpsscikitlearnorgstablemodulescrossvalidationhtmlirandompermutationscrossvalidationakashufflesplit

conjunto de entrenamiento test y validación 

series temporales atención al validar

el

para problemas de series temporales tenemos que prestar especial cuidado con los datos pues si
pasamos al modelo dato futuro antes de tiempo estaríamos haciendo data leakage esto es como
si le hiciéramos spoiler al modelo y le contaremos el final de la película antes de que la vea esto
causaría overfitting

al hacer el split inicial de datos estos deberán estar ordenados por fecha y no podemos
mezclarlos

para ayudarnos con el crossvalidation sklearn

nos provee de timeseriessplit
timeseriessplit

timeseriessplit es una variante adaptada de kfolds que evita la fuga de datos para hacerlo va

httpswwwwaprendemachinelearningcompronosticodeseriestemporalesconredes neuronalesenpython
 httpswwwaprendemachinelearningcomqueesoverfitting y underfittingy comosolucionarlo

 httpsscikitlearnorgstablemodulescrossvalidationhtml
httpsscikitlearnorgstablemodulescrossvalidationhtmlitimeseriessplit

conjunto de entrenamiento test y validación 

iterando los folds de a uno usando una ventana de tiempo que se desplaza y usando el fold más
reciente cómo el set de validación se puede entender mejor viendo una animación

timeseriessplit

en amarillo las muestras para entrenar y en verde el conjunto de validación

pero entonces cuando uso crossvalidation

es una buena práctica usar crossvalidation en nuestros proyectos de hecho usarlo nos ayudará a
elegir el modelo correcto y nos da mayor seguridad y respaldo ante nuestra decisión

pero siempre hay un pero

en casos en los que hacer sólo entrenamiento normal tome muchísimo tiempo y recursos podría
ser nuestra perdición imaginen que hacer un kfolds de implica hacer entrenos aunque un
poco más pequeños pero que consumirían mucho tiempo

entonces en la medida de lo posible siempre usar validación cruzada y vuelvo a reforzar el concepto
luego se probará el modelo contra el conjunto de pruebas test

para hacer tuneo de hiperparámetros como randomsearch gridsearch ó tuneo baye
siano es muy útil hacer crossvalidation

conjunto de entrenamiento test y validación 

si ya estoy conforme y quiero llevar el modelo a un
entorno de producción

supongamos que el entrenamiento haciendo cross validation y el predict en test nos están dando
buenos accuracy y similares y estamos conformes con nuestro modelo pues si lo queremos usar

en un entorno real y productivo antes de publicarlo es recomendado que agreguemos el

conjunto de test al modelo pues así estaremos aprovechando el de nuestros datos espero que
esto último también se entienda porque es super importante lo que estoy diciendo es que si al final
de todas nuestras iteraciones pre procesado de dato mejoras de modelo ajuste de hiperparámetros
y comparando con el conjunto de test estamos seguros que el modelo funciona correctamente es
entonces ahora que usaremos las muestras para entrenar al modelo y ese modelo final será
el que publicamos en producción

es una última iteración que debería mejorar el modelo final aunque este no lo podemos contrastar
contra nada excepto con su comportamiento en el entorno real

si esta última iteración te causara dudas no la hagas excepto que tu problema sea de tipo serie

s httpswwwaprendemachinelearningcomtupropioservicio demachinelearning

conjunto de entrenamiento test y validación 

temporal en ese caso sí que es muy importante hacerlo o quedaremos con un modelo que no es
el más actual

resumen

lo más importante que quisiera que quede claro en este capítulo es que entonces tenemos conjuntos
uno de train y otro de test el conjunto de validación no existe como tal si no que vive
temporalmente al momento de entrenar y nos ayuda a obtener al mejor modelo de entre los distintos
que probaremos para conseguir nuestro objetivo esa técnica es lo que se llama validación cruzada
ó en inglés crossvalidation

nota en los ejemplos de la documentación de sklearn podremos ver que usan las
palabras train y test pero conceptualmente se está refiriendo al conjunto de validación y
no al de test que usaremos al final esto es en parte el causante de tanta confusión con
este tema

tener en cuenta el tamaño de split es el usual pero puede ser distinto y esta proporción puede
cambiar sustancialmente las métricas obtenidas del modelo entrenado ojo con eso el tamaño ideal
dependerá del dominio de nuestro problema deberemos pensar en una cantidad de muestras para
test que nos aseguren que estamos el modelo creado está funcionando correctamente teniendo
 registros puede que con testear filas ya estemos conformes ó que necesitemos para
estar megaseguros por supuesto debemos recordar que las filas que estemos quitando para testear
no las estamos usando al entrenar

otro factor al hacer el experimento y tomar las muestras mezcladas mantener la semilla ó no
podremos reproducir el mismo experimento para comparar y ver si mejora o no este suele ser un
parámetro llamado randomstate y está bien que lo usemos para fijarlo

recomendaciones finales

 en principio separar train y test en una proporción de 
 hacer cross validation siempre que podamos

 no usar kfolds usar stratifiedkfolds en su lugar

 la cantidad de folds dependerá del tamaño del dataset que tengamos pero la cantidad
usual es pues es similar al que hacemos con traintest

 para problemas de tipo timeseries usar timeseriessplit

 si el accuracy métrica que usamos es similar en los conjuntos de train donde hicimos
cross validation y test podemos dar por bueno al modelo

httpswwwaprendemachinelearningcompronosticodeventasredesneuronalespythonembeddings

nota inicial

si has adquirido ó descargado este ejemplar primero que nada quiero agradecerte

este libro es un trabajo de gran ilusión y esfuerzo para mi ten en cuenta que lo fui construyendo en
mis tiempos libres entre el trabajo cursar un master cuidar de mis hijos pequeños y una pandemia
de contexto

escribir un artículo lleva desde la idea inicial en mi cabeza a investigar e informarme bien de cada
tema crear un ejercicio original en código python recopilar el conjunto de datos testear crear las
gráficas y redactar el texto y alguna cosilla más editar revisar corregir enlazar difundir pull
push

todos los artículos son versiones corregidas actualizadas y mejoradas de los originales publicados
hasta julio que podrás encontrar en el blog aprende machine learning

espero que sigas en contacto conmigo y si el libro es de tu agrado lo compartas con amigos

repositorio

el código completo y las jupyter notebooks las podrás ver y descargar desde mi repositorio github

 z

tu opinión

todos los comentarios para mejorar el libro son bienvenidos por lo que eres libre de enviarme
sugerencias ó correcciones por las vías que ofrece leanpub ó por twitter en ojbagnato ó por el
formulario de contacto del blog

httpswwwaprendemachinelearningcom
httpsgithubcomjbagnatomachinelearning
httpsleanpubcomaprendemlemailauthornew
httpstwittercomjbagnato
httpswwwaprendemachinelearningcomcontacto

conjunto de entrenamiento test y validación 

recursos adicionales
otros artículos interesantes en inglés

 documentación scikit learn sobre cross validation

 reasons why you should use cross validation
 random forest and kfold cross validation

y ejemplos en código python

httpsscikitlearnorgstablemodulescrossvalidationhtml
 httpstowardsdatasciencecomreasons whyyoushouldusecrossvalidationinyourdatascienceprojectae
s httpswwwkagglecomynourirandomforestkfoldcross validation

kmeans

kmeans es un algoritmo no supervisado de clustering se utiliza cuando tenemos un montón
de datos sin etiquetar el objetivo de este algoritmo es el de encontrar k grupos clusters entre
los datos crudos en este artículo repasaremos sus conceptos básicos y veremos un ejemplo paso a
paso en python

cómo funciona kmeans

el algoritmo trabaja iterativamente para asignar a cada punto las filas de nuestro conjunto de
entrada forman una coordenada uno de los k grupos basado en sus características son agrupados
en base a la similitud de sus features las columnas como resultado de ejecutar el algoritmo
tendremos

 los centroids de cada grupo que serán unas coordenadas de cada uno de los k conjuntos
que se utilizarán para poder etiquetar nuevas muestras

 etiquetas para el conjunto de datos de entrenamiento cada etiqueta perteneciente a uno de
los k grupos formados

los grupos se van definiendo de manera orgánica es decir que se va ajustando su posición en cada
iteración del proceso hasta que converge el algoritmo una vez hallados los centroids deberemos
analizarlos para ver cuales son sus características únicas frente a la de los otros grupos estos grupos
son las etiquetas que genera el algoritmo

casos de uso de kmeans

el algoritmo de clustering kmeans es uno de los más usados para encontrar grupos ocultos o

sospechados en teoría sobre un conjunto de datos no etiquetado esto puede servir para confirmar
 desterrar alguna teoría que teníamos asumida de nuestros datos y también puede ayudarnos a
descubrir relaciones asombrosas entre conjuntos de datos que de manera manual no hubiéramos
reconocido una vez que el algoritmo ha ejecutado y obtenido las etiquetas será fácil clasificar
nuevos valores o muestras entre los grupos obtenidos algunos casos de uso son

 segmentación por comportamiento relacionar el carrito de compras de un usuario sus tiempos
de acción e información del perfil

t httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningnosupervisado
httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearningclustering
httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearning

kmeans 

 categorización de inventario agrupar productos por actividad en sus ventas
 detectar anomalías o actividades sospechosas según el comportamiento en una web reconocer
un troll o un bot de un usuario normal

datos de entrada para kmeans

las features o características que utilizaremos como entradas para aplicar el algoritmo kmeans
deberán ser de valores numéricos continuos en lo posible en caso de valores categóricos por ej
hombremujer o ciencia ficción terror novelaetc se puede intentar pasarlo a valor numérico
pero no es recomendable pues no hay una distancia real como en el caso de géneros de película
o libros además es recomendable que los valores utilizados estén normalizados manteniendo una
misma escala en algunos casos también funcionan mejor datos porcentuales en vez de absolutos
no conviene utilizar features que estén correlacionados o que sean escalares de otros

el algoritmo kmeans

el algoritmo utiliza una proceso iterativo en el que se van ajustando los grupos para producir el
resultado final para ejecutar el algoritmo deberemos pasar como entrada el conjunto de datos y un
valor de k el conjunto de datos serán las características o features para cada punto las posiciones
iniciales de los k centroids serán asignadas de manera aleatoria de cualquier punto del conjunto de
datos de entrada luego se itera en dos pasos

 paso de asignación de datos en este paso cada fila de nuestro conjunto de datos se asigna al
centroide más cercano basado en la distancia cuadrada euclideana se utiliza la siguiente fórmula
donde dist es la distancia euclideana standard

argmin distc x
cec

paso de actualización de centroid en este paso los centroid de cada grupo son recalculados esto
se hace tomando una media de todos los puntos asignados en el paso anterior

l
dy es i

el algoritmo itera entre estos pasos hasta cumplir un criterio de detención

 si no hay cambios en los puntos asignados a los grupos

kmeans 

 o si la suma de las distancias se minimiza
 se alcanza un número máximo de iteraciones

el algoritmo converge a un resultado que puede ser el óptimo local por lo que será conveniente
volver a ejecutar más de una vez con puntos iniciales aleatorios para confirmar si hay una salida
mejor

elegir el valor de k

este algoritmo funciona preseleccionando un valor de k para encontrar el número de clusters en
los datos deberemos ejecutar el algoritmo para un rango de valores k ver los resultados y comparar
características de los grupos obtenidos en general no hay un modo exacto de determinar el valor
k pero se puede estimar con aceptable precisión siguiendo la siguiente técnica una de las métricas
usada para comparar resultados es la distancia media entre los puntos de datos y su centroid
como el valor de la media diminuirá a medida de aumentemos el valor de k deberemos utilizar la
distancia media al centroide en función de k y entontrar el punto codo donde la tasa de descenso
se afila aquí vemos una gráfica a modo de ejemplo

elbow point example







elbow point k





average withincluster distance to centroid

 
number of clusters k

ejemplo kmeans con scikitlearn

como ejemplo utilizaremos de entradas un conjunto de datos que obtuve de un proyecto propio en
el que se analizaban rasgos de la personalidad de usuarios de twitter he filtrado a famosos del

rn 

o 




kmeans 

mundo en diferentes areas deporte cantantes actores etc basado en una metodología de psicología
conocida como ocean the big five tendemos como características de entrada

 usuario el nombre en twitter

 op openness to experience grado de apertura mental a nuevas experiencias curiosidad
arte

 co conscientiousness grado de orden prolijidad organización

 ex extraversion grado de timidez solitario o participación ante el grupo social

 ag agreeableness grado de empatía con los demás temperamento

 ne neuroticism grado de neuroticismo nervioso irritabilidad seguridad en sí mismo

 wordcount cantidad promedio de palabras usadas en sus tweets

 categoria actividad laboral del usuario actor cantante etc

utilizaremos el algoritmo kmeans para que agrupe estos usuarios no por su actividad laboral si no
por sus similitudes en la personalidad si bien tenemos columnas de entrada sólo utilizaremos 
en este ejemplo de modo que podamos ver en un gráfico tridimensional y sus proyecciones a d los
grupos resultantes pero para casos reales podemos utilizar todas las dimensiones que necesitemos
una de las hipótesis que podríamos tener es todos los cantantes tendrán personalidad parecida
y así con cada rubro laboral pues veremos si lo probamos o por el contrario los grupos no están
relacionados necesariamente con la actividad de estas celebridades

agrupar usuarios twitter de acuerdo a su
personalidad con kmeans

implementando kmeans en python con sklearn

comenzaremos importando las librerías que nos asistirán para ejecutar el algoritmo y graficar

import pandas as pd

import numpy as np

import matplotlibpyplot as plt

import seaborn as sb

from sklearncluster import kmeans

from sklearnmetrics import pairwisedistancesargminmin

zmatplotlib inline

from mpltoolkitsmplotd import axesd
pltrcparamsfigure figsize 
pltstyleuse ggplot

importamos el archivo csv para simplificar suponemos que el archivo se encuentra en el mismo

directorio que el notebook y vemos los primeros registros del archivo tabulados

 httpwwwaprendemachinelearningcomwpcontentuploadsanalisiscsv

kmeans

dataframe head 

dataframe pdreadcsvranalisiscsv



usuario op co ex ag ne wordcount categoria
 gerardpique 
 aguerosergiokun 
albertochicote 
 alejandrosanz 
 alfredocasero 

dataframe describe 

también podemos ver una tabla de información estadística que nos provee pandas dataframe

op co ex ag ne wordcount categoria
count 
mean 
std 
min 
 
 
 
max 

el archivo contiene diferenciadas categorías actividades

ooon lerona

para saber cuantos registros tenemos de cada uno hacemos

 actoractriz
 cantante

 modelo

 tv series
radio

 tecnología
 deportes

 politica

 escritor

aborales que son

kmeans

printdataframe groupby categoria size

categoria

v oauewwhh

dtype int











como vemos tenemos cantantes actores deportistas políticosetc

visualización de datos

veremos graficamente nuestros datos para tener una idea de la dispersión de los mismos

dataframe drop categoriahist
pltshow



a b

s



wordcount

kmeans 

en este caso seleccionamos dimensiones op ex y ag y las cruzamos para ver si nos dan alguna
pista de su agrupación y la relación con sus categorías
sbpairplotdataframedropna huecategoriasizevarsop
catter

ex agkindsy

categoria

ssescsccc
oaualn

revisando la gráfica no pareciera que hay algún tipo de agrupación o correlación entre los usuarios
y sus categorías

n

o

l













kmeans 

definimos la entrada

concretamos la estructura de datos que utilizaremos para alimentar el algoritmo como se ve sólo
cargamos las columnas op ex y ag en nuestra variable x

x nparraydataframe op exag
y nparraydataframe categoria
xshape

 python



ahora veremos una gráfica en d con colores representando las categorías
 python
fig plt figure
ax axesd fig
coloresblue red green blue cyan yellow orange black pink brown n
 purple
asignar
for row in y
asignar appendcolores row
axscatter x x x casignar s

d



l



kmeans 

veremos si con kmeans podemos pintar esta misma gráfica de otra manera con clusters
diferenciados

obtener el valor k

vamos a hallar el valor de k haciendo una gráfica e intentando hallar el punto de codo que
comentábamos antes este es nuestro resultado

nc range 

kmeans kmeansnclustersi for i in nc

kmeans

score kmeansifitxscorex for i in range len kmeans
score

pltplotncscore

pltxlabel number of clusters

pltylabel score

plttitle elbow curve

pltshow

qué es el machine learning

veamos algunas definiciones existentes sobre machine learning para intentar dar comprensión a
esta revolucionaria materia

definiendo machine learning

el machine learning traducido al español como aprendizaje automático ó aprendizaje de
máquinas es un subcampo de la inteligencia artificial que busca resolver el cómo construir
programas de computadora que mejoran automáticamente adquiriendo experiencia

esta definición implica que el programa que se crea con ml no necesita que el programador indique
explicitamente las reglas que debe seguir para lograr su tarea si no que este mejora automáticamente

en los últimos años han surgido grandes volúmenes de datos de diversas fuentes públicas big data y
el aprendizaje automático relacionado al campo estadístico consiste en extraer y reconocer patrones
y tendencias para comprender qué nos dicen los datos para ello se vale de algoritmos que pueden
procesar gygas yo terabytes en tiempos razonables y obtener información útil

z 

una definición técnica

podemos encontrar la siguiente definición técnica sobre aprendizaje automático

a computer program is said to learn from experience e with respect to some class of tasks
tand performance measure p ifits performance at tasks in t as measured by p improves
wwith experience e

la experiencia e hace referencia a grandes volúmenes de datos recolectados big data para la toma
de decisiones t y la forma de medir su desempeño p para comprobar que esos algoritmos mejoran
con la adquisición de más experiencia

diagrama de venn

drew conway creó un simpático diagrama de venn en el que inerrelaciona diversos campos aquí
copio su versión al español

kmeans 

elbow curve

number of clusters

realmente la curva es bastante suave considero a como un buen número para k según vuestro
criterio podría ser otro

ejecutamos kmeans

ejecutamos el algoritmo para clusters y obtenemos las etiquetas y los centroids

kmeans kmeansnclustersfitx
centroids kmeansclustercenters

printcentroids
 
 
 
 
 

ahora veremos esto en una gráfica d con colores para los grupos y veremos si se diferencian las
estrellas marcan el centro de cada cluster

n

o

o






kmeans

 predicting the clusters
labels kmeanspredictx
 getting the cluster centers
c kmeansclustercenters
coloresred green blue cyan yellow
asignar
for row in labels
asignar appendcolores row

fig plt figure
ax axesd fig
axscatter x x x casignar s

axscatterc c c marker ccolores s





aqui podemos ver que el algoritmo de kmeans con k ha agrupado a los usuarios twitter
por su personalidad teniendo en cuenta las dimensiones que utilizamos openess extraversion y

agreeablenes pareciera que no hay necesariamente una relación en los grupos con sus actividades

de celebrity haremos gráficas en dimensiones con las proyecciones a partir de nuestra gráfica

d para que nos ayude a visualizar los grupos y su clasificación

n

 a

 s

 a

kmeans

 getting the values and plotting it
f dataframe opvalues
f dataframe exvalues

pltscatterf f casignar s
pltscatterc c marker ccolores s
pltshow

l 
 
 
 
s 
 
 or p 
 
 í 
 
 
 r 
be o
 í 
 
 

 

 getting the values and plotting it
f dataframe opvalues
f dataframe agvalues

pltscatterf f casignar s
pltscatterc c marker ccolores s
pltshow

eo n 

o

o

kmeans



 
o
d o c
j f 
 e
o r 
 
c a 
 rik r 
 
 n c q r 




dataframe ex values

dataframe ag values

pltscatterf f casignar s

pltscatterc c marker

pltshow

cecolores s

n

o

l

kmeans 

 r 
 ooo o 
 o o 
 
l ji e 
 

en estas gráficas vemos que están bastante bien diferenciados los grupos podemos ver cada uno de
los clusters cuantos usuarios tiene

copy pddataframe
copy usuario dataframe usuario values
copy categoria dataframe categoria values
copylabel labels
pddataframe
cantidadgrupocolorcolores
cantidadgrupocantidad copy groupby labelsize
cantidadgrupo

cantidadgrupo

cantidad

n

o

o

kmeans 

y podemos ver la diversidad en rubros laborales de cada uno por ejemplo en el grupo rojo vemos
que hay de todas las actividades laborales aunque predominan de actividad y correspondiente a
actores y cantantes con y famosos

groupreferrerindex copylabel 
groupreferrals copygroupreferrerindex

diversidadgrupo pddataframe 
diversidadgrupocategoria
diversidadgrupocantidad groupreferralsgroupby categoria size 
diversidadgrupo

categoria cantidad

ojo nan
 
 
 
 

de categoría modelos hay sobre un total de buscaremos los usuarios que están más cerca a los
centroids de cada grupo que podríamos decir que tienen los rasgos de personalidad característicos
que representan a cada cluster

vemos el representante del grupo el usuario cercano a su centroid
closest pairwisedistancesargminminkmeansclustercenters x
closest

array posicion en el array de usuarios

ne

a

d

a

kmeans 

usersdataframe usuario values
for row in closest
printusers row

carmenelectra pabloiglesias judgejudy jpvarsky kobebryant

en los centros vemos que tenemos una modelo un político presentadora de tv locutor de radio y
un deportista

clasificar nuevas muestras

y finalmente podemos agrupar y etiquetar nuevos usuarios twitter con sus características y
clasificarlos vemos el ejemplo con el usuario de david guetta y nos devuelve que pertenece al
grupo verde

xnew nparray davidguetta

newlabels kmeanspredictxnew
printnewlabels



resumen

el algoritmo de kmeans nos ayudará a crear clusters cuando tengamos grandes grupos de datos
sin etiquetar cuando queramos intentar descubrir nuevas relaciones entre features o para probar o
declinar hipótesis que tengamos de nuestro negocio

atención puede haber casos en los que no existan grupos naturales o clusters que contengan
una verdadera razón de ser si bien kmeans siempre nos brindará k clusters quedará en nuestro
criterio reconocer la utilidad de los mismos o bien revisar nuestras features y descartar las que no
sirven o conseguir nuevas

también tener en cuenta que en este ejemplo estamos utilizando como medida de similitud entre
features la distancia euclideana pero podemos utilizar otras diversas funciones que podrían
arrojar mejores resultados como manhattan lavenshtein mahalanobis etc hemos visto
una descripción del algoritmo aplicaciones y un ejemplo python paso a paso que podrán descargar
también desde los siguientes enlaces

p httpseswikipediaorgwikidistanciaeuclidiana

 httpseswikipediaorgwikigeometrcadadeltaxista
httpsenwikipediaorgwikilevenshteindistance

 httpsenwikipediaorgwikimahalanobisdistance

kmeans 



 notebook jupiter online
 descargar archivo csv y notebook ejercicio kmeans
 visualizar y descargar desde jbagnato github



httpnbviewerjupyterorggithubjbagnatomachinelearningblobmasterejerciciokmeansipynb
httpwwwaprendemachinelearningcomwpcontentuploadsanalisiscsv
httpwwwaprendemachinelearningcomwpcontentuploadsejerciciokmeansipynb
httpsgithubcomjbagnatomachinelearningblobmasterejerciciokmeansipynb

 httpsgithubcomjbagnatomachinelearning

knearestneighbor

knearestneighbor es un algoritmo basado en instancia de tipo supervisado de machine

learning puede usarse para clasificar nuevas muestras valores discretos o para predecir regresión
valores continuos al ser un método sencillo es ideal para introducirse en el mundo del aprendizaje
automático sirve esencialmente para clasificar valores buscando los puntos de datos más similares
por cercanía aprendidos en la etapa de entrenamiento y haciendo conjeturas de nuevos puntos
basado en esa clasificación

a diferencia de kmeans que es un algoritmo no supervisado y donde la k significa la cantidad
de grupos clusters que deseamos clasificar en knearest neighbor la k significa la cantidad
de puntos vecinos que tenemos en cuenta en las cercanías para clasificar los n grupos que ya se
conocen de antemano pues es un algoritmo supervisado

qué es el algoritmo knearest neighbor 

es un método que simplemente busca en las observaciones más cercanas a la que se está tratando de
predecir y clasifica el punto de interés basado en la mayoría de datos que le rodean como dijimos
antes es un algoritmo

 supervisado esto brevemente quiere decir que tenemos etiquetado nuestro conjunto de datos
de entrenamiento con la clase o resultado esperado dada una fila de datos

 basado en instancia esto quiere decir que nuestro algoritmo no aprende explícitamente un
modelo como por ejemplo en regresión logística o árboles de decisión en cambio memoriza
las instancias de entrenamiento que son usadas como base de conocimiento para la fase de
predicción

dónde se aplica knearest neighbor

aunque sencillo se utiliza en la resolución de multitud de problemas como en sistemas de
recomendación búsqueda semántica y detección de anomalías

 httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearninginstancia
httpwwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado

 httpwwwaprendemachinelearningcomkmeansenpythonpasoapaso
httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningnosupervisado

 httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearningclustering

knearestneighbor 

pros y contras

como pros tiene sobre todo que es sencillo de aprender e implementar tiene como contras que
utiliza todo el dataset para entrenar cada punto y por eso requiere de uso de mucha memoria
y recursos de procesamiento cpu por estas razones knn tiende a funcionar mejor en datasets
pequeños y sin una cantidad enorme de features las columnas

cómo funciona knn

 calcular la distancia entre el item a clasificar y el resto de items del dataset de entrenamiento

 seleccionar los k elementos más cercanos con menor distancia según la función que se use

 realizar una votación de mayoría entre los k puntos los de una claseetiqueta que decidirán su clasificación final

teniendo en cuenta el punto veremos que para decidir la clase de un punto es muy importante
el valor de k pues este terminará casi por definir a qué grupo pertenecerán los puntos sobre
todo en las fronteras entre grupos por ejemplo y a priori yo elegiría valores impares de k para
desempatar si las features que utilizamos son pares no será lo mismo tomar para decidir valores
que esto no quiere decir que necesariamente tomar más puntos implique mejorar la precisión
lo que es seguro es que cuantos más puntos k más tardará nuestro algoritmo en procesar y
darnos respuesta las formas más populares de medir la cercanía entre puntos son la distancia
euclidiana la de siempre o la cosine similarity mide el ángulo de los vectores cuanto
menores serán similares recordemos que este algoritmo y prácticamente todos en ml funcionan
mejor con varias características de las que tomemos datos las columnas de nuestro dataset lo
que entendemos como distancia en la vida real quedará abstracto a muchas dimensiones que no
podemos visualizar fácilmente como por ejemplo en un mapa

un ejemplo knearest neighbor en python

exploremos el algoritmo con scikit learn

realizaremos un ejercicio usando python y su librería scikitlearn que ya tiene implementado el
algoritmo para simplificar las cosas veamos cómo se hace

el ejercicio app reviews

para nuestro ejercicio tomaremos registros con opiniones de usuarios sobre una app reviews
utilizaremos columnas de datos como fuente de alimento del algoritmo recuerden que sólo tomaré

qué es el machine learning 

diagrama de venn

en esta aproximación al ml podemos ver que es una intersección entre conocimientos de matemá
ticas y estadística con habilidades de hackeo del programador

aproximación para programadores

los programadores sabemos que los algoritmos de búsqueda pueden tomar mucho tiempo en
concluir y que cuanto mayor sea el espacio de búsqueda crecerán exponencialmente las posibilidades
de combinación de una respuesta óptima haciendo que los tiempos de respuesta tiendan al infinito
o que tomen más tiempo de lo que un ser humano pueda tolerar por quedarse sin vida o por
impaciencia

para poder resolver este tipo de situaciones surgen soluciones de tipo heurísticas que intentan dar
intuición al camino correcto a tomar para resolver un problema estos logran buenos resultados
en tiempos menores de procesamiento pero muchas veces su intuición es arbitraria y pueden fallar

d



l










knearestneighbor 

 features para poder graficar en dimensiones pero para un problema en la vida real conviene
tomar más características de lo que sea que queramos resolver esto es únicamente con fines de
enseñanza las columnas que utilizaremos serán wordcount con la cantidad de palabras utilizadas
y sentimentvalue con un valor entre y que indica si el comentario fue valorado como positivo
o negativo nuestras etiquetas serán las estrellas que dieron los usuarios a la app que son valores
discretos del al podemos pensar que si el usuario puntúa con más estrellas tendrá un sentimiento
positivo pero no necesariamente siempre es así

comencemos con el código

primero hacemos imports de librerías que utilizaremos para manejo de datos gráficas y nuestro
algoritmo

import pandas as pd

import numpy as np

import matplotlibpyplot as plt

from matplotlibcolors import listedcolormap
import matplotlibpatches as mpatches

import seaborn as sb

zmatplotlib inline
pltrcparamsfigure figsize 
pltstyleuse ggplot

from sklearnmodelselection import traintestsplit
from sklearnpreprocessing import minmaxscaler

from sklearnneighbors import kneighborsclassifier
from sklearnmetrics import classificationreport
from sklearnmetrics import confusionmatrix

cargamos el archivo entrada csv con pandas usando separador de punto y coma pues en las reviews
hay textos que usan coma con head vemos los primeros registros

dataframe pdreadcsvrreviewssentimentcsvsep
dataframe head

knearestneighbor



review title review text wordcount titlesentiment textsentiment star rating sentimentvalue
 sin conexión hola desde hace algo más de un mes me pone sin negative negative 
 faltan cosas han mejorado la apariencia pero no negative negative 
 es muy buena lo recomiendo andres e puto amooo nan negative 
 version antigua me gustana mas la version anterior esta es mas nan negative 
 esta bien sin ser la biblia esta bien negative negative 
 buena nada del otro mundo pero han mejorado mucho positive negative 
 de gran ayuda lo malo q necesita de pero la app es muy buena positive negative 
 muy buena estaba más acostumbrado al otro diseño pero e positive negative 
 ta to guapa va de escándalo positive negative 
 se han corregido han corregido muchos fallos pero el diseño es negative negative 

aprovechamos a ver un resumen estadístico de los datos

dataframe describe 

wordcount star rating sentimentvalue

count 

mean 
std 

min 

 
 
 

max 


























son registros las estrellas lógicamente vemos que van del al la cantidad de palabras van
de sóla hasta y las valoraciones de sentimiento están entre y con una media de 

y a partir del desvío estándar podemos ver que la mayoría están entre y 

un poco de visualización

veamos unas gráficas simples y qué información nos aportan

dataframehist
pltshow

knearestneighbor 

star rating sentimentvalue

 

wordcount

vemos que la distribución de estrellas no está balanceada esto no es bueno convendría tener
las mismas cantidades en las salidas para no tener resultados tendenciosos para este ejercicio lo
dejaremos así pero en la vida real debemos equilibrarlos la gráfica de valores de sentimientos
parece bastante una campana movida levemente hacia la derecha del cero y la cantidad de palabras
se centra sobre todo de a veamos realmente cuantas valoraciones de estrellas tenemos

 printdataframegroupby star ratingsize

star rating









dtype int

n e w

con eso confirmamos que hay sobre todo de y estrellas y aqui una gráfica más bonita

 sbfactorplot star ratingdatadataframekindcount aspect

knearestneighbor 

 





 t q t 
 

star rating

count

graficamos mejor la cantidad de palabras y confirmamos que la mayoría están entre y palabras

 sbfactorplotwordcountdatadataframekindcount aspect

iiiiiiiiiiii iiiiiiiiiiiiii

 
wordcount

count
 t

u

preparamos las entradas

creamos nuestro x e y de entrada y los sets de entrenamiento y test

dataframe wordcount sentimentvalue values
dataframe star ratingvalues

scaler minmaxscaler
xtrain scalerfittransformxtrain




 x train x test y train ytest traintestsplitx y randomstate


 x test scalertransformxtest

d



l

ne

a

knearestneighbor 

usemos knearest neighbor con scikit learn

definimos el valor de k en esto realmente lo sabemos más adelante ya veréis y creamos nuestro
clasificador

nneighbors 

knn kneighborsclassifier nneighbors

knnfitxtrain ytrain

printaccuracy of knn classifier on training set f
 format knnscorextrain y train

printaccuracy of knn classifier on test set f
 formatknnscorextest ytest

accuracy of knn classifier on training set 
accuracy of knn classifier on test set 

vemos que la precisión que nos da es de en el set de entrenamiento y del para el de test

nota como verán utilizamos la clase kneighborsclassifier de scikit learn puesto
que nuestras etiquetas son valores discretos estrellas del al pero deben saber que
también existe la clase kneighborsregressor para etiquetas con valores continuos

precisión del modelo

confirmemos la precisión viendo la confusión matrix y el reporte sobre el conjunto de test que
nos detalla los aciertos y fallos

pred knnpredictxtest
printconfusion matrixytest pred
printclassificationreportytest pred

httpscikitlearnorgstablemodulesgeneratedsklearnneighborskneighborsclassifierhtmlisklearnneighborskneighborsclassifier
httpscikitlearnorgstablemodulesgeneratedsklearnneighborskneighborsregressor htmlsklearnneighborskneighborsregressor
shttpwwwaprendemachinelearningcomclasificacioncondatos desbalanceados

n

o

l











knearestneighbor 

 
 
 
 
 

precision recall flscore support

 

 

 

 

 

avg total 

cómo se ve la puntuación f es del bastante buena nota recuerden que este es sólo un
ejercicio para aprender y tenemos muy pocos registros totales y en nuestro conjunto de test por
ejemplo de estrellas sólo tiene valoración y esto es evidentemente insuficiente

y ahora la gráfica que queríamos ver

ahora realizaremos la grafica con la clasificación obtenida la que nos ayuda a ver fácilmente en
donde caerán las predicciones nota al ser features podemos hacer la gráfica d y si fueran 
podría ser en d pero para usos reales podríamos tener más de dimensiones y no importaría poder
visualizarlo sino el resultado del algoritmo

h step size in the mesh

 create color maps
cmaplight listedcolormap ffaaaa ffcc ffffbbffffcfc
cmapbold listedcolormap ff ffffffoo ooffffffoo

 we create an instance of neighbours classifier and fit the data
clf kneighborsclassifier nneighbors weightsdistance
clffitx y

 plot the decision boundary for that we will assign a color to each
 point in the mesh xmin xmaxxymin y max
xxmin x max x min x max 
y min y max x min x max 
xx yy npmeshgridnparangexmin xmax h
nparangeymin ymax h
z clfpredictnpcxxravel yy ravel

knearestneighbor

 put the result into a color plot
 zreshapexxshape
pltfigure

 plot also the training points

edgecolork s
pltxlimxxmin xxmax
pltylimyymin yymax

patch mpatchespatchcolorff 
patch mpatchespatchcolorff
patch mpatchespatchcolorffff
patch mpatchespatchcolorffff
patch mpatchespatchcolorff 

ó

p

plttitleclass classification k i

 nneighbors weights

pltshow

pltpcolormeshxx yy z cmapcmaplight

pltscatterx x cy cmapcmapbold

label
label
label
label
label

 weights

tlegendhandlespatch patch patch patchpatch

s

knearestneighbor 

class classification k weights distance

erwne

vemos las zonas en las que se relacionan cantidad de palabras con el valor de sentimiento de la
review que deja el usuario se distinguen regiones que podríamos dividir así

class classification k weights distance

erwne

 o
 aíñzgaieni ai i

 s e

 


 



es decir que a ojo una review de palabras y sentimiento nos daría una valoración de zona

knearestneighbor 

celeste con estas zonas podemos intuir ciertas características de los usuarios que usan y valoran la
app

 los usuarios que ponen estrella tienen sentimiento negativo y hasta palabras

 los usuarios que ponen estrellas dan muchas explicaciones hasta palabras y su
sentimiento puede variar entre negativo y algo positivo

 los usuarios que ponen estrellas son bastante neutrales en sentimientos puesto que están en
torno al cero y hasta unas palabras

 los usuarios que dan estrellas son bastante positivos de en adelante aproximadamente
y ponen pocas palabras hasta 

elegir el mejor valor de k

sobre todo importante para desempatar o elegir los puntos
frontera

antes vimos que asignamos el valor nneighbors como valor de kk y obtuvimos buenos
resultados pero de donde salió ese valor pues realmente tuve que ejecutar este código que viene
a continuación donde vemos distintos valores k y la precisión obtenida

krange range 
scores 
for k in krange
knn kneighborsclassifier nneighbors k
knnfitxtrain ytrain
scores appendknnscorextest y test
pltfigure
pltxlabelk
pltylabel accuracy
pltscatterkrange scores
pltxticks

knearestneighbor 

en la gráfica vemos que con valores k a k es donde mayor precisión se logra

clasificar ó predecir nuevas muestras

ya tenemos nuestro modelo y nuestro valor de k ahora lo lógico será usarlo pues supongamos que
nos llegan nuevas reviews veamos como predecir sus estrellas de maneras la primera

printclfpredict 


este resultado nos indica que para palabras y sentimiento nos valorarán la app con estrellas
pero también podríamos obtener las probabilidades que de nos den o estrellas con predict

proba

printclfpredictproba 

 

aquí vemos que para las coordenadas hay probabilidades que nos den estrellas puedes
comprobar en el gráfico anterior que encajan en las zonas que delimitamos anteriormente

qué es el machine learning 

los algoritmos de ml intentan utilizar menos recursos computacionales para entrenar grandes
volúmenes de datos e ir aprendiendo por sí mismos podemos dividir el ml en grandes categorías
aprendizaje supervisado o aprendizaje no supervisado hay una tercer categoría llamada apren
dizaje por refuerzo pero no será tratada en este libro

entre los algoritmos más utilizados en inteligencia artificial encontramos

 arboles de decisión

 regresión lineal

 regresión logística

 k nearest neighbor

 pca principal component analysis

 svm

 gaussian naive bayes

 kmeans

 redes neuronales artificiales

 aprendizaje profundo ó deep learning

una mención especial a las redes neuronales artificiales

una mención distintiva merecen las rnas ya que son algoritmos que imitan al comportamiento de
las neuronas humanas y su capacidad de sinápsis para la obtención de resultados interrelacionando
diversas capas de neuronas para darle mayor poder de aprendizaje

aunque este código existe desde hace más de años en la última década han evolucionado
notoriamente en paralelo a la mayor capacidad tecnológica de procesamiento memoria ram y
disco la nube etc y están logrando impresionantes resultados para analizar textos y síntesis de
voz traducción automática de idiomas procesamiento de lenguaje natural visión artificial análisis
de riesgo clasificación y predicción y la creación de motores de recomendación

resumen

el machine learning es una nueva herramienta clave que posibilitará el desarrollo de un futuro
mejor para la humanidad brindando inteligencia a robots coches y hogares las smart cities el
iot internet of things ya se está volviendo una realidad y también las aplicaciones de machine
learning en asistentes como siri las recomendaciones de netflix o sistemas de navegación autónoma
en drones para los ingenieros o informáticos es una disciplina fundamental para modelar construir
y transitar este nuevo futuro

knearestneighbor 

resumen

en este ejercicio creamos un modelo con python para procesar y clasificar puntos de un conjunto de
entrada con el algoritmo knearest neighbor cómo su nombre en inglés lo dice se evaluán los k
vecinos más cercanos para poder clasificar nuevos puntos al ser un algoritmo supervisado debemos
contar con suficientes muestras etiquetadas para poder entrenar el modelo con buenos resultados
este algoritmo es bastante simple y como vimos antes necesitamos muchos recursos de memoria
y cpu para mantener el dataset vivo y evaluar nuevos puntos esto no lo hace recomendable para
conjuntos de datos muy grandes en el ejemplo sólo utilizamos dimensiones de entrada para poder
graficar y ver en dos dimensiones cómo se obtienen y delimitan los grupos finalmente pudimos
hacer nuevas predicciones y a raíz de los resultados comprender mejor la problemática planteada

recursos y enlaces

 descarga la jupyter notebook y el archivo de entrada csv
 ó puedes visualizar online
 o ver y descargar desde mi cuenta github

más artículos de interés sobre knearest neighbor en inglés

 implementing knn in scikit learn

 introduction to knn
 complete guide to knn

 solving a simple classification problem with python

httpwwwaprendemachinelearningcomwpcontentuploadsejercicioknearestneighboripynb

 httpwwwaprendemachinelearningcomwpcontentuploadsreviewssentimentcsv

httpnbviewer jupyterorggithubjbagnatomachinelearningblobmasterejercicioknearestneighboripynb
httpsgithubcomjbagnatomachinelearning
httpstowardsdatasciencecomimplementingknearestneighborswithscikitlearneeea

 httpstowardsdatasciencecomintroductiontoknearestneighborsbbbd
httpskevinzakkagithubioknearestneighbor

 httpstowardsdatasciencecomsolvingasimpleclassification problemwithpythonfruitslovers editiondabbd

naive bayes comprar casa o
alquilar

en este capítulo veremos un ejercicio práctico utilizando naive bayes intentando llevar los algo
ritmos de machine learning a un ejemplo de la vida real repasaremos la teoría del teorema de
bayes de estadística para poder tomar una decisión muy importante me conviene comprar
casa ó alquilar

veamos si la ciencia de datos nos puede ayudar a resolver el misterio si alquilo casa
estoy tirando el dinero a la basura ó es realmente conveniente pagar una hipoteca
durante el resto de mi vida

si bien tocaremos el tema superficialmente sin meternos en detalles como taza de interés de
hipotecas variablefija comisiones de bancos etc haremos un planteo genérico para obtener
resultados y tomar la mejor decisión dada nuestra condición actual en otros capítulos vimos diversos
algoritmos supervisados del aprendizaje automático que nos dejan clasificar datos yo obtener
predicciones o asistencia a la toma de decisiones árbol de decisión regresión logística y lineal
red neuronal por lo general esos algoritmos intentan minimizar algún tipo de coste iterando las
entradas y las salidas y ajustando internamente las pendientes ó pesos para hallar una salida
esta vez el algoritmo que usaremos se basa completamente en teoría de probabilidades y resultados
estadísticos será suficiente el teorema de bayes para obtener buenas decisiones veamos

los datos de entrada

importemos las librerías que usaremos y visualicemos la información que tenemos de entrada

httpswwwyoutubecomwatchvvxwouco

 httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado
httpwwwaprendemachinelearningcomqueesmachinelearning

 httpwwwaprendemachinelearningcomarbolde decisionenpythonclasificacionyprediccion
httpwwwaprendemachinelearningcomregresionlogisticaconpythonpasoapaso
httpwwwaprendemachinelearningcomregresionlinealenespanolconpython

 httpwwwaprendemachinelearningcomunasencillaredneuronalenpythonconkerasytensorflow
httpseswikipediaorgwikiteoremadebayes

ne



l









naive bayes comprar casa o alquilar 

import pandas as pd

import numpy as np

import matplotlibpyplot as plt
from matplotlib import colors
import seaborn as sb

zmatplotlib inline
pltrcparamsfigure figsize 
pltstyleuse ggplot

from sklearnmodelselection import traintestsplit
from sklearnmetrics import classificationreport
from sklearnmetrics import confusionmatrix

from sklearnnaivebayes import gaussiannb

from sklearn featureselection import selectkbest

carguemos la información sobre inmuebles del archivo csv

dataframe pdreadcsvrcompraralquilarcsv
dataframe head

ingresos qgastos comunes pago coche gastos otros ahorros vivienda estado civil hijos trabajo comprar
click to expand output double click to hide output

o uuu tuuu lo 
 o
 
 o o o
 o o 
 
 s 
 o o o
 o 
 

las columnas que tenemos son

 ingresos los ingresos de la familia mensual

 gastos comunes pagos de luz agua gas etc mensual

 pago coche si se está pagando cuota por uno o más coches y los gastos en combustible etc al
mes

 httpwwwaprendemachinelearningcomwpcontentuploadscompraralquilarcsv
 httpwwwaprendemachinelearningcomwpcontentuploadsbayesentradaspng

naive bayes comprar casa o alquilar 

 gastosotros compra en supermercado y lo necesario para vivir al mes
 ahorros suma de ahorros dispuestos a usar para la compra de la casa

 vivienda precio de la vivienda que quiere comprar esa familia

 estado civil

 osoltero
 casado
 divorciado

hijos cantidad de hijos menores y que no trabajan
 trabajo

 sin empleo

 autónomo freelance

 empleado

 empresario

 pareja autónomos

 pareja empleados

 pareja autónomo y asalariado

 parejaempresario y autónomo

 pareja empresarios los dos o empresario y empleado
 comprar no comprar comprar esta será nuestra columna de salida

algunos supuestos para el problema formulado

 está planteado en euros pero podría ser cualquier otra moneda

 notiene en cuenta ubicación geográfica cuando sabemos que dependerá mucho los precios de
los inmuebles de distintas zonas

 se supone una hipoteca fija a años con interés de mercado bajo

con esta información queremos que el algoritmo aprenda y que como resultado podamos consultar
nueva información y nos dé una decisión sobre comprar o alquilar casa

el teorema de bayes

el teorema de bayes es una ecuación que describe la relación de probabilidades condicionales

de cantidades estadísticas en clasificación bayesiana estamos interesados en encontrar la proba
bilidad de que ocurra una clase dadas unas características observadas datos lo podemos escribir
como p clase datos el teorema de bayes nos dice cómo lo podemos expresar en términos de
cantidades que podemos calcular directamente

httpseswikipediaorgwikiteoremadebayes
shttpseswikipediaorgwikiclasificadorbayesianoingenuo

naive bayes comprar casa o alquilar 

pdatosclase pclase

pclase datos 
pdatos



 clase es una salida en particular por ejemplo comprar

 datos son nuestras características en nuestro caso los ingresos gastos hijos etc
pclasedatos se llama posterior y es el resultado que queremos hallar
pdatosclase se llama verosimilitud en inglés likelihood

pclase se llama anterior pues es una probabilidad que ya tenemos

pdatos se llama probabilidad marginal

si estamos tratando de elegir entre dos clases como comprar ó alquilar entonces una manera de
tomar la decisión es calcular la tasa de probabilidades a posterior

httpwwwaprendemachinelearningcomwpcontentuploadsteoremabayespng

naive bayes comprar casa o alquilar 

pcomprar datos pdatoscomprar pcomprar

palquilar datos pdatosalquilar palquilar



con esta maniobra nos deshacemos del denominador de la ecuación anterior pdatos el llamado
probabilidad marginal

clasificador gaussian naive bayes

uno de los tipos de clasificadores más populares es el llamado en inglés gaussian naive bayes
classifier

veamos cómo es su fórmula para comprender este curioso nombre aplicaremos clases comprar
alquilar y tres características ingresos ahorros e hijos

 httpwwwaprendemachinelearningcomwpcontentuploadsteoremabayespng
 httpseswikipediaorgwikiclasificadorbayesianoingenuo

naive bayes comprar casa o alquilar 

pcomprarpingresoscomprarpahorroscomprar phijoscomprar

posteriorcomprar l o

probabilidad marginal

palquilarpingresosjalquilarpahorrosalquilar phijosalquilar

posterioralquilar 

probabilidad marginal



posterior de comprar es lo que queremos hallar pcomprardatos explicaremos los demás

 pcomprar es la probabilidad que ya tenemos es sencillamente el número de veces que se
selecciona comprar en nuestro conjunto de datos dividido el total de observaciones en
nuestro caso luego lo veremos en python son 

 plingresoscomprarpahorroscomprarphijoscomprar es la verosimilitud los nombres

gaussian y naive ingenuo del algoritmo vienen de dos suposiciones
 asumimos que las características de la verosimilitud no estan correlacionada entre ellas

esto seria que los ingresos sean independientes a la cantidad de hijos y de los ahorros
como no es siempre cierto y es una suposición ingenua es que aparece en el nombre
naive bayes

 asumimos que el valor de las características ingresos hijos etc tendrá una distribución
normal gaussiana esto nos permite calcular cada parte pingresoscomprar usando la

función de probabilidad de densidad normal
e probabilidad marginal muchas veces es difícil de calcular sin embargo por la ecuación que

vimos más arriba no la necesitaremos para obtener nuestro valor a posterior esto simplifica
los cálculos

fin de la teoría sigamos con el ejercicio ahora toca visualizar nuestras entradas y programar un
poquito

visualización de datos

veamos qué cantidad de muestras de comprar o alquilar tenemos

 httpwwwaprendemachinelearningcomwpcontentuploadsbayesposterioripng
 httpseswikipediaorgwikidistribucicbnnormal
httpseswikipediaorgwikifuncicbndedensidaddeprobabilidad

naive bayes comprar casa o alquilar 

printdataframe groupby comprarsize

comprar dtype int

esto son entradas en las que se recomienda comprar y en las que no hagamos un histograma
de las características quitando la columna de resultados comprar

dataframe drop comprar axishist
pltshow

ahorros estadocivil financiar







pareciera a grandes rasgos que la distribución de hijos e ingresos se parece un poco a una distribución
normal

preparar los datos de entrada

procesemos algunas de estas columnas por ejemplo podríamos agrupar los diversos gastos también
crearemos una columna llamada financiar que será la resta del precio de la vivienda con los ahorros
de la familia

 httpwwwaprendemachinelearningcomwpcontentuploadsbayeshistogrampng

naive bayes comprar casa o alquilar

ne

a

pagocoche

dataframe financiar dataframe vivienda dataframe ahorros



dataframe gastos dataframe gastoscomunes data frame gastosotros dataframe y

 dataframedrop gastoscomunes gastosotrospagocoche axishead



ingresos












ahorros





















vivienda estadocivil













o o



hijos trabajo comprar

e o o

o o











o



gastos












financiar























y ahora veamos un resumen estadístico que nos brinda la librería pandas con describe

 reduced dataframedrop gastoscomunes gastosotros pagocoche axis

 reduceddescribe

ingresos ahorros vivienda estado civil hijos trabajo comprar gastos financiar
count 
mean 
std 
min 
 
 
 
max 

httpwwwaprendemachinelearningcomwpcontentuploadsbayespreprocesapng
 httpwwwaprendemachinelearningcomwpcontentuploadsbayesstatspng

d



l

d



l

naive bayes comprar casa o alquilar 

feature selection ó selección de características

en este ejercicio haremos feature selection para mejorar nuestros resultados con este algoritmo

en vez de utilizar las columnas de datos de entrada que tenemos vamos a utilizar una clase de
sklearn llamada selectkbest con la que seleccionaremos las mejores características y usaremos
sólo esas

xdataframe drop comprar axis
ydataframe comprar 

bestselectkkbest k

xnew bestfittransformx y
xnewshape

selected bestgetsupport indicestrue
printxcolumns selected

index ingresos ahorros hijos trabajo financiar dtypeobject

ien u ísti u ue má 
bien entonces usaremos de las características que teníamos las que más aportan al momento
de clasificar veamos qué grado de correlación tienen

usedfeatures xcolumnsselected

colormap pltcmviridis

pltfigure figsize

plttitle pearson correlation of features y size

sbheatmap dataframe usedfeatures astype floatcorrlinewidthsvmax sqn

uaretrue cmapcolormap linecolorwhite annottrue

httpscikitlearnorgstablemodulesfeatureselectionhtml

instalar el ambiente de desarrollo
python

para programar tu propia máquina de inteligencia artificial necesitarás tener listo tu ambiente de
desarrollo local en tu computadora de escritorio o portátil en este capítulo explicaremos una manera
sencilla de obtener python y las librerías necesarias para programar como un científico de datos y
poder utilizar los algoritmos más conocidos de machine learning

por qué instalar python y anaconda en mi ordenador

python es un lenguaje sencillo rápido y liviano y es ideal para aprender experimentar practicar y
trabajar con machine learning redes neuronales y aprendizaje profundo

utilizaremos la suite gratuita de anaconda que nos facilitará la tarea de instalar el ambiente e
incluye las jupyter notebooks que es una aplicación web que nos ayudará a hacer ejercicios paso
a paso en machine learning visualizacion de datos y escribir comentarios tal como si se tratase de
un cuaderno de notas de la universidad

esta suite es multiplataforma y se puede utilizar en windows linux y macintosh
agenda
nuestra agenda de hoy incluye

 descargar anaconda

 instalar anaconda

 iniciar y actualizar anaconda

 actualizar paquete scikitlearn

 instalar librerías para deep learning

comencemos

 descargar anaconda

veamos como descargar anaconda a nuestro disco y obtener esta suite científica de python

naive bayes comprar casa o alquilar

pearson correlation of features

f 


financiar

aho financiar



con esto comprobamos que en general están poco correlacionadas sin embargo también tenemos 
valores de esperemos que el algoritmo sea lo suficientemente naive para dar buenos resultados


crear el modelo gaussian naive bayes con sklearn

primero vamos a dividir nuestros datos de entrada en entrenamiento y test

httpwwwaprendemachinelearningcomwpcontentuploadsbayescorrelationpng

ne

a

ne



l







naive bayes comprar casa o alquilar 

 split dataset in training and test datasets

xtrain x test traintestsplitdataframe testsize randomstate
ytrain x traincomprar

ytest x testcomprar

y creamos el modelo lo ponemos a aprender con fit y obtenemos predicciones sobre nuestro
conjunto de test

 instantiate the classifier
gnb gaussiannb
 train classifier
gnbfit
xtrainusedfeatures values
y train

y pred gnbpredictxtest usedfeatures 

print precisión en el set de entrenamiento f

 format gnbscorextrainusedfeatures ytrain
print precisión en el set de test f

 formatgnbscorextest usedfeatures ytest

precisión en el set de entrenamiento 
precisión en el set de test 

pues hemos obtenido un bonito de aciertos en el conjunto de test con nuestro querido
clasificador bayesiano también puedes ver los resultados obtenidos aplicando pca en este otro
capítulo

probemos el modelo comprar o alquilar

ahora hagamos predicciones para probar nuestra máquina

 en un caso será una familia sin hijos con de ingresos que quiere comprar una casa de
 y tiene sólo ahorrados

 el otro será una familia con hijos con ingresos por al mes en ahorros y consultan
si comprar una casa de 

 httpwwwaprendemachinelearningcomcomprendeprincipalcomponentanalysis

ne

a

naive bayes comprar casa o alquilar 

a ingresos ahorros hijos trabajo financiar
printgnbpredict 
 

resultado esperado alquilar comprar casa

 

los resultados son los esperados en el primer caso recomienda alquilar y en el segundo comprar
la casa 

resumen

a lo largo del artículo repasamos el teorema de bayes y vimos un ejemplo para aplicarlo en

una toma de decisiones pero no olvidemos que en el proceso también hicimos preprocesamiento
de los datos visualizaciones y selección de características durante diversas charlas que tuve con
profesionales del data science en mi camino de aprendizaje sale un mismo mensaje que dice no
es tan importante el algoritmo a aplicar si no la obtención y preprocesado de los datos que se van a
utilizar naive bayes como clasificador se utiliza mucho en nlp natural language processing
tanto en el típico ejemplo de detectar spam o no en tareas más complejas como reconocer
un idioma o detectar la categoría apropiada de un artículo de texto también puede usarse para
detección de intrusiones o anomalías en redes informáticas y para diagnósticos médicos dados unos
síntomas observados por último veamos los pros y contras de utilizar gaussian naive bayes

 pros es rápido simple de implementar funciona bien con conjunto de datos pequeños va bien
con muchas dimensiones features y llega a dar buenos resultados aún siendo ingenuo sin
que se cumplan todas las condiciones de distribución necesarias en los datos

 contras requiere quitar las dimensiones con correlación y para buenos resultados las entradas
deberían cumplir las suposiciones de distribución normal e independencia entre sí muy difícil
que sea así ó deberíamos hacer transformaciones en lo datos de entrada

recursos adicionales
 el código lo puedes ver en mi cuenta de github ó 
 lo puedes descargar desde aquí jupyter notebook ejercicio bayes python code

 descarga el archivo csv de entrada compraralquilarcsv

otros artículos de interés sobre bayes y python en inglés

 httpseswikipediaorgwikiteoremadebayes
ohttpwwwaprendemachinelearningcomprocesamientodellenguajenaturalnlp
httpsgithubcomjbagnatomachinelearning
httpwwwaprendemachinelearningcomwpcontentuploadsejerciciobayesipynb
 httpwwwaprendemachinelearningcomwpcontentuploadscompraralquilarcsv

naive bayes comprar casa o alquilar 

 naive bayes classifier from scratch

 naive bayes classification with sklearn
in depth naive bayes classification

 bayesian statistic for data science
feature selection 

 comprende principal component analysis





httpschrisalboncommachinelearningnaivebayesnaivebayesclassifierfromscratch
shttpsblogsicaracomnaivebayesclassifiersklearnpythonexampletipse
shttpsjakevdpgithubiopythondatasciencehandbooknaivebayeshtml
httpstowardsdatasciencecombayesianstatisticsfordatascienceecc
httpscikitlearnorgstablemodulesfeatureselectionhtml

 httpwwwaprendemachinelearningcomcomprendeprincipalcomponentanalysis

sistemas de recomendación

crea en python un motor de recomendación con collaborative
filtering

una de las herramientas más conocidas y utilizadas que aportó el machine learning fueron
los sistemas de recomendación son tan efectivos que estamos invadidos todos los días por
recomendaciones sugerencias y productos relacionados aconsejados por distintas apps webs y
correos

sin dudas los casos más conocidos de uso de esta tecnología son netflix acertando en recomendar
series y películas spotify sugiriendo canciones y artistas ó amazon ofreciendo productos de
venta cruzada sospechosamente muy tentadores para cada usuario

pero también google nos sugiere búsquedas relacionadas android aplicaciones en su tienda y
facebook amistades o las típicas lecturas relacionadas en los blogs y periódicos

todo ecommerce que se precie de serlo debe utilizar esta herramienta y si no lo hace estará
perdiendo una ventaja competitiva para potenciar sus ventas

qué son los sistemas ó motores de recomendación

i ión a v a 
los sistemas de recomendación a veces llamados en inglés recommender systems son
algoritmos que intentan predecir los siguientes ítems productos canciones etc que
querrá adquirir un usuario en particular

antes del machine learning lo más común era usar rankings ó listas con lo más votado ó más
popular de entre todos los productos entonces a todos los usuarios se les recomendaba lo mismo
es una técnica que aún se usa y en muchos casos funciona bien por ejemplo en librerías ponen
apartados con los libros más vendidos best sellers pero y si pudiéramos mejorar eso si hubiera
usuarios que no se guían como un rebaño y no los estamos reteniendo

los sistemas de recomendación intentan personalizar al máximo lo que ofrecerán a cada
usuario esto es ahora posible por la cantidad de información individual que podemos recabar de
las personas y nos da la posibilidad de tener una mejor tasa de aciertos mejorando la experiencia
del internauta sin ofrecer productos a ciegas

ohttpswwwaprendemachinelearningcomqueesmachinelearning

sistemas de recomendación 

tipos de motores

entre las estrategias más usadas para crear sistemas de recomendación encontramos

 itv sopularidad uctos s vendidos

popularity aconseja por la popularidad de los productos por ejemplo los más vendidos
globalmente se ofrecerán a todos los usuarios por igual sin aprovechar la personalización es
fácil de implementar y en algunos casos es efectiva

 contentbased a partir de productos visitados por el usuario se intenta adivinar qué busca
el usuario y ofrecer mercancías similares

 colaborative es el más novedoso pues utiliza la información de masas para identificar
perfiles similares y aprender de los datos para recomendar productos de manera individual

en este artículo comentaré mayormente el collaborative filtering y realizaremos un ejercicio en
python

cómo funciona collaborative filtering

para explicar cómo funciona collaborative filtering vamos a entender cómo será el dataset

ejemplo de dataset

necesitaremos ítems y las valoraciones de los usuarios los ítems pueden ser canciones películas
productos ó lo que sea que queremos recomendar

entonces nos quedará una matriz de este tipo donde la intersección entre fila y columna es una
valoración del usuario

sistemas de recomendación 

en esta gráfica educativa tenemos una matriz con productos a la izquierda y los ítems arriba
en este ejemplo los ítems serán frutas y cada celda contiene la valoración hecha por cada usuario
de ese ítem las casillas vacías significa que el usuario aún no ha probado esa fruta

entonces veremos que tenemos huecos en la tabla pues evidentemente no todos los usuarios tienen
o valoraron todos los ítems por ejemplo si los ítems fueran películas es evidente que un usuario
no habrá visto todas las películas del mundo entonces esos huecos son justamente los que con
nuestro algoritmo rellenaremos para recomendar ítems al usuario

una matriz con muchas celdas vacías se dice en inglés que es sparce y suele ser normal en
cambio si tuviéramos la mayoría de las celdas cubiertas con valoraciones se llamará dense

tipos de collaborative filtering

 userbased este es el que veremos a continuación
 se identifican usuarios similares
 se recomiendan nuevos ítems a otros usuarios basado en el rating dado por otros usuarios
similares que no haya valorado este usuario
 titembased
 calcular la similitud entre items

 encontrar los mejores items similares a los que un usuario no tenga evaluados y
recomendárselos

 httpdbaoraclecomoraclenewswhatssparsity htm

sistemas de recomendación 

predecir gustos userbased

collaborative filtering intentará encontrar usuarios similares para ofrecerle ítems bien valorados
para ese perfil en concreto lo que antes llamé rellenar los huecos en la matriz hay diversas
maneras de medir ó calcular la similitud entre usuarios y de ello dependerá que se den buenas
recomendaciones pero tengamos en cuenta que estamos hablando de buscar similitud entre gustos
del usuario sobre esos ítems me refiero a que no buscaremos perfiles similares por ser del mismo
sexo edad ó nivel educativo sólo nos valdremos de los ítems que ha experimentado valorado y
podría ser su secuencia temporal para agrupar usuarios parecidos

una de las maneras de medir esa similitud se llama distancia por coseno de los vectores y por

simplificar el concepto digamos que crea un espacio vectorial con n dimensiones correspondientes
a los n items y sitúa los vectores siendo su medida el valor rating de cada usuario a ese item
luego calcula el ángulo entre los vectores partiendo de la coordenada cero a poca distancia
entre ángulos se corresponde con usuarios con mayor similitud

este método no es siempre es perfecto pero es bastante útil y rápido de calcular

calcular los ratings

una vez que tenemos la matriz de similitud nos valdremos de otra operación matemática

calcular las recomendaciones

n

ru rusu ísu

u u 

formula para calcular los ratings faltantes sería algo así como matriz de similitud prodvectorial
ratings sumatoria de cada fila de ratings transpuesta

para

lo que haremos es cada rating se multiplica por el factor de similitud de usuario que dio el rating
la predicción final por usuario será igual a la suma del peso de los ratings dividido por la suma
ponderada

bueno no te preocupes que este cálculo luego lo verás en código y no tiene tanto truco

httpblogchristianperonecommachinelearningcosinesimilarityforvectorspacemodelspartiii

 httpsrealpythoncombuildrecommendationenginecollaborativefiltering
 httpswwwaprendemachinelearningcomwpcontentuploadsratingponderadopng

d



l

d



o

sistemas de recomendación 

ejercicio en python sistema de recomendación de
repositorios github

vamos a crear un motor de recomendación de repositorios github es la propuesta que hago en
el blog porque los recomendadores de música películas y libros ya están muy vistos

contaremos con un set de datos limitado pequeño pero podremos editar e ir agregando usuarios
y repositorios para mejorar las sugerencias

vamos al código

cargamos las librerías que utilizaremos

import pandas as pd

import numpy as np

from sklearnmetrics import meansquarederror

from sklearnmodelselection import traintestsplit
from sklearnneighbors import nearestneighbors
import matplotlibpyplot as plt

import sklearn

cargamos y previsualizamoás los archivos de datos csv que utilizaremos

dfusers pdreadcsv userscsv
dfrepos pdreadcsvreposcsv
dfratings pdreadcsvratingscsv
printdfusershead 
printdfrepos head 
printdfratingshead

userid username name
o iris isabel ruiz buriticá
 dianaciarke diana
 nateprewitt nate prewitt
 oldani ordanis sanchez
 waflessnet waflessnet



 httpswwwaprendemachinelearningcomwpcontentuploadsrecommenddfuserspng

n

sistemas de recomendación 

repold title categories stars
 airbnb javascript completar nan
 kamranahmedse developerroadmap roadmap to becoming a web developer in 
 microsoft vscode visual studio code 
 torvalds linux linux kernel source tree 
 ytdiorg youtubedl commandline program to download videos from y 
userid repold rating
o 
 
 
 
 

vemos que tenemos un archivo con la información de los usuarios y sus identificadores un archivo
con la información de los repositorios y finalmente el archivo ratings que contiene la valoración
por usuario de los repositorios como no tenemos realmente una valoración del al como
podríamos tener por ejemplo al valorar películas la columna rating es el número de usuarios que
tienen ese mismo repositorio dentro de nuestra base de datos sigamos explorando para comprende
un poco mejor

nusers dfratingsuseridunique shape 

nitems dfratingsrepolidunique shape 

l

print strnusers users

l

print strnitems items

 users
 items

vemos que es un dataset reducido pequeño tenemos usuarios y repositorios valorados

plthistdfratingsratingbins

 httpswwwaprendemachinelearningcomwpcontentuploadsrecommenddfrepospng
httpswwwaprendemachinelearningcomwpcontentuploadsrecommenddfratingspng

instalar el ambiente de desarrollo python 

nos dirigimos a la home de anaconda e iremos a la sección de download descargas

elegimos nuestra plataforma windows mac o linux

anaconda installers

windows macos é linux 

bit graphical installer mb bit graphical installer mb bit x installer mb

bit graphical installer mb bit command line installer mb bit power and power installer 
mb

bit graphical installer mb bit graphical installer mb

bit x installer mb
bit graphical installer mb bit command line installer mb

bit power and power installer 
mb

atención elegir la versión de python y no la de y seleccionar el instalador gráfico
graphical installer

con esto guardaremos en nuestro disco duro unos mb según sistema operativo y obtendremos
un archivo con el nombre similar a anacondamacosxxpkg

 instalar anaconda

en este paso instalaremos la app en nuestro sistema deberá tener permisos de administrador si
instala para todos los usuarios

ejecutamos el archivo que descargamos haciendo doble click
se abrirá un típico wizard de instalación

seguiremos los pasos podemos seleccionar instalación sólo para nuestro usuario seleccionar la ruta
en disco donde instalaremos y listo

al instalarse el tamaño total podrá superar gb en disco
 iniciar y actualizar anaconda

en este paso comprobaremos que se haya instalado correctamente y verificaremos tener la versión
más reciente

httpswwwanacondacomproductsindividualtdownload

o ojiaaada bafb




sistemas de recomendación 

 



tenemos más de valoraciones con una puntuación de y unas con puntuación en veamos
las cantidades exactas

dfratings groupby rating userid count

rating

















name userld dtype int

o i oaññswln 

plthistdfratingsgroupby repoid repoid countbins

 httpswwwaprendemachinelearningcomwpcontentuploadsrecommend ratingshistpng

sistemas de recomendación 

 



aquí vemos la cantidad de repositorios y cuantos usuarios los siguen la mayoría de repos los
tiene sólo usuario y no los demás hay unos que los tienen usuarios y unos que coinciden
 usuarios la suma total debe dar 

creamos la matriz usuariosratings
ahora crearemos la matriz en la que cruzamos todos los usuarios con todos los repositorios

dfmatrix pdpivottabledfratings valuesrating indexuserlid columnsrepx
oldfillna
dfmatrix

 httpswwwaprendemachinelearningcomwpcontentuploadsrecommendreposhistpng

rn 

sistemas de recomendación

repold 
userld

 
 
 
 

 

o a r n

 
 
 
 

 































vemos que rellenamos los huecos de la matriz con ceros y esos ceros serán los que deberemos

reemplazar con las recomendaciones

sparcity

veamos el porcentaje de sparcity

que tenemos
ratings df matrixvalues

sparsity floatlenratingsnonzero
sparsity ratingsshape ratingsshape
sparsity 

printsparsity fformat sparsity

sparsity 

esto serán muchos ceros que rellenar predecir

ohttpswwwaprendemachinelearningcomwpcontentuploadsusersrepomatrixpng

 httpswwwwquoracomwhatisaclear explanationofdatasparsity

d

a

ne

a

sistemas de recomendación 

dividimos en train y test set

separamos en train y test para más adelante poder medir la calidad de nuestras recomendaciones

ratingstrain ratingstest traintestsplitratings testsize randomstat x
e

printratingstrainshape

printratingstest shape

 
 

matriz de similitud distancias por coseno
ahora calculamos en una nueva matriz la similitud entre usuarios

simmatrix sklearnmetricspairwisecosinedistancesratings
printsimmatrixshape

 

pltimshowsimmatrix
pltcolorbar
pltshow

httpswwwaprendemachinelearningcomclasificacioncon datos desbalanceados

o o ojiduadabpaa t 

sistemas de recomendación 

o 



cuanto más cercano a mayor similitud entre esos usuarios

predicciones ó llamémosle sugeridos para ti

separar las filas y columnas de train y test
simmatrixtrain sim matrix
simmatrixtest simmatrix

userspredictions simmatrixtraindotratingstrain nparray npabssimmatriwv
xtrainsumaxist

pltrcparams figurefigsize 
pltimshowuserspredictions
pltcolorbar

pltshow

shttpswwwaprendemachinelearningcomwpcontentuploadssimilitudmatrixplotpng

o oiddaaaafsaa n 



d e o n 

sistemas de recomendación 









 





 

vemos pocas recomendaciones que logren puntuar alto la mayoría estará entre y puntos esto
tiene que ver con nuestro dataset pequeño

vamos a tomar de ejemplo mi usuario de github que es jbagnato

usuarioejemplo jbagnato
data dfusersdfusersusername usuario ejemplo
usuariover datailocuserid resta para obtener el index de pandas

userouserspredictionsargsort usuariover

 veamos los tres recomendados con mayor puntaje en la predic para este usuario
for i akepo in enumerateuser
selrepo dfreposdfreposrepolidarepo

l

printselrepotitle puntaje userspredictions usuariover arepo

 ytdlorg youtubedl
name title dtype object puntaje 
 dipanjans practicalmachinelearningwithpy
name title dtype object puntaje 
 abhat datasciencecheatsheet
name title dtype object puntaje 

vemos que los tres repositorios con mayor puntaje para sugerir a mi usuario son el de data
sciencecheatsheet con una puntuación de practicalmachinelearningwithpy con 
y youtubedl con no son puntuaciones muy altas pero son buenas sugerencias

validemos el error



sobre el test set comparemos la métrica mean squared error con el conjunto de entrenamiento

 httpswwwaprendemachinelearningcomwpcontentuploadsrecommendtrainpredictionspng
shttpsgithubcomjbagnatomachinelearning
 httpswwwstatisticshowtodatasciencecentralcommeansquarederror

ne



l









sistemas de recomendación 

def get msepreds actuals
if predsshape actualsshape
actuals actualst
preds predsactualsnonzeroflatten
actuals actualsactualsnonzero flatten
return meansquarederror preds actuals

getmseuserspredictions ratingstrain

 realizo las predicciones para el test set

userspredictionstest simmatrixdotratings nparray npabssimmatrixsum x
axist

userspredictionstest userspredictionstest

getmseuserspredictionstest ratingstest




vemos que para el conjunto de train y test el mae es bastante cercano un indicador de que no tiene
buenas predicciones sería si el mae en test fuera veces más ó la mitad del valor del de train

hay más

en la notebook completa en github encontrarás más opciones de crear el recomendador
utilizando knearest neighbors como estimador y también usando la similitud entre ítems ítem
based sin embargo para los fines de este artículo espero haber mostrado el funcionamiento básico
del collaborative filtering te invito a que luego lo explores por completo

resumen

vimos que es relativamente sencillo crear un sistema de recomendación en python y con machine
learning como muchas veces en data science una de las partes centrales para que el modelo
funcione se centra en tener los datos correctos y un volumen alto también es central el valor que
utilizaremos como rating siendo una valoración real de cada usuario ó un valor artificial que
creemos adecuado recuerda que me refiero a rating como ese puntaje que surge de la intersección
entre usuario e ítems en nuestro dataset luego será cuestión de evaluar entre las opciones de motores
userbased ítembased y seleccionar la que menor error tenga y no descartes probar en el mundo
real y ver qué porcentaje de aciertos o feedback te dan los usuarios reales de tu aplicación

shttpsgithubcomjbagnatomachinelearningblobmasterejerciciosistemasrecomendacionipynb
c httpswwwaprendemachinelearningcomclasificarconknearestneighborejemploenpython

sistemas de recomendación 

existen algunas librerías que se utilizan para crear motores de recomendación como la llamada
surprise
por último decir que como en casi todo el machine learning tenemos la opción de crear redes

neuronales con embeddings como recomendados y hasta puede que sean las que mejor funcionan
para resolver esta tarea

recursos del artículo
descarga los archivos csv y el notebook con el ejercicio python completo y adicionales

 userscsv

 teposcsv
 ratingscsv
e ejerciciosistemasderecomendación jupyter notebook





otros artículos de interés en inglés



build a recommendation engine
collaborative filtering and embeddings
how to build a simple songrecommendersystem
collaborative filtering with python

 machine learning for recommender system






s

 httpswwwaprendemachinelearningcompronosticodeventasredesneuronales pythonembeddings

ohttpsgithubcomjbagnatomachinelearningblobmasteruserscsv

 httpsgithubcomjbagnatomachinelearningblobmasterreposcsv

httpsgithubcomjbagnatomachinelearningblobmasterratingscsv

 httpsgithubcomjbagnatomachinelearningblobmasterejerciciosistemasrecomendacionipynb

httpsrealpythoncombuildrecommendation engine collaborativefiltering

 shttpstowardsdatasciencecomcollaborativefilteringandembeddings partbbce

 httpstowardsdatasciencecomhowtobuildasimplesongrecommenderfcbcc

 httpwwwsalemmaraficomcodecollaborativefilteringwithpython

 httpsmediumcomrecombeeblogmachinelearningforrecommendersystemspartalgorithmsevaluationandcoldstart
fded

breve historia de las redes
neuronales artificiales

arquitecturas y aplicaciones de las redes neuronales

vamos a hacer un repaso por las diversas estructuras inventadas mejoradas y utilizadas a lo largo de
la historia para crear redes neuronales y sacar el mayor potencial al deep learning para resolver
toda clase de problemas de regresión y clasificación

evolución de las redes neuronales en ciencias de la
computación

vamos a revisar las siguientes redesarquitecturas

 perceptron
 multilayer perceptron
 s
 neuronas sigmoidales
 redes feedforward
 backpropagation
 convolutional neural networks cnn recurent neural networks rnn
 long short term memory lstm
 deep belief networks dbn nace el deep learning
 restricted boltzmann machine
 encoder decoder autoencoder
 generative adversarial networks gan

si bien esta lista no es exhaustiva y no se abarcan todos los modelos creados desde los años he
recopilado las que fueron las redes y tecnologías más importantes desarrolladas para llegar al
punto en que estamos hoy el aprendizaje profundo

 httpwwwaprendemachinelearningcomaprendizajeprofundounaguiarapida
httpwwwaprendemachinelearningcomaplicacionesdelmachinelearning

breve historia de las redes neuronales artificiales 

el inicio de todo la neurona artificial

 perceptron

entre las décadas de y el científico frank rosenblatt inspirado en el trabajo de warren
mcculloch y walter pitts creó el perceptron la unidad desde donde nacería y se potenciarían las
redes neuronales artificiales un perceptron toma varias entradas binarias x x etc y produce una
sóla salida binaria para calcular la salida rosenblatt introduce el concepto de pesos w w
etc un número real que expresa la importancia de la respectiva entrada con la salida la salida de
la neurona será si la suma de la multiplicación de pesos por entradas es mayor o menor a un
determinado umbral sus principales usos son decisiones binarias sencillas o funciones lógicas como
or and

 httpwwwaprendemachinelearningcomwpcontentuploadsnetperceptronpng
httpseswikipediaorgwikifrankrosenblatt

instalar el ambiente de desarrollo python 

anaconda viene con una suite de herramientas gráficas llamada anaconda navigator iniciemos la
aplicación y veremos una pantalla como esta

o anaconda navigator

 anaconda navigator

 upgrade now

sign in to anaconda

m home m
applications on root v channels refresh
environments

o s e e





m projects beta jupyter q

n a
b ao notebook atconsole spyder

 

webbased interactive computing notebook
environment edit and run humanreadable

a community

docs while describing the data analysis

pyqt gui that supports inline figures proper
multiline editing with syntax highlighting
graphical calltips and more

scientific python development
environment powerful python ide with
advanced editing interactive testing
debugging and introspection features

launch launch launch
e e e
pe
documentation r
a
developer blog n
glueviz jupyterlab orange
feedback 
multidimensional data visualization across component based data mining framework
files explore relationships within and among data visualization and data analysis for
y a o related datasets novice and expert interactive workflows

entre otros íconos vemos que podemos lanzar las jupyter notebooks

para comprobar la instalación abrimos una terminal de maclinuxubuntu o la línea de comandos

de windows

escribimos

 conda v

y obtenemos la versión
conda 

luego tipeamos

 python v

y verificamos la versión de python de nuestro sistema

para asegurarnos de tener la versión más reciente de la suite ejecutaremos

breve historia de las redes neuronales artificiales 

 multilayer perceptron



como se imaginarán el multilayer perceptron es una amplicación del perceptrón de una única
neurona a más de una además aparece el concepto de capas de entrada oculta y salida pero con
valores de entrada y salida binarios no olvidemos que tanto el valor de los pesos como el de umbral
de cada neurona lo asignaba manualmente el científico cuantos más perceptrones en las capas
mucho más difícil conseguir los pesos para obtener salidas deseadas

los s aprendizaje automático

neuronas sigmoides

para poder lograr que las redes de neuronas aprendieran solas fue necesario introducir un nuevo
tipo de neuronas las llamadas neuronas sigmoides son similares al perceptron pero permiten que
las entradas en vez de ser ceros o unos puedan tener valores reales como ó ó lo que sea
también aparecen las neuronas bias que siempre suman en las diversas capas para resolver ciertas
situaciones ahora las salidas en vez de ser ó será dw x b donde d será la función sigmoide
definida como dz e esta es la primer función de activación

 httpwwwaprendemachinelearningcomwpcontentuploadsnetmultilayerpng

breve historia de las redes neuronales artificiales 

 e o 



imagen de la curva logística normalizada de wikipedia

con esta nueva fórmula se puede lograr que pequeñas alteraciones en valores de los pesos
deltas produzcan pequeñas alteraciones en la salida por lo tanto podemos ir ajustando muy
de a poco los pesos de las conexiones e ir obteniendo las salidas deseadas

redes feedforward

se les llama así a las redes en que las salidas de una capa son utilizadas como entradas en la próxima
capa esto quiere decir que no hay loops hacia atrás siempre se alimenta de valores hacia
adelante hay redes que veremos más adelante en las que sí que existen esos loops recurrent neural
networks además existe el concepto de fully connected feedforward networks y se refiere a que
todas las neuronas de entrada están conectadas con todas las neuronas de la siguiente capa

 backpropagation

gracias al algoritmo de backpropagation

se hizo posible entrenar redes neuronales de multiples
capas de manera supervisada al calcular el error obtenido en la salida e ir propagando hacia las capas
anteriores se van haciendo ajustes pequeños minimizando costo en cada iteración para lograr que

la red aprenda consiguiendo que la red pueda clasificar las entradas correctamente

 httpwwwaprendemachinelearningcomwpcontentuploadslogisticcurvepng
shttpseswikipediaorgwikipropagacicbnhaciaatrcas

breve historia de las redes neuronales artificiales 

 convolutional neural network



las convolutional neural networks

son redes multilayered que toman su inspiración del cortex vi
sual de los animales esta arquitectura es útil en varias aplicaciones principalmente procesamiento
de imágenes la primera cnn fue creada por yann lecun y estaba enfocada en el reconocimiento
de letras manuscritas la arquitectura constaba de varias capas que implementaban la extracción
de características y luego clasificación la imagen se divide en campos receptivos que alimentan
una capa convolutional que extrae features de la imagen de entrada por ejemplo detectar lineas
verticales vértices etc el siguiente paso es pooling que reduce la dimensionalidad de las features
extraídas manteniendo la información más importante luego se hace una nueva convolución y
otro pooling que alimenta una red feedforward multicapa la salida final de la red es un grupo de
nodos que clasifican el resultado por ejemplo un nodo para cada número del al es decir 
nodos se activan de a uno

esta arquitectura usando capas profundas y la clasificación de salida abrieron un mundo nuevo de
posibilidades en las redes neuronales las cnn se usan también en reconocimiento de video y tareas
de procesamiento del lenguaje natural

 httpwwwaprendemachinelearningcomwpcontentuploadsnetconvolutionalpng
 httpwwwaprendemachinelearningcomcomofuncionanlas convolutionalneuralnetworks visionporordenador
httpsenwikipediaorgwikiyannlecun

breve historia de las redes neuronales artificiales 

 long short term memory recurrent neural network



aqui vemos que la red lstm tiene neuronas ocultas con loops hacia atrás en azul esto permite
que almacene información en celdas de memoria

las long short term memory son un tipo de recurrent neural network esta arquitectura permite
conexiones hacia atrás entre las capas esto las hace buenas para procesar datos de tipo time
series datos históricos en se crearon las lstm que consisten en unas celdas de memoria
que permiten a la red recordar valores por períodos cortos o largos una celda de memoria contiene
compuertas que administran cómo la información fluye dentro o fuera la puerta de entrada
controla cuando puede entran nueva información en la memoria la puerta de olvido controla
cuanto tiempo existe y se retiene esa información la puerta de salida controla cuando la información
en la celda es usada como salida de la celda la celda contiene pesos que controlan cada compuerta el
algoritmo de entrenamiento conocido como backpropagationthroughtime optimiza estos pesos
basado en el error de resultado las lstm se han aplicado en reconocimiento de voz de escritura
texttospeech y otras tareas

 httpwwwaprendemachinelearningcomwpcontentuploadsnetistmpng

breve historia de las redes neuronales artificiales 

se alcanza el deep learning

 deep belief networks dbn



la deep belief network utiliza un autoencoder con restricted boltzmann machines para preentre
nar a las neuronas de la red y obtener un mejor resultado final

antes de las dbn en los modelos con profundidad decenas o cientos de capas eran
considerados demasiado difíciles de entrenar incluso con backpropagation y el uso de las redes
neuronales artificiales quedó estancado con la creación de una dbn que logro obtener un mejor
resultado en el dataset mnist se devolvió el entusiasmo en poder lograr el aprendizaje profundo
en redes neuronales hoy en día las dbn no se utilizan demasiado pero fueron un gran hito en la
historia en el desarrollo del deep learning y permitieron seguir la exploración para mejorar las redes
existentes cnn lstm etc las deep belief networks demostraron que utilizar pesos aleatorios
al inicializar las redes son una mala idea por ejemplo al utilizar backpropagation con descenso
por gradiente muchas veces se caía en mínimos locales sin lograr optimizar los pesos mejor será
utilizar una asignación de pesos inteligente mediante un preentrenamiento de las capas de la red
se basa en el uso de la utilización de restricted boltzmann machines y autoencoders para
preentrenar la red de manera no supervisada ojo luego de preentrenar y asignar esos pesos
iniciales deberemos entrenar la red de forma habitual supervisada con backpropagation ese
preentrenamiento es una de las causas de la gran mejora en las redes neuronales permitió el deep

 httpwwwaprendemachinelearningcomwpcontentuploadsdeepbeleifpng

 httpstensorflowrstudiocomtensorflowarticlestutorialmnistbeginnershtml
httpstowardsdatasciencecomdeeplearning meetsphysicsrestrictedboltzmann machinespartidfcc
httpstowardsdatasciencecomthevariationalautoencoderasatwoplayergameparticfb

 httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningnosupervisado

 httpwwwaprendemachinelearningcomaplicaciones delmachinelearningsupervisado

breve historia de las redes neuronales artificiales 

learning pues para asignar los valores se evalúa capa a capa de a una y no sufre de cierto sesgo
que causa el backpropagation al entrenar a todas las capas en simultáneo

 generative adversarial networks

httpwwwaprendemachinelearningcomwpcontentuploadsganpng

las gan entrenan dos redes neuronales en simultáneo la red de generación y la red de discrimi
nación a medida que la máquina aprende comienza a crear muestras que son indistinguibles de
los datos reales

estas redes pueden aprender a crear muestras de manera similar a los datos con las que las
alimentamos la idea detrás de gan es la de tener dos modelos de redes neuronales compitiendo
uno llamado generador toma inicialmente datos basura como entrada y genera muestras el
otro modelo llamado discriminador recibe a la vez muestras del generador y del conjunto de
entrenamiento real y deberá ser capaz de diferenciar entre las dos fuentes estas dos redes juegan
una partida continua donde el generador aprende a producir muestras más realistas y el
discriminador aprende a distinguir entre datos reales y muestras artificiales estas redes son
entrenadas simultáneamente para finalmente lograr que los datos generados no puedan diferenciarse
de datos reales sus aplicaciones principales son la de generación de imágenes artificiales realistas
pero también la de mejorar imágenes ya existentes o generar textos captions en imágenes o
generar textos siguiendo un estilo determinado y hasta para el desarrollo de moléculas para industria
farmacéutica

breve historia de las redes neuronales artificiales 

resumen

hemos recorrido estos primeros casi años de avances en las redes neuronales en la historia de la
inteligencia artificial se suele dividir en etapas del al en donde se pasó del asombro de estos
nuevos modelos hasta el escepticismo el retorno de un invierno de años cuando en los ochentas
surgen mejoras en mecanismos y maneras de entrenar las redes backpropagation y se alcanza
una meseta en la que no se puede alcanzar la profundidad de aprendizaje seguramente también
por falta de poder de cómputo y una tercer etapa a partir de en la que se logra superar esa
barrera y aprovechando el poder de las gpu y nuevas técnicas se logra entrenar cientos de capas
jerárquicas que conforman y potencian el deep learning y dan una capacidad casi ilimitada a estas
redes como último comentario me gustaría decir que recientemente feb surgieron nuevos
estudios de las neuronas humanas biológicas en las que se está redescubriendo su funcionamiento
y se está produciendo una nueva revolución pues parece que es totalmente distinto a lo que hasta
hoy conocíamos esto puede ser el principio de una nueva etapa totalmente nueva y seguramente
mejor del aprendizaje profundo el machine learning y la inteligencia artificial

más recursos

 cheat sheets for ap

 neural networks and deep learning
 from perceptrons to deep networks

 neural networks architectures

 a beginners guide to machine learning
 a guide on time series prediction using lstm

 convolutional neural networks in python with keras
 history of neural networl







 httpwwwaprendemachinelearningcomcrearunaredneuronalenpythondesdecero

 httpsmediumcomintuitionmachineneuronsaremorecomplexthanwhatwehaveimaginedbddadcd
 httpwwwaprendemachinelearningcomqueesmachinelearning

 httpsbecominghumanaicheatsheetsforaineuralnetworksmachinelearning deeplearningbigdatacbb
ottpneuralnetworksanddeeplearningcomchaphtmlperceptrons

 httpswwwtoptalcommachinelearninganintroductionto deeplearningfromperceptronstodeepnetworks
httpsmlcheatsheetreadthedocsioenlatestarchitectureshtml
shttpswwwibmcomdeveloperworkslibraryccbeginnerguide machinelearningaicognitiveindexhtml
httpsblogstatsbotcotimeseries predictionusingrecurrentneuralnetworksistmsfacaf

 httpswwwdatacampcomcommunitytutorialsconvolutionalneuralnetworkspython

httpsmediumcom okarthikeyanahistoryofneuralnetworkdfc

aprendizaje profundo una guía
rápida
deep learning y redes neuronales sin código

explicaré brevemente en qué consiste el deep learning ó aprendizaje profundo utilizado en
machine learning describiendo sus componentes básicos

nos centraremos en aprendizaje profundo aplicando redes neuronales artificiales

cómo funciona el deep learning mejor un ejemplo

el aprendizaje profundo es un método del machine learning que nos permite entrenar
una inteligencia artificial para obtener una predicción dado un conjunto de entradas esta
inteligencia logrará un nivel de cognición por jerarquías se puede utilizar aprendizaje
supervisado o no supervisado

explicaré cómo funciona el deep learning mediante un ejemplo teórico de predicción sobre quién
ganará el mundial de futbol utilizaremos aprendizaje supervisado mediante algoritmos de redes
neuronales artificiales para lograr las predicciones de los partidos de fútbol podemos tener las
siguientes entradas

 cantidad de partidos ganados

 cantidad de partidos empatados

 cantidad de partidos perdidos

 cantidad de goles a favor

 cantidad de goles en contra

 racha ganadora del equipo cant max de partidos ganados seguidos sobre el total jugado

y podríamos tener muchísimas entradas más la puntuación media de los jugadores del equipo o el
score que da la fifa al equipo como en cada partido tenemos a rivales deberemos incluir estos 
datos de entrada por cada equipo es decir entradas del equipo y otras del equipo dando un
total de entradas la predicción de salida será el resultado del partido local empate o visitante

httpwwwaprendemachinelearningcombrevehistoria delasredesneuronalesartificiales
httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado
httpwwwaprendemachinelearningcomaplicacionesdelmachinelearningnosupervisado

a httpwwwaprendemachinelearningcomprincipalesalgoritmosusadosenmachinelearningredneuronal

aprendizaje profundo una guía rápida 

creamos una red neuronal

en la programación tradicional escribiríamos código en donde indicamos reglas por ejemplo si
goles de equipo mayor a goles de equipo entonces probabilidad de local aumenta es decir que
deberíamos programar artesanalmente unas reglas de inteligencia bastante extensa e interrelacionar
las variables para posibles resultados para evitar todo ese enredo y hacer que nuestro código
sea escalaba y flexible a cambios recurrimos a las redes neuronales para describir una arquitectura
de interconexiones y capas y dejar que este modelo aprenda por sí mismo y descubra él mismo
relaciones entre variables que nosotros desconocemos

vamos a crear una red neuronal con valores de entrada input layer y con neuronas de salida
output layer las neuronas que tenemos en medio se llaman hidden layers y podemos tener
muchas cada una con una distinta cantidad de neuronas todas las neuronas estarán interconectadas
unas con otras en las distintas capas como vemos en el dibujo las neuronas son los círculos blancos

esquema de capas en una red neuronal

capa de capa capa capa de
entrada oculta oculta salida




 la capa de entrada recibe los datos de entrada y los pasa a la primer capa oculta
 las capas ocultas realizarán cálculos matemáticos con nuestras entradas uno de los desafíos

aprendizaje profundo una guía rápida 

al crear la red neuronal es decidir el número de capas ocultas y la cantidad de neuronas de

cada capa

 la capa de salida devuelve la predicción realizada en nuestro caso de resultados discretos
las salidas podrán ser para local para empate y para visitante

partidos
ganados

partidos
empatados

partidos
perdidos

goles
a favor

goles
en contra

racha
ganadora

red neuronal para la predicción
mundial rusia 

m v
wo 
n

k

xy n local
p m
y aó m c x a
n h 
oo 
á m w l á

 ya am w 
 q lx visitante
y ak whw 

 para simplificar ilustramos sólo entradas del equipo faltan otras entradas del rival equipo 

la cantidad total de capas en la cadena le da profundidad al modelo de aquí es que surge la
terminología de aprendizaje profundo

cómo se calcula la predicción

cada conexión de nuestra red neuronal está asociada a un peso este peso dictamina la importancia
que tendrá esa relación en la neurona al multiplicarse por el valor de entrada los valores iniciales
de peso se asignan aleatoriamente spoiler más adelante los pesos se ajustarán solos

o ia ek an 

aoooo aha
o i aada ik w dn ho yo

instalar el ambiente de desarrollo python

 conda update conda

s o jbagnato conda update conda conda conda update conda x

 conda update conda

fetching package metadata eee

solving package specifications 

package plan for installation in environment anaconda

the following packages will be updated

conda pyheda py
pycosat py pyhcce

proceed yn b

debemos poner y para confirmar y se descargarán luego ejecutamos

 conda update anaconda

para confirmar que todo funciona bien crearemos un archivo de texto para escribir un breve script

de python nombra al archivo versionespy y su contenido será

 scipy

import scipy

printscipy s scipy version

 numpy

import numpy

printnumpy s numpy version

 matplotlib

import matplotlib

printmatplotlib s matplotlib version
 pandas

import pandas

printpandas s pandas version

 statsmodels

import statsmodels

print statsmodels s statsmodelsversion
 scikitlearn

import sklearn

printsklearn s sklearn version

en la linea de comandos en el mismo directorio donde está el archivo escribiremos

aprendizaje profundo una guía rápida 

distribución de pesos en una red neuronal



imitando a las neuronas biológicas cada neurona tiene una función de activación esta función
determinará si la suma de sus valores recibidos previamente multiplicados por el peso de la
conexión supera un umbral que hace que la neurona se active y dispare un valor hacia la siguiente
capa conectada hay diversas funciones de activación conocidas que se suelen utilizar en estas
redes cuando todas las capas finalizan de realizar sus cómputos se llegará a la capa final con una
predicción por ejemplo si nuestro modelo nos devuelve está prediciendo que ganará

local con probabilidades será empate o que gane visitante 

entrenando nuestra red neuronal

entrenar nuestra ia puede llegar a ser la parte más difícil del deep learning necesitamos

 gran cantidad y diversidad de valores en nuestro conjunto de datos de entrada
 gran poder de cálculo computacional

 httpsenwikipediaorgwikiactivationfunction

aprendizaje profundo una guía rápida 

en nuestro ejemplo de predicción de partidos de futbol para el mundial deberemos crear una
base de datos con todos los resultados históricos de los equipos de fútbol en mundiales en partidos
amistosos en clasificatorios los goles las rachas a lo largo de los años etc

para entrenar nuestra máquina deberemos alimentarla con el conjunto de datos de entrada y
comparar el resultado local empate visitante contra la predicción obtenida como nuestro modelo
fue inicializado con pesos aleatorios y aún está sin entrenar las salidas obtenidas seguramente serán
erróneas una vez que tenemos nuestro conjunto de datos comenzaremos un proceso iterativo
usaremos una función para comparar cuan buenomalo fue nuestro resultado contra el resultado
real esta función es llamada función coste idealmente queremos que nuestro coste sea cero
es decir sin error cuando el valor de la predicción es igual al resultado real del partido a medida
que entrena el modelo irá ajustando los pesos de interconexiones de las neuronas de manera
automática hasta obtener buenas predicciones a ese proceso de ir hacia atrás y venir por las capas
de neuronas se le conoce como backpropagation más detalle a continuación

cómo reducimos la función coste y mejoramos las
predicciones

para poder ajustar los pesos de las conexiones entre neuronas haciendo que el coste se aproxime a
cero usaremos una técnica llamada gradient descent esta técnica permite encontrar el mínimo de
una función en nuestro caso buscaremos el mínimo en la función coste funciona cambiando los
pesos en pequeños incrementos luego de cada iteración del conjunto de datos al calcular la derivada
o gradiente de la función coste en un cierto conjunto de pesos podremos ver en qué dirección
descender hacia el mínimo global aquí se puede ver un ejemplo de descenso de gradiente en 
dimensiones imaginen la dificultad de tener que encontrar un mínimo global en dimensiones

 httpsenwikipediaorgwikilossfunction
 httpsenwikipediaorgwikigradientdescent

aprendizaje profundo una guía rápida 

 gradiente
peso linicial 



coste mínimo
global

para minimizar la función de coste necesitaremos iterar por el conjunto de datos cientos de miles
de veces más por eso es tan importante contar con una gran capacidad de cómputo en
el ordenador en el que entrenamos la red la actualización del valor de los pesos se realizará
automáticamente usando el descenso de gradiente

esta es parte de la magia del aprendizaje profundo automático una vez que finalizamos de
entrenar nuestro predictor de partidos de futbol del mundial sólo tendremos que alimentarlo con
los partidos que se disputarán y podremos saber quién ganará el mundial

resumen

 el aprendizaje profundo utiliza algoritmos de redes neuronales artificiales que imitan el
comportamiento biológico del cerebro

 hay tipos de capas de neuronas de entrada ocultas y de salida

 las conexiones entre neuronas llevan asociadas un peso que denota la importancia del valor
de entrada en esa relación

 las neuronas aplican una función de activación para estandarizar su valor de salida a la
próxima capa de neuronas

aprendizaje profundo una guía rápida 

 para entrenar una red neuronal necesitaremos un gran conjunto de datos

 lterar el conjunto de datos y comparar sus salidas producirá una función coste que indicará
cuán alejado está nuestra predicción del valor real

 luego de cada iteración del conjunto de datos de entrada se ajustarán los pesos de las neuronas
utilizando el descenso de gradiente para reducir el valor de coste y acercar las predicciones a
las salidas reales

crear una red neuronal en python
desde cero

programaremos una red neuronal artificial en python sin utilizar librerías de terceros entrenare
mos el modelo y en pocas lineas el algoritmo podrá conducir por sí mismo un coche robot

para ello explicaremos brevemente la arquitectura de la red neuronal explicaremos el concepto
forward propagation y a continuación el de backpropagation donde ocurre la magia y aprenden
las neuronas

el proyecto

coche robot

este amigable coche robot arduino será a quien le implantaremos nuestra red neuronal para que
pueda conducir sólo evitando los obstáculos



vamos a crear una red neuronal que conduzca un coche de juguete arduino que más adelante

construiremos y veremos en el mundo real nuestros datos de entrada serán

 httpwwwaprendemachinelearningcombrevehistoriadelasredesneuronalesartificiales

 shttpwwwaprendemachinelearningcomprogramauncochearduinoconinteligenciaartificial
 httpneuralnetworksanddeeplearningcomchaphtml

 httpwwwaprendemachinelearningcomprogramauncochearduinoconinteligenciaartificial
 httpwwwaprendemachinelearningcomprogramauncochearduinoconinteligenciaartificial

crear una red neuronal en python desde cero 

 sensor de distancia al obstáculo

 si es no hay obstáculos a la vista

 si es se acerca a un obstáculo

 si es está demasiado cerca de un obstáculo
 posición del obstáculo izquierdaderecha

 el obstáculo es visto a la izquierda será 

 visto a la derecha será 

las salidas serán

 girar

 derecha izquierda 
e dirección

 avanzar retroceder 

la velocidad del vehículo podría ser una salida más por ejemplo disminuir la velocidad si nos
aproximamos a un objeto y podríamos usar más sensores como entradas pero por simplificar
el modelo y su implementación mantendremos estas entradas y salidas para entrenar la red
tendremos las entradas y salidas que se ven en la tabla

entrada entrada salida giro salida acción de la

sensor posición dirección salida

distancia obstáculo

 avanzar

 avanzar

 avanzar

 giro a la
izquierda

 giro a la
derecha

 avanzar

 retroceder

 retroceder

 retroceder

 avanzar

 avanzar

 avanzar

esta será la arquitectura de la red neuronal propuesta

crear una red neuronal en python desde cero 



en la imagen anterior y durante el ejemplo usamos la siguiente notación en las neuronas

 xi son las entradas
 ai activación en la capa 
 yi son las salidas

y quedan implícitos pero sin representación en la gráfica

 oj los pesos de las conexiones entre neuronas será una matriz que mapea la capa j a la j

 recordemos que utilizamos neurona extra en la capa y una neurona extra en la capa a
modo de bias no están en la gráfica para mejorar la precisión de la red neuronal dandole
mayor libertad algebraica

los cálculos para obtener los valores de activación serán
a gotx
a gotx
a gotx

nota la t indica matriz traspuesta para poder hacer el producto

en las ecuaciones la g es una función sigmoide que refiere al caso especial de función logística y
definida por la fórmula

gz ez

 httpwwwaprendemachinelearningcomwpcontentuploadsredneuronalcochepng
ohttpswwwquoracomwhatisbiasinartificialneuralnetworkshare
 httpseswikipediaorgwikifuncicbnsigmoide

crear una red neuronal en python desde cero 

funciones sigmoide

una de las razones para utilizar la función sigmoide función logística es por sus propiedades

matemáticas en nuestro caso sus derivadas cuando más adelante la red neuronal haga backpropa
gation para aprender y actualizar los pesos haremos uso de su derivada en esta función puede ser
expresada como productos de f y f entonces f t ft ft por ejemplo la función tangente y
su derivada arcotangente se utilizan normalizadas donde su pendiente en el origen es y cumplen
las propiedades

b
b
b

b
b
b
b

 e a 



imagen de la curva logística normalizada de wikipedia

forward propagation ó red feedforward

con feedforward nos referimos al recorrido de izquierda a derecha que hace el algoritmo de la
red para calcular el valor de activación de las neuronas desde las entradas hasta obtener los valores
de salida si usamos notación matricial las ecuaciones para obtener las salidas de la red serán

x x x x
zlayer x
alayer gzlayer
zlayer oalayer
y gtelayers

httpseswikipediaorgwikifuncicbnlogcadstica
 httpwwwaprendemachinelearningcomwpcontentuploadslogisticcurvepng

crear una red neuronal en python desde cero 

resumiendo tenemos una red tenemos entradas éstas se multiplican por los pesos de las
conexiones y cada neurona en la capa oculta suma esos productos y les aplica la función de
activación para emitir un resultado a la siguiente conexión concepto conocido en biología como
sinápsis química

como dijimos los pesos iniciales se asignan con valores entre y de manera
aleatoria el desafío de este algoritmo será que las neuronas aprendan por sí mismas
a ajustar el valor de los pesos para obtener las salidas correctas

backpropagation cómputo del gradiente

al hacer backpropagtion es donde el algoritmo itera para aprender esta vez iremos de derecha

a izquierda en la red para mejorar la precisión de las predicciones el algoritmo de backpropagation
se divide en dos fases propagar y actualizar pesos

fase propagar

esta fase implica pasos

 hacer forward propagation de un patrón de entrenamiento recordemos que es este es un
algoritmo supervisado y conocemos las salidas para generar las activaciones de salida de la red

 hacer backward propagation de las salidas activación obtenida por la red neuronal usando las
salidas y reales para generar los deltas error de todas las neuronas de salida y de las neuronas
de la capa oculta

fase actualizar pesos

para cada sinápsis de los pesos
 multiplicar su delta de salida por su activación de entrada para obtener el gradiente del peso
 substraer un porcentaje del gradiente de ese peso

el porcentaje que utilizaremos en el paso tiene gran influencia en la velocidad y calidad del
aprendizaje del algoritmo y es llamado learning rate ó tasa de aprendizaje si es una tasa muy
grande el algoritmo aprende más rápido pero tendremos mayor imprecisión en el resultado si es
demasiado pequeño tardará mucho tiempo y podría no acercarse nunca al valor óptimo

httpseswikipediaorgwikisinapsisqucadmica
httpneuralnetworksanddeeplearningcomchaphtml
shttpwwwaprendemachinelearningcomaplicacionesdelmachinelearningsupervisado

crear una red neuronal en python desde cero 

 gradiente
peso inicial 

jw

coste minimo
 global



en esta gráfica vemos cómo utilizamos el gradiente paso a paso para descender y minimizar el coste
total cada paso utilizará la tasa de aprendizaje learning rate que afectará la velocidad y calidad
de la red

deberemos repetir las fases y hasta que la performance de la red neuronal sea satisfactoria si
denotamos al error en el layer como di para nuestras neuronas de salida en layer la activación
menos el valor actual será usamos la forma vectorial

d alayer y
d ot d gzlayer
g zlayer alayer alayer

al fin aparecieron las derivadas nótese que no tendremos delta para la capa puesto que son los
valores x de entrada y no tienen error asociado el valor del costo que es lo que queremos minimizar
de nuestra red será

j alayer diayer 

usamos este valor y lo multiplicamos al learning rate antes de ajustar los pesos esto nos asegura
que buscamos el gradiente iteración a iteración apuntando hacia el mínimo global

selfweightsi learningrate layertdotdelta

nota el layer en el código es realmente a

 httpwwwaprendemachinelearningcomwpcontentuploadsdescensoporgradientepng

tuitea sobre el libro

por favor ayuda a juan ignacio bagnato hablando sobre el libro en twitter

el hashtag sugerido para este libro es aprendeml
descubre lo que otra gente dice sobre el libro haciendo clic en este enlace para buscar el hashtag en
twitter

aprendeml

instalar el ambiente de desarrollo python 
 python versionespy
y deberemos ver una salida similar a esta

scipy 
numpy 
matplotlib 
pandas 
statsmodels 
sklearn 

dd aañprraw lh 

 actualizar libreria scikitlearn

en este paso actualizaremos la librería más usada para machine learning en python llamada scikit
learn

en la terminal escribiremos

 conda update scikitlearn

 jpagnato conda update scikitlearn conda conda update scikitlearn x
the following new packages will be installed

imageio pyhd
libcxx heds
libexxabi hebd
libgfortran hfe

the following packages will be updated

astropy nppy pyhab

 bottleneck nppy pyhfa
hspy nppy nppy

 llvmlite py pyhdfed

 matplotlib nppy nppy
numba nppy nppyhe

 numexpr nppy nppy

 numpy py pyha
pandas nppy pyha

 pytables nppy nppy
pywavelets nppy pyhdea

 scikitimage nppy pyhee 
scikitlearn nppy nppy
scipy nppy nppy

 statsmodels nppy pyhddbf

proceed yn h

deberemos confirmar la actualización poniendo y en la terminal

podemos volver a verificar que todo es correcto ejecutando

n

o ad




























crear una red neuronal en python desde cero

el código de la red neuronal



veamos el código recuerda que lo puedes ver y descargar desde la cuenta de github del libro
primero declaramos la clase neuralnetwork

import numpy as np

def sigmoidx

return npexpx

def sigmoidderivadax

return sigmoidxsigmoidx

def tanhx

return nptanhx

def tanhderivadax
return x

class neuralnetwork

def init self layers activationtanh

if activation sigmoid

selfactiva
selfactiva
elif activation
selfactiva
selfactiva

tion sigmoid

tionprime sigmoidderivada
 tanh

tion tanh

tionprime tanhderivada

 inicializo los pesos

selfweights
selfdeltas
 capas 



 rando de pesos varia entre 

 asigno valores aleatorios a capa de entrada y capa oculta
for i in range
r nprandomrandom layersi layersi 

 lenlayers 

selfweightsappendr

 asigno aleatorios a capa de salida

r nprandomrandom layersi layersi 

httpsgithubcomjbagnatomachinelearning

crear una red neuronal en python desde cero 

def

ea

def

selfweightsappendr

fitself x y learningrate epochs

 agrego columna de unos a las entradas x

 con esto agregamos la unidad de bias a la capa de entrada
ones npatleastdnponesxshape

x npconcatenate onest x axis

for k in rangeepochs

i nprandom randintxshape

xi

a

for in range lenselfweights
dotvalue npdotal selfweights
activation selfactivationdotvalue
aappendactivation
 calculo la diferencia en la capa de salida y el valor obtenido
error yi a
deltas error selfactivationprimea

 empezamos en el segundo layer hasta el ultimo
 una capa anterior a la de salida
for in rangelena 
deltasappenddeltas dotselfweightstsel f activation primy

selfdeltasappenddeltas

 invertir
 levelutputlevelhidden levelhiddenleveloutput
deltasreverse 

 backpropagation
 multiplcar los delta de salida con las activaciones de entrada
h para obtener el gradiente del peso
 actualizo el peso restandole un porcentaje del gradiente
for i in range lenselfweights
layer npatleastdai
delta npatleastaddeltasi
selfweightsi learningrate layertdotdelta

if k printepochs k

predictself x

o i añfmsyos e















crear una red neuronal en python desde cero 

ones npatleastdnponesxshape
a npconcatenate nponest nparrayx axis
for in range lenselfweights

a selfactivationnpdota selfweights
return a

def printweightsself
printlistado pesos de conexiones
for i in range lenselfweights
printselfweightsi

def getdeltasself
return selfdeltas

y ahora creamos una red a nuestra medida con neuronas de entrada ocultas y de salida
deberemos ir ajustando los parámetros de entrenamiento learning rate y la cantidad de iteraciones
epochs para obtener buenas predicciones

 funcion coche evita obstáculos
nn neuralnetwork activation tanh
x nparray sin obstaculos

 sin obstaculos
 sin obstaculos
 obstaculo detectado a derecha
 obstaculo a izq
 demasiado cerca a derecha
 demasiado cerca a izq
y nparray avanzar
 avanzar
 avanzar
 giro izquierda
 giro derecha
 retroceder
 retroceder

nnfitx y learningrateepochs

index

for e in x
printxeyyindex networknnpredicte
indexindex

la salidas obtenidas son comparar los valores y con los de network

n

o

l

n

o

l









crear una red neuronal en python desde cero 

x y network 
x y network 
x y network 
x y network 
x y network 
x y network e e
x y network 

como podemos ver son muy buenos resultados aquí podemos ver como el coste de la función se
va reduciendo y tiende a cero

import matplotlibpyplot as plt

deltas nngetdeltas

valores

index

for arreglo in deltas
valores appendarreglo arreglo 
indexindex

pltplotrange lenvalores valores colorb
pltylim 

pltylabel cost

pltxlabel epochs

plttightlayout

pltshow

crear una red neuronal en python desde cero 



 

cost





 
epochs



y podemos ver los pesos obtenidos de las conexiones con nnprintweights pues estos valores
serán los que usaremos en la red final que en el próximo capítulo implementaremos en arduino
para que un cocherobot conduzca sólo evitando obstáculos

resumen

creamos una red neuronal en pocas líneas de código python

 comprendimos cómo funciona una red neuronal básica

 el porqué de las funciones sigmoides y sus derivadas que 
 nos permiten hacer backpropagation
 hallar el gradiente para minimizar el coste
 reducir el error iterando y obtener las salidas buscadas

 logrando que la red aprenda por sí misma en base a un conjunto de datos de entrada y sus
salidas como buen algoritmo supervisado que es

en el próximo capitulo finalizaremos el proyecto al aplicar esta red que construimos en el mundo
real y comprobar si un coche arduino será capaz de conducir por sí mismo y evitar obstáculos

 httpwwwaprendemachinelearningcomwpcontentuploadsdescensogradientepng
ohttpwwwaprendemachinelearningcomprogramauncochearduinoconinteligenciaartificial
 httpwwwaprendemachinelearningcomprogramauncochearduinoconinteligenciaartificial

crear una red neuronal en python desde cero 

recursos

 en donde se enseña la

 el código utilizado es una adaptación del original del bogotobogo
función xor
 pueden descargar el código de este artículo en un jupyter notebook aquí

ó pueden acceder a mi github

ovisualizar online

httpwwwbogotobogocompythonpythonneuralnetworksbackpropagation forxorusingonehiddenlayerphp
ttpwwwaprendemachinelearningcomwpcontentuploadsredneuronaldesdeceroipynb
httpnbviewer jupyterorggithubjbagnatomachinelearningblobmasterredneuronaldesdeceroipynb
httpsgithubcomjbagnatomachinelearning

programa un coche robot arduino
que conduce con ia

el machine learning nos permitirá utilizar redes neuronales para que un coche arduino

conduzca sólo evitando obstáculos

 httpwwwaprendemachinelearningcombrevehistoria delasredesneuronalesartificiales

programa un coche robot arduino que conduce con ia

t

con intelicencia
artificial

programa un coche robot arduino que conduce con ta 

en el capítulo anterior creamos una red neuronal desde cero en python en este artículo
mejoraremos esa red y copiaremos sus pesos a una red con propagación hacia adelante en arduino
que permitirá que el coche robot conduzca sólo sin chocar

la nueva red neuronal

por simplificar el modelo de aprendizaje en el capítulo anterior teníamos una red de tres capas
con neuronas de entrada ocultas y de salida giro y dirección para este ejercicio haremos que
la red neuronal tenga salidas una para cada motor además las salidas serán entre y apagar o
encender motor también cambiaremos las entradas para que todas comprendan valores entre y
 y sean acordes a nuestra función tangente hiperbólica aquí vemos los cambios en esta tabla

entrada entrada salida salida salida salida
sensor posición motor motor motor motor 
distancia obstáculo

 

 

 

 

 

 

 

 

 

 httpwwwaprendemachinelearningcomcrearunaredneuronalenpythondesdecero
httpwwwaprendemachinelearningcomcrearunaredneuronalenpythondesdecero
 httpseswikipediaorgwikitangentehiperbcblica

o ia eu n


























programa un coche robot arduino que conduce con ia 

siendo el valor de los motores y 

acción motor motor motor motor 
avanzar 
retroceder 
giro derecha 
giro izquierda 

para instanciar nuestra red ahora usaremos este código

 red coche para evitar obstáculos

nn neuralnetwork activation tanh

x nparray 
 
 
 






d

 sin obstaculos

 sin obstaculos

sin obstaculos

obstaculo detectado a derecha
obstaculo a izq

obstaculo centro

demasiado cerca a derecha
demasiado cerca a izq

h h h ohoheoh e

demasiado cerca centro

 las salidas y se corresponden con encender o no los motores

y mnparray 









d



 avanzar

 avanzar

avanzar

giro derecha

giro izquierda cambie izq y derecha
avanzar

retroceder

retroceder

h h h h ohoh o e

retroceder

nnfitx y learningrateepochs

def valnnx

return intabsroundx

index
for e in x

prediccion nnpredicte

printxe esperadoyindex obtenido valnnprediccionvalnnpr

ediccionvalnnprediccionvalnnprediccion 

indexindex

ne



o

d

a

instalar el ambiente de desarrollo python 

 python versionespy

 instalar librerías para deep learning

en este paso instalaremos las librerías utilizadas para aprendizaje profundo específicamente serán
keras y la famosa y querida tensorflow de google

para ello ejecutaremos en nuestra línea de comandos

 conda install c condaforge tensorflow

 pip install keras

y crearemos un nuevo script para probar que se instalaron correctamente le llamaremos versiones 
deeppy y tendrá las siguientes lineas

 tensorflow

import tensorflow

printtensorflow s tensorflow version 
 keras

import keras

printkeras s keras version 

ejecutamos en línea de comandos
 python versionesdeeppy

en la terminal veremos la salida
tensorflow 

using tensorflow backend

keras 

ya tenemos nuestro ambiente de desarrollo preparado para el combate

resumen

para nuestra carrera en machine learning y el perfeccionamiento como data scientist necesitamos
un buen entorno en el que programar y cacharrear lease probar cosas y divertirse para ello
contamos con la suite de herramientas gratuitas de anaconda que nos ofrece un entorno amable
y sencillo en el que crear nuestras máquinas en código python

programa un coche robot arduino que conduce con ia 

aquí podemos ver el código python completo modificado de la jupyter notebook y también
vemos la gráfica del coste que disminuye a medida que se entrena tras iteraciones



 

 

cost

 





 
epochs sa

no es impresionante cómo con apenas datos de entrada podemos enseñar a un robot
a conducir

el coche arduino

en mi caso es un coche arduino elegoo uno v de motores si eres maker te resultará fácil
construir el tuyo o puede que ya tengas uno en casa para programarlo el coche puede ser cualquier
otro de hecho podría ser de motores y modificando apenas la red funcionaría en caso de querer
construirlo tu mismo explicaré brevemente los requerimientos

necesitaremos

 una placa arduino uno y una placa de expansión de o
 o puede ser una placa arduino mega
 el controlador de motor ln

ohttpsgithubcomjbagnatomachinelearningblobmasterredneuronalcocheipynb

a httpwwwaprendemachinelearningcomwpcontentuploadsgraficacostemilpng

 httpswwwamazonesgpproductbpzhmtrefaslitlieutfétagaprendemlcamp creativelinkcode
ascreativeasinbpzhmtérlinkidaffceffaccae

programa un coche robot arduino que conduce con ia 

 motores dc o podrían ser y sus ruedas

servo motor sg

sensor ultrasónico

 baterias para alimentar los motores y la placa obviamente
chasis para el coche

 cables

circuito del coche

no entraré en detalle ya que va más allá de los alcances de este libro pero básicamente tenemos el
siguiente circuito ignorar el bluetooth y los sensores infrarrojos

fritzing

montar el coche

utilizaremos un servomotor en la parte delantera del coche que moverá al sensor ultrasónico de
distancia de izquierda a derecha a modo de radar para detectar obstáculos

más allá de eso es un coche pondremos las ruedas y las placas arduino encima del chasis el
objetivo de este capítulo es enseñar a programar una red neuronal en la ide de arduino

este es el video tutorial oficial de ensamblaje de elegoo de este coche

 httpwwwaprendemachinelearningcomwpcontentuploadswireconnectpng
 httpswwwyoutubecomwatchvwyuydvulistplkfeyzkrtzzoglvieipjosgonnkkeindexfragsplcwn

programa un coche robot arduino que conduce con ia 

así nos quedará montado

copiar la red neuronal

una vez obtenida la red neuronal python haremos copiar y pegar de la matriz de pesos en el
código arduino reemplazaremos las lineas y 

shttpwwwaprendemachinelearningcomwpcontentuploadscochearduinoiapng

programa un coche robot arduino que conduce con ia 

 jupyter red neuronaldesdecero q logout
fe edit view inset cel kemel hep trstod pyhon 
b m han ec ce e notily
disabled eho mi
 o 
epochs
he en s
he enb 
in def to strname w he n 
 strwtolistreplace replace meiv
return eloat namesstrwshapelostrimshap de to 
he t
executodin sms inished 
in obtenermos los pesos enfrenados para poder usarlos en el c
pesos nngetweigl 
int inputnodes incluye neurona de
 reemplazar estas lineas en tu codigo arduino int hiddennodes ncluye neurona de bias
 float hiddenweights int outputniodes 
 loat outputweighte ee

 con lo pesos entrenados
na

tostrhiddenveights pesos
printtostroutputweights pesos

 hiddenfhi ddennodes
 output outputnodes

 
 
enecuted in tims finished 

lee el artículo completo en ywwaprendemachineleamingcom

e 

copiamos los pesos que obtenemos en la jupyter notebook de nuestra red neuronal en el código
arduino reemplazando las variables por los nuevos valores

vittar a

el código arduino

el código arduino controlará el servo motor con el sensor de distancia que se moverá de izquierda
a derecha y nos proveerá las entradas de la red distancia y direcciónó giro

el resto lo hará la red neuronal en realidad la red ya aprendió en python es decir sólo
hará multiplicaciones y sumas de los pesos para obtener salidas realizará el camino forward
propagation y las salidas controlarán directamente los motores

hay código adicional para darle ciclos de tiempo a las ruedas a moverse variable accionencurso y
dar más o menos potencia a los motores al cambiar de dirección son relativamente pocas líneas de
código y logramos que la red neuronal conduzca el coche

nota para el movimiento del servomotor se utiliza una librería servo estándard

aquí vemos el código arduino completo tal vez la parte más interesante sea la función conducir

 httpwwwaprendemachinelearningcomwpcontentuploadscopyjupyterarduinogif

oo idíawn 

eobb b w wwwwwwwwwwlbnnnnnnnnennrhehrshehelrera ra
v d oooizaamaww nh ocggia afefg nh e o oxooiddaualmaw haeo

programa un coche robot arduino que conduce con ia

include servo library

servo myservo create servo object to control servo
int echo a
int trig a

define ena 
define enb 
define in 
define in 
define in 
define in 

a i i i i o i i

network configuration

aaa

const int inputnodes incluye neurona de bias
const int hiddennodes incluye neurona de bias
const int outputnodes 

int i j

double accum

double hiddenhiddennodes

double output outputnodes 
float hiddenweights 



 x
 y
 x
 

float outputweights n

end network configuration

aaa

void stop 
digitalwriteena low desactivamos los motores
digitalwriteenb low desactivamos los motores
serial printinstop

medir distancia en centimetros
int distancetest 

 n
 n
 on

 
a i i i i o i i

programa un coche robot arduino que conduce con ia

digitalwritetrig low
delaymicroseconds 
digitalwritetrig high
delaymicroseconds
digitalwritetrig low
float fdistance 
fdistance fdistance 
return intfdistance

void setup 
myservoattach
serialbegin
pinmodeecho input
pinmodetrig output
pinmode in output

pinmode in output
pinmode in output
pinmode in output
pinmodeena output
pinmodeenb output
stop
myservowrite
delay 

unsigned long previousmillis 
const long interval 
int gradosservo 

ultrasonico

bool clockwise true
const long angulomin 
const long angulomax 
double ditanciamaxima 
a actuar la nn
int incrementos 
vo
int accionencurso 
int multiplicador interval
mpo a que el coche pueda girar

const int speed 
vez

pulseinecho high

 para medir
 intervalos
 posicion

 sentido de



 attach servo on pin to servo object

posicion inicial en el centro

ciclos de tiempo
cada x milisegundos

del servo que mueve el sensor 

giro del servo

 distancia de lejania desde la que empieza

 incrementos por ciclo de posicion del ser 

 cantidad de ciclos ejecutando una accion

 multiplica

la cant de ciclos para dar tie x

 velocidad del coche de las ruedas a la

programa un coche robot arduino que conduce con ia

void loop 
unsigned long currentmillis millis

if currentmillis previousmillis interval 
previousmillis currentmillis

aaaobadadoaadaoeekaooeasaebssasob
manejar giro de servo
roassoeaaooeadaoeeassoasssedass 
ifgradosservoangulomin gradosservoangulomax 
clockwiseclockwise cambio de sentido
gradosservo constraingradosservo angulomin angulomax

ifclockwise
gradosservogradosservoincrementos
else
gradosservogradosservoincrementos

ifaccionencurso 
accionencursoaccionencurso

jelse
aa i i i i a i a i a

llamamos a la funcion de conduccion
aaa

conducir



myservowrite gradosservo 

usa la red neuronal ya entrenada
void conducir

double testinput 
double entradaentrada

aaa i i i i i i

obtener distancia del sensor
aaa

double distance doubledistancetest
distance doubleconstraindistance ditanciamaxima



entrada ditanciamaximadoubledistance uso una funcion lin

programa un coche robot arduino que conduce con ia 

neal para obtener cercania
accionencurso entrada multiplicador si esta muy cerca del y
obstaculo necestia mas tiempo de reaccion

aaa i i i i i i
obtener direccion segun angulo del servo

aaa
entrada mapgradosservo anculomin anculomax 
entrada doubleconstrainentrada 

aa i i i i a i a i a
llamamos a la red feedforward con las entradas

rrr

serialprintentrada

serialprintlnentrada

serialprintentrada

serial printlnentrada

testinput bias unit
testinput entrada
testinput entrada

inputtooutputtestinput testinput testinput input to ann to obt 
ain output

int out roundabsoutput
int out roundabsoutput
int out roundabsoutput
int out roundabsoutput

serialprintsalida
serialprintinout
serialprintsalida
serialprintinout
serialprintinoutput
serialprintsalida
serialprintlnout
serialprintsalida

serialprintinout

aaa i i i i i i

impulsar motores con la salida de la red
aaa

int carspeed speed hacia adelante o atras

programa un coche robot arduino que conduce con ia



ifoutut outut si es giro necesita doble fuerza los y

motores

carspeed speed 



analogwriteena carspeed
analogwriteenb carspeed
digitalwritein out high
digitalwritein out high
digitalwritein out high
digitalwritein out high

void inputtooutputdouble in double in double in



double testinput 
testinput in
testinput in
testinput in

aaa i i i i i i

calcular las activaciones en las capas ocultas
roassoeaaooeadaoeeassoasssedass 

for i i hiddennodes i 
accum hiddenweightsinputnodesi 
for j j inputnodes j 
accum testinputj hiddenweightsji 

hiddeni expaccum sigmoid
hiddeni tanhaccum tanh

aaa i i i i i i

calcular activacion y error en la capa de salida
roassoeaaooeadaoeeassoasssedass 

for i i outputnodes i 
accum utputweightshiddennodesi
for j j hiddennodes j 
accum hiddenj outputwweightsji 

outputi tanhaccum tanh

programa un coche robot arduino que conduce con ia 

el coche en acción

conecta tu coche sube el código y pruébalo



veamos un video del coche funcionando con su propia inteligencia artificial en el planeta tierra

resumen

aplicamos machine learning y sus redes neuronales a un objeto del mundo real y vimos cómo
funciona haciendo que el coche evite obstáculos y tome las decisiones por sí mismo sin haberle
dado instrucciones ni código explícito

mejoras a futuro

tengamos en cuenta que estamos teniendo como entradas los datos proporcionados por un sólo
sensor de distancia y un servo motor que nos indica si está a izquierda o derecha podríamos tener
más sensores de distancia infrarrojos medir velocidad luz sonido en fin si tuviéramos que
programar manualmente ese algoritmo tendría una complejidad enorme y sería muy difícil de
mantener o modificar en cambio hacer que una red neuronal aprenda sería muy sencillo tan sólo
agregaríamos features columnas a nuestro código y volveríamos a entrenar nuevamente la red con
las salidas deseadas voila copiar y pegar los pesos obtenidos en arduino y nuestro coche tendría
la inteligencia de manejarse por sí mismo nuevamente

recursos adicionales
 descarga el código python con la nueva red neuronalp

 descarga el código arduino

 si quieres comprar el mismo coche arduino que yo click aqui elegoo uno proyecto kit
de coche robot inteligente v te recuerdo que puedes construir tu propio coche arduino
si te das maña y utilizar este código de hecho puede funcionar con un coche de motores
modificando las salidas de la red en vez de tener tener 

 aquí puedes ver a otro maker que hizo un coche arduino con red neuronal que escapa de la

luz

 httpswwwyoutubecomwatchvbedqjktmusrfeatureemblogo

shttpsgithubcomjbagnatomachinelearningblobmasterredneuronalcocheipynb

 httpsgithubcomjbagnatomachinelearningtreemasternnobstaclecar

ohttpswwwamazonesgpproductbpzhmtrefaslitlieutfétagaprendemlcampcreativelinkcode
ascreativeasinbpzhmtérlinkidaffceffaccae

 httpswwwinstructablescomidarduinoneuralnetworkrobot

instalar el ambiente de desarrollo python

otros artículos de interés en inglés

usar anaconda navigator
instalar pip

instalar tensorflow
instalación de keras

httpsdocsanacondacomanacondanavigator
httpsrecursospythoncomguiasymanualesinstalacion y utilizaciondepipenwindowslinuxysx
httpswwwtensorfloworginstall

thttpskerasioinstallation

una sencilla red neuronal con keras
y tensorflow

crearemos una red neuronal artificial muy sencilla en python con keras y tensorflow para
comprender su uso implementaremos la compuerta xor y compararemos las ventajas del
aprendizaje automático frente a la programación tradicional

requerimientos para el ejercicio

puedes simplemente leer el código y comprenderlo o si quieres ejecutarlo deberás tener un ambiente
de desarrollo python como anaconda para ejecutar el jupyter notebook también funciona con
python en línea de comandos

las compuertas xor

para el ejemplo utilizaremos las compuertas xor si no las conoces o no las recuerdas funcionan
de la siguiente manera tenemos dos entradas binarias ó y la salida será sólo si una de las
entradas es verdadera y la otra falsa es decir que de cuatro combinaciones posibles sólo dos
tienen salida y las otras dos serán como vemos aquí

una red neuronal artificial sencilla con python y
keras

veamos el código completo en donde creamos una red neuronal con datos de entrada las 
combinaciones de xor y sus salidas ordenadas luego analizamos el código linea a linea

 httpwwwaprendemachinelearningcombrevehistoria delasredesneuronalesartificiales
ttpseswikipediaorgwikipuerta xor
httpwwwaprendemachinelearningcominstalarambientededesarrollopythonanacondaparaaprendizajeautomatico
ttpdataspeakslucadcompythonparatodosjupyternotebookhtml

httpseswikipediaorgwikipuerta xor

n

o

l

















una sencilla red neuronal con keras y tensorflow 

import numpy as np
from kerasmodels import sequential
from keraslayerscore import dense

 cargamos las combinaciones de las compuertas xor
trainingdata nparray float

 y estos son los resultados que se obtienen en el mismo orden
targetdata nparray float

model sequential

model adddense inputdim activationrelu

model adddense activationsigmoid

model compile lossmeansquarederror
optimizeradam
metricsbinaryaccuracy

model fittrainingdata targetdata epochs

 evaluamos el modelo
scores model evaluatetrainingdata targetdata

printwns f model metricsnames scores
print model predicttrainingdataround

keras y tensorflow what

utilizaremos keras que es una librería de alto nivel para que nos sea más fácil describir las capas
de la red que creamos y en background es decir el motor que ejecutará la red neuronal y la entrenará
estará la implementación de google llamada tensorflow que es la mejor que existe hoy en día

analicemos la red neuronal que hicimos

importamos las clases que utilizaremos

httpskerasio
httpswwwtensorfloworg

ne

a

d



o

ne

a

una sencilla red neuronal con keras y tensorflow 

import numpy as np
from kerasmodels import sequential
from keraslayerscore import dense

utilizaremos numpy para el manejo de arrays de keras importamos el tipo de modelo sequential y
el tipo de capa dense que es la normal creamos los arrays de entrada y salida

 cargamos las combinaciones de las compuertas xor
trainingdata nparray float

 y estos son los resultados que se obtienen en el mismo orden
targetdata nparray float

como se puede ver son las cuatro entradas posibles de la función xor y sus
cuatro salidas ahora crearemos la arquitectura de nuestra red neuronal

model sequential
model adddense inputdim activationrelu
model adddense activationsigmoid

creamos un modelo vació de tipo sequential este modelo indica que crearemos una serie de capas
de neuronas secuenciales una delante de otra agregamos dos capas dense con modeladdy
realmente serán capas pues al poner inputdim estamos definiendo la capa de entrada con 
neuronas para nuestras entradas de la función xor y la primer capa oculta hidden de neuronas
como función de activación utilizaremos relu que sabemos que da buenos resultados podría ser
otra función esto es un mero ejemplo y según la implementación de la red que haremos deberemos
variar la cantidad de neuronas capas y sus funciones de activación agregamos una capa con 
neurona de salida y función de activación sigmoid

visualización de la red neuronal

veamos que hemos hecho hasta ahora

 httpwwwaprendemachinelearningcombrevehistoriadelasredesneuronalesartificiales

una sencilla red neuronal con keras y tensorflow 

n








a entrenar la red

antes de de entrenar la red haremos unos ajustes de nuestro modelo
model compile lossmeansquarederror
optimizeradam

metricsbinaryaccuracy

con esto indicamos el tipo de pérdida loss que utilizaremos el optimizador de los pesos de las
conexiones de las neuronas y las métricas que queremos obtener ahora sí que entrenaremos la red

model fittrainingdata targetdata epochs

indicamos con modelfit las entradas y sus salidas y la cantidad de iteraciones de aprendizaje
epochs de entrenamiento este es un ejemplo sencillo pero recuerda que en modelos más grandes
y complejos necesitarán más iteraciones y a la vez será más lento el entrenamiento

resultados del entrenamiento

si vemos las salidas del entrenamiento vemos que las primeras linea pone

 httpwwwaprendemachinelearningcomwpcontentuploadsredneuronalxorpng

ne



o

d



l





una sencilla red neuronal con keras y tensorflow 

epoch 

 s msstep loss binaryaccuracy n


epoch 

 s usstep loss binaryaccuracy

y 

con esto vemos que la primer iteración acertó la mitad de las salidas pero a partir de la segunda
sólo acierta de cada luego en la epoch recupera el de aciertos ya no es por suerte
si no por haber ajustado correctamente los pesos de la red

epoch 
 s usstep loss binaryaccurac y
y 

epoch 
 s usstep loss binaryaccurac y
y 

epoch 
 s msstep loss binaryaccuracy y

y en mi caso en la iteración aumenta los aciertos al son de y en la iteración logra el
 de aciertos y se mantiene así hasta finalizar como los pesos iniciales de la red son aleatorios
puede que los resultados que tengas en tu ordenador sean ligeramente distintos en cuanto a las
iteraciones pero llegarás a la precisión binaria binaraaccuracy de 

evaluamos y predecimos

primero evaluamos el modelo

scores model evaluatetrainingdata targetdata
printwns f model metricsnames scores

y vemos que tuvimos un de precisión recordemos lo trivial de este ejemplo y hacemos las 
predicciones posibles de xor pasando nuestras entradas

print model predicttrainingdataround

y vemos las salidas que son las correctas

